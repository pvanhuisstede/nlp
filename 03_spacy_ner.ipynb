{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp spacy_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%matplotlib inline\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from IPython.display import display_html\n",
    "import tabulate\n",
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating spaCy's NER system\n",
    "(follows: https://github.com/nlptown/nlp-notebooks/blob/master/Updating%20spaCy's%20Named%20Entity%20Recognition%20System.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although pre-trained models are simple to use, we just have to plug them in, results will be disappointing when the data we work with differs, even slightly, from the data the model was trained on.\n",
    "\n",
    "So, we want to be able to train our own model. SpaCy has us covered:\n",
    "\n",
    "- we can train our model from scratch\n",
    "- we can continue a trained model with our own data.\n",
    "\n",
    "This second option aligns a bit more with our views: Context is king. We start with a contextualized dataset (persons, affiliations, topics together with the content of publications), start with unsupervised ML. Then use the output TOGETHER with the context of the questions we want to answer to use supervised ML (data) to make the models better.\n",
    "\n",
    "Let's look at a toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Alexander   </td><td>B</td><td>PERSON</td></tr>\n",
       "<tr><td>Boris       </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>de          </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>Pfeffel     </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>Johnson     </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>(           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>born        </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>19          </td><td>B</td><td>DATE  </td></tr>\n",
       "<tr><td>June        </td><td>I</td><td>DATE  </td></tr>\n",
       "<tr><td>1964        </td><td>I</td><td>DATE  </td></tr>\n",
       "<tr><td>)           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>is          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>a           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>British     </td><td>B</td><td>NORP  </td></tr>\n",
       "<tr><td>politician  </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>who         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>has         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>served      </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>as          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Prime       </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Minister    </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>of          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>the         </td><td>B</td><td>GPE   </td></tr>\n",
       "<tr><td>United      </td><td>I</td><td>GPE   </td></tr>\n",
       "<tr><td>Kingdom     </td><td>I</td><td>GPE   </td></tr>\n",
       "<tr><td>and         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Leader      </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>of          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>the         </td><td>B</td><td>ORG   </td></tr>\n",
       "<tr><td>Conservative</td><td>I</td><td>ORG   </td></tr>\n",
       "<tr><td>Party       </td><td>I</td><td>ORG   </td></tr>\n",
       "<tr><td>since       </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>2019        </td><td>B</td><td>DATE  </td></tr>\n",
       "<tr><td>;           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>he          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>lead        </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>the         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Vote        </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Leave       </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>campaign    </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>for         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Brexit      </td><td>B</td><td>PERSON</td></tr>\n",
       "<tr><td>.           </td><td>O</td><td>      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Alexander Boris de Pfeffel Johnson (born 19 June 1964) is a British politician who has served as Prime Minister of the United Kingdom and Leader of the Conservative Party since 2019; he lead the Vote Leave campaign for Brexit . \"\n",
    "\n",
    "doc = nlp(text)\n",
    "entities = [(t.text, t.ent_iob_, t.ent_type_) for t in doc]\n",
    "display(display_html(tabulate.tabulate(entities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the spaCy NER is actually quite good, we want to train the model some more with extra training data. So that a word like \"Brexit\" for example is properly recognized (Brexit is now labelled as PERSON). For this we do not use the actual sentence itself, too easy. But we will use similar sentences. Here we will use just a couple of sentences from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the 18 NER labels that spaCy uses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n",
      "CARDINAL: Numerals that do not fall under another type\n",
      "\n",
      "DATE: Absolute or relative dates or periods\n",
      "\n",
      "EVENT: Named hurricanes, battles, wars, sports events, etc.\n",
      "\n",
      "FAC: Buildings, airports, highways, bridges, etc.\n",
      "\n",
      "GPE: Countries, cities, states\n",
      "\n",
      "LANGUAGE: Any named language\n",
      "\n",
      "LAW: Named documents made into laws.\n",
      "\n",
      "LOC: Non-GPE locations, mountain ranges, bodies of water\n",
      "\n",
      "MONEY: Monetary values, including unit\n",
      "\n",
      "NORP: Nationalities or religious or political groups\n",
      "\n",
      "ORDINAL: \"first\", \"second\", etc.\n",
      "\n",
      "ORG: Companies, agencies, institutions, etc.\n",
      "\n",
      "PERCENT: Percentage, including \"%\"\n",
      "\n",
      "PERSON: People, including fictional\n",
      "\n",
      "PRODUCT: Objects, vehicles, foods, etc. (not services)\n",
      "\n",
      "QUANTITY: Measurements, as of weight or distance\n",
      "\n",
      "TIME: Times smaller than a day\n",
      "\n",
      "WORK_OF_ART: Titles of books, songs, etc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "ner_lst = nlp.pipe_labels['ner']\n",
    "print(ner_lst)\n",
    "for i in ner_lst:\n",
    "  print(f\"{i}: {spacy.explain(i)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have this existing pre-trained spaCy model that we want to update with some new examples (ideally these should be around 200-300 examples).\n",
    "\n",
    "These examples should be presented to spaCy as a list of tuples, that contain the text, and a dictionary of tuples, named entities that contains: the start and end indices of the named entity in the text, and the label of that named entity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "train_data = [\n",
    "  (\"Boris Johnson announced his pending resignation on 7 July 2022.\", {\"entities\": [(0,13,\"PERSON\"), (36,47,\"EVENT\"), (51,62,\"DATE\")]}),\n",
    "  (\"He will remain as prime minister until a new party leader is elected.\", {\"entities\": [(18,32,\"NORP\"), (45,57,\"NORP\"), (61,68,\"EVENT\")]}),\n",
    "  (\"He served as Secretary of State for Foreign and Commonwealth Affairs from 2016 to 2018.\", {\"entities\": [(13,68,\"NORP\"), (74,86,\"DATE\")]}),\n",
    "  (\"Boris Johnson served as Mayor of London from 2008 to 2016.\", {\"entities\": [(0,13,\"PERSON\"), (24,39,\"NORP\"), (45,57,\"DATE\")]}),\n",
    "  (\"He became a prominent figure in the successful Vote Leave campaign for Brexit in the 2016 European Union (EU) membership referendum.\", {\"entities\": [(47,66,\"EVENT\"), (71,77,\"EVENT\"), (85,89,\"DATE\"), (90,104,\"ORG\"), (106,108,\"ORG\"), (121,131,\"EVENT\")]}),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we set up the NER pipeline with the content of our training data, we make sure that we got the indices right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'resignation'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "test = \"Boris Johnson announced his pending resignation on 7 July 2022.\"\n",
    "test[36:47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set thing up, let's check if we have a NER in our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks OK, now we assign the NER to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to add these labels to the NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "for _, annotations in train_data:\n",
    "  for ent in annotations.get(\"entities\"):\n",
    "    ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start training, but only for the NER component of the pipeline, hence the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly train the NER model, we need to:\n",
    "\n",
    "- let the ner model loop over the examples for a sufficient number of iterations (10)\n",
    "- shuffle the examples in order NOT to base the training on the sequence (`random.shuffle()`)\n",
    "- pass the training data in batches (`minibatch`)\n",
    "- the use `nlp.update()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 16.5259361276355}\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "  for iteration in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "      texts, annotations = zip(*batch)\n",
    "      # new way of updating nlp NOT using nlp.update() anymore\n",
    "      example = []\n",
    "      # update the model with iterating each text\n",
    "      for i in range(len(texts)):\n",
    "        doc = nlp.make_doc(texts[i])\n",
    "        example.append(Example.from_dict(doc, annotations[i]))\n",
    "\n",
    "      nlp.update(example, drop=0.5, losses=losses)\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how our updated NER model now performs, using our earlier sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Alexander   </td><td>B</td><td>PERSON</td></tr>\n",
       "<tr><td>Boris       </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>de          </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>Pfeffel     </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>Johnson     </td><td>I</td><td>PERSON</td></tr>\n",
       "<tr><td>(           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>born        </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>19          </td><td>B</td><td>DATE  </td></tr>\n",
       "<tr><td>June        </td><td>I</td><td>DATE  </td></tr>\n",
       "<tr><td>1964        </td><td>I</td><td>DATE  </td></tr>\n",
       "<tr><td>)           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>is          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>a           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>British     </td><td>B</td><td>NORP  </td></tr>\n",
       "<tr><td>politician  </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>who         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>has         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>served      </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>as          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Prime       </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Minister    </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>of          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>the         </td><td>B</td><td>GPE   </td></tr>\n",
       "<tr><td>United      </td><td>I</td><td>GPE   </td></tr>\n",
       "<tr><td>Kingdom     </td><td>I</td><td>GPE   </td></tr>\n",
       "<tr><td>and         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Leader      </td><td>B</td><td>NORP  </td></tr>\n",
       "<tr><td>of          </td><td>I</td><td>NORP  </td></tr>\n",
       "<tr><td>the         </td><td>B</td><td>ORG   </td></tr>\n",
       "<tr><td>Conservative</td><td>I</td><td>ORG   </td></tr>\n",
       "<tr><td>Party       </td><td>I</td><td>ORG   </td></tr>\n",
       "<tr><td>since       </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>2019        </td><td>B</td><td>DATE  </td></tr>\n",
       "<tr><td>;           </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>he          </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>lead        </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>the         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Vote        </td><td>B</td><td>EVENT </td></tr>\n",
       "<tr><td>Leave       </td><td>I</td><td>EVENT </td></tr>\n",
       "<tr><td>campaign    </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>for         </td><td>O</td><td>      </td></tr>\n",
       "<tr><td>Brexit      </td><td>B</td><td>EVENT </td></tr>\n",
       "<tr><td>.           </td><td>O</td><td>      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| export\n",
    "text = \"Alexander Boris de Pfeffel Johnson (born 19 June 1964) is a British politician who has served as Prime Minister of the United Kingdom and Leader of the Conservative Party since 2019; he lead the Vote Leave campaign for Brexit . \"\n",
    "\n",
    "doc = nlp(text)\n",
    "entities = [(t.text, t.ent_iob_, t.ent_type_) for t in doc]\n",
    "display(display_html(tabulate.tabulate(entities, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but we trained a little bit on the subject which is precisely why we, the RePubXL team, propose supervised ML on smaller contextual datasets. The power of these NER updates is that, based on the examples, the model can still generalize due to the word-embeddings vectorspace.\n",
    "\n",
    "Now, we want to keep our updated model for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: content\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# save the model to a directory\n",
    "output_dir = Path('content/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Saved model to: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: content\n",
      "Entities [('Johnson', 'PERSON'), ('British', 'NORP')]\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Load the saved model to predict\n",
    "print(f\"Loading from: {output_dir}\")\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Johnson is a controversial figure in British politics.\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells above we started with a pre-trained model. One can also choose to start with an empty model, using `spacy.blank()`, passing in the \"en\" argument for the English language. Because it is an empty model, we have to add this `ner` to the pipeline using `add_pipe()`. We do not have to disable other pipelines, as we are just adding a new one, **not** changing an existing one, and just that one and not the other parts of the pipeline.\n",
    "\n",
    "One does have to use a large(r) number of training cases.\n",
    "\n",
    "Just a small example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treblinka 0 9 GPE\n",
      "Treblinka 61 70 GPE\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "            {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our new model to get more info and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Treblinka is a small village in Poland.', {'entities': [[0, 9, 'GPE']]}], ['Wikipedia notes that Treblinka is not large.', {'entities': [[21, 30, 'GPE']]}]]\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"Treblinka is a small village in Poland. Wikipedia notes that Treblinka is not large.\"\n",
    "\n",
    "corpus = []\n",
    "\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    corpus.append(sent.text)\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "            {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "#iterate over the corpus again\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #remember, entities needs to be a dictionary in index 1 of the list, so it needs to be an empty list\n",
    "    entities = []\n",
    "    \n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        #appending to entities in the correct format\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "        \n",
    "    TRAIN_DATA.append([sentence, {\"entities\": entities}])\n",
    "\n",
    "print (TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a completely new entity type in spaCy\n",
    "\n",
    "All code above was directed at training the `ner` to categorize correctly, either adjusting a pre-trained model or starting from a new blank model and adjusting that as one goes.\n",
    "\n",
    "But what to do if you want to work with a category that is NOT defined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Get the `ner` component of the pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ner = nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Add the new label\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,15, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate souffl√© is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to train the model:\n",
    "\n",
    "- first add the new label with `ner.add_label()`\n",
    "- Resume training\n",
    "- Select the pipes to be trained\n",
    "- Single out the pipes NOT to be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 0.0024407938347574204}\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Begin training by disabling other pipeline components\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "  sizes = compounding(1.0, 4.0, 1.001)\n",
    "  for iteration in range(30):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "    for batch in batches:\n",
    "      texts, annotations = zip(*batch)\n",
    "      # new way of updating nlp NOT using nlp.update() anymore\n",
    "      example = []\n",
    "      # update the model with iterating each text\n",
    "      for i in range(len(texts)):\n",
    "        doc = nlp.make_doc(texts[i])\n",
    "        example.append(Example.from_dict(doc, annotations[i]))\n",
    "\n",
    "      nlp.update(example, drop=0.5, losses=losses)\n",
    "\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training complete, let's test our `ner`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Sushi\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "  print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a2ef650857c097cc4b789456eae376654e6121eead7baaf4f6a1af0eeae8bb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
