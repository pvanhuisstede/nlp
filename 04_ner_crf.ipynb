{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp ner_crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with Conditional Random Fields (CRF)\n",
    "(follows: )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRF is a powerful technique before Deep Learning became popular. POS tagging of sequences in order to label the most important information related to the problem at hand: generic named entities (locations, people, and organizations) or more specialized entities such as disease or symptomes.\n",
    "\n",
    "For this we use `sklearn-crfsuite`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess, due to the upsurge of Deep Learning the `sklearn-crfsuite` is not updated anymore. So instead of the snippet below, we have to install an updated version of the library to be able to produce reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#!pip install sklearn-crfsuite # Run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#%pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git\\#egg=sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import nltk\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn_crfsuite as crfsuite\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#nltk.download('conll2002') # Just run this line once\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('ned.train'))\n",
    "dev_sents = list(nltk.corpus.conll2002.iob_sents('ned.testa'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('ned.testb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data. They are a list of tokenized sentences: the string, POS tag and it's entity tag. Nowadays the POS tag is not used in deep learning, but with CRF it provides useful information: Nouns are more common denoting entities than verbs, so the POS tags carry useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('De', 'Art', 'O'),\n",
       " ('tekst', 'N', 'O'),\n",
       " ('van', 'Prep', 'O'),\n",
       " ('het', 'Art', 'O'),\n",
       " ('arrest', 'N', 'O'),\n",
       " ('is', 'V', 'O'),\n",
       " ('nog', 'Adv', 'O'),\n",
       " ('niet', 'Adv', 'O'),\n",
       " ('schriftelijk', 'Adj', 'O'),\n",
       " ('beschikbaar', 'Adj', 'O'),\n",
       " ('maar', 'Conj', 'O'),\n",
       " ('het', 'Art', 'O'),\n",
       " ('bericht', 'N', 'O'),\n",
       " ('werd', 'V', 'O'),\n",
       " ('alvast', 'Adv', 'O'),\n",
       " ('bekendgemaakt', 'V', 'O'),\n",
       " ('door', 'Prep', 'O'),\n",
       " ('een', 'Art', 'O'),\n",
       " ('communicatiebureau', 'N', 'O'),\n",
       " ('dat', 'Conj', 'O'),\n",
       " ('Floralux', 'N', 'B-ORG'),\n",
       " ('inhuurde', 'V', 'O'),\n",
       " ('.', 'Punc', 'O')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "train_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does CRF work? Deep learning neural nets just learn their relevant features from the input texts themselves. CRFs learn the realtionship between **the features we give them** and the **label of a token in a given context**. They **do not** learn these features themselves, the quality of the model highly depends on the features we present to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore:\n",
    "\n",
    "- use a method to collect the features for every token:\n",
    "  - the word itself + POS tag\n",
    "  - completely uppercase? starts with digit? starts with capital?\n",
    "  - character bigram or trigram the word ends with\n",
    "  - use a `bias` feature that always has the same value (through it the CRF model can learn the relative freq. of each label type in the training data)\n",
    "\n",
    "- we use the word embeddings to give the model more information about the meaning of a word (500 wikipedia clusters of word embeddings). We read those from file and map each word to the ID of the cluster it is in. Define `read_clusters`.\n",
    "\n",
    "- we want the CRF to look at the context of a token. We provide the CRF with the 2 words on either side of the token: their case, POS tags. If there is no left or right, we give it that information: BOS or EOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Making use of the Wikipedia word embeddings\n",
    "def read_clusters(cluster_file):\n",
    "  word2cluster = {}\n",
    "  with open(cluster_file) as i:\n",
    "    for line in i:\n",
    "      word, cluster = line.strip().split('\\t')\n",
    "      word2cluster[word] = cluster\n",
    "  return word2cluster\n",
    "\n",
    "# Using features of the words AND looking at the context of a token (neigbours +/- 2)\n",
    "def word2features(sent, i, word2cluster):\n",
    "  word = sent[i][0]\n",
    "  postag = sent[i][1]\n",
    "  features = [\n",
    "    'bias',\n",
    "    'word.lower=' + word.lower(),\n",
    "    'word[-3]=' + word[-3:], # looking at the last 3 chars of the token\n",
    "    'word[-2]=' + word[-2:], # looking at the last 2 chars of the token\n",
    "    'word.isupper=%s' % word.isupper(),\n",
    "    'word.istitle=%s' % word.istitle(),\n",
    "    'word.isdigit=%s' % word.isdigit(),\n",
    "    'word.cluster=%s' % word2cluster[word.lower()] if word.lower() in word2cluster else '0'\n",
    "    'postag=' + postag\n",
    "  ]\n",
    "  # Look at the first neighbour token to the left\n",
    "  if i > 0:\n",
    "    word1 = sent[i-1][0]\n",
    "    postag1 = sent[i-1][1]\n",
    "    features.extend([\n",
    "      '-1:word.lower=' + word1.lower(),\n",
    "      '-1:word.istitle=%s' % word1.istitle(),\n",
    "      '-1:word.isupper=%s' % word1.isupper(),\n",
    "      '-1:postag=' + postag1\n",
    "    ])\n",
    "  else:\n",
    "    features.append('BOS')\n",
    "  # Look at the second neighbour to the left\n",
    "  if i > 1: \n",
    "    word2 = sent[i-2][0]\n",
    "    postag2 = sent[i-2][1]\n",
    "    features.extend([\n",
    "      '-2:word.lower=' + word2.lower(),\n",
    "      '-2:word.istitle=%s' % word2.istitle(),\n",
    "      '-2:word.isupper=%s' % word2.isupper(),\n",
    "      '-2:postag=' + postag2\n",
    "    ])\n",
    "  # look at the first neigbour to the right\n",
    "  if i < len(sent)-1:\n",
    "    word1 = sent[i+1][0]\n",
    "    postag1 = sent[+1][0]\n",
    "    features.extend([\n",
    "      '+1:word.lower=' + word1.lower(),\n",
    "      '+1:word.istitle=%s' % word1.istitle(),\n",
    "      \"+1:word.isupper=%s\" % word1.isupper(),\n",
    "      '+1:postag=' + postag1\n",
    "    ])\n",
    "  else:\n",
    "    features.append('EOS')\n",
    "  # Look at the second neighbour to the right\n",
    "  if i < len(sent)-2:\n",
    "    word2 = sent[i+2][0]\n",
    "    postag2 = sent[+2][0]\n",
    "    features.extend([\n",
    "      '+2:word.lower=' + word2.lower(),\n",
    "      '+2:word.istitle=%s' % word2.istitle(),\n",
    "      \"+2:word.isupper=%s\" % word2.isupper(),\n",
    "      '+2:postag=' + postag2\n",
    "    ])\n",
    "  return features\n",
    "\n",
    "# Now we define the functions to do all the work\n",
    "def sent2features(sent, word2cluster):\n",
    "  return [word2features(sent, i, word2cluster) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "  return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "  return [token for token, postag, label in sent]\n",
    "\n",
    "word2cluster = read_clusters('/home/peter/Documents/data/nlp/clusters_nl.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the `sent2features` function out using the first word from the first training_sent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('De', 'Art', 'O')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "train_sents[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'word.lower=de',\n",
       " 'word[-3]=De',\n",
       " 'word[-2]=De',\n",
       " 'word.isupper=False',\n",
       " 'word.istitle=True',\n",
       " 'word.isdigit=False',\n",
       " 'word.cluster=38',\n",
       " 'BOS',\n",
       " '+1:word.lower=tekst',\n",
       " '+1:word.istitle=False',\n",
       " '+1:word.isupper=False',\n",
       " '+1:postag=tekst',\n",
       " '+2:word.lower=van',\n",
       " '+2:word.istitle=False',\n",
       " '+2:word.isupper=False',\n",
       " '+2:postag=van']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "sent2features(train_sents[0], word2cluster)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign our training and test sets the appropriate labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "X_train = [sent2features(s, word2cluster) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_dev = [sent2features(s, word2cluster) for s in dev_sents]\n",
    "y_dev = [sent2labels(s) for s in dev_sents]\n",
    "\n",
    "X_test = [sent2features(s, word2cluster) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a CRF model and start the training using the standard `lbfgs` algorithm for parameter estimation and run it for 100 iterations.\n",
    "\n",
    "When done, we save the model using `joblib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 15806/15806 [00:01<00:00, 10135.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading dev data to CRFsuite: 100%|██████████| 2895/2895 [00:00<00:00, 9396.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Holdout group: 2\n",
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 171320\n",
      "Seconds required: 0.414\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 100\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.29  loss=104683.26 active=171320 precision=0.100  recall=0.111  F1=0.105  Acc(item/seq)=0.901 0.496  feature_norm=1.00\n",
      "Iter 2   time=0.16  loss=96793.85 active=171320 precision=0.100  recall=0.111  F1=0.105  Acc(item/seq)=0.901 0.496  feature_norm=1.15\n",
      "Iter 3   time=0.16  loss=92785.91 active=171320 precision=0.100  recall=0.111  F1=0.105  Acc(item/seq)=0.901 0.496  feature_norm=1.26\n",
      "Iter 4   time=0.16  loss=87079.17 active=171320 precision=0.100  recall=0.111  F1=0.105  Acc(item/seq)=0.901 0.496  feature_norm=1.46\n",
      "Iter 5   time=0.16  loss=74874.43 active=171320 precision=0.100  recall=0.111  F1=0.105  Acc(item/seq)=0.901 0.496  feature_norm=2.06\n",
      "Iter 6   time=0.16  loss=56175.11 active=171320 precision=0.367  recall=0.212  F1=0.205  Acc(item/seq)=0.914 0.528  feature_norm=3.80\n",
      "Iter 7   time=0.17  loss=49697.02 active=171320 precision=0.327  recall=0.336  F1=0.312  Acc(item/seq)=0.928 0.570  feature_norm=4.88\n",
      "Iter 8   time=0.16  loss=43311.61 active=171320 precision=0.347  recall=0.378  F1=0.342  Acc(item/seq)=0.935 0.605  feature_norm=6.41\n",
      "Iter 9   time=0.16  loss=39075.88 active=171320 precision=0.552  recall=0.435  F1=0.440  Acc(item/seq)=0.940 0.626  feature_norm=8.23\n",
      "Iter 10  time=0.17  loss=36519.06 active=171320 precision=0.553  recall=0.446  F1=0.447  Acc(item/seq)=0.942 0.645  feature_norm=8.92\n",
      "Iter 11  time=0.17  loss=34579.58 active=171320 precision=0.539  recall=0.452  F1=0.445  Acc(item/seq)=0.943 0.651  feature_norm=9.76\n",
      "Iter 12  time=0.16  loss=33027.56 active=171320 precision=0.540  recall=0.457  F1=0.451  Acc(item/seq)=0.943 0.654  feature_norm=10.85\n",
      "Iter 13  time=0.16  loss=30299.19 active=171320 precision=0.537  recall=0.493  F1=0.489  Acc(item/seq)=0.945 0.664  feature_norm=13.28\n",
      "Iter 14  time=0.16  loss=29249.47 active=171320 precision=0.577  recall=0.523  F1=0.516  Acc(item/seq)=0.948 0.670  feature_norm=14.53\n",
      "Iter 15  time=0.16  loss=28170.22 active=171320 precision=0.668  recall=0.494  F1=0.508  Acc(item/seq)=0.948 0.668  feature_norm=13.45\n",
      "Iter 16  time=0.17  loss=27023.73 active=171320 precision=0.679  recall=0.511  F1=0.526  Acc(item/seq)=0.949 0.676  feature_norm=13.75\n",
      "Iter 17  time=0.16  loss=25631.34 active=171320 precision=0.666  recall=0.555  F1=0.573  Acc(item/seq)=0.952 0.698  feature_norm=14.54\n",
      "Iter 18  time=0.16  loss=24453.50 active=171320 precision=0.658  recall=0.573  F1=0.591  Acc(item/seq)=0.954 0.709  feature_norm=15.41\n",
      "Iter 19  time=0.17  loss=22684.11 active=171320 precision=0.633  recall=0.593  F1=0.600  Acc(item/seq)=0.954 0.711  feature_norm=17.45\n",
      "Iter 20  time=0.16  loss=21363.83 active=171320 precision=0.669  recall=0.598  F1=0.612  Acc(item/seq)=0.956 0.721  feature_norm=19.11\n",
      "Iter 21  time=0.16  loss=20830.16 active=171320 precision=0.673  recall=0.594  F1=0.617  Acc(item/seq)=0.957 0.721  feature_norm=19.29\n",
      "Iter 22  time=0.17  loss=20155.15 active=171320 precision=0.683  recall=0.596  F1=0.620  Acc(item/seq)=0.957 0.722  feature_norm=19.42\n",
      "Iter 23  time=0.16  loss=19700.84 active=171320 precision=0.695  recall=0.598  F1=0.626  Acc(item/seq)=0.957 0.719  feature_norm=19.22\n",
      "Iter 24  time=0.17  loss=19128.84 active=171320 precision=0.681  recall=0.590  F1=0.611  Acc(item/seq)=0.954 0.710  feature_norm=19.78\n",
      "Iter 25  time=0.17  loss=18404.39 active=171320 precision=0.682  recall=0.598  F1=0.624  Acc(item/seq)=0.957 0.717  feature_norm=19.61\n",
      "Iter 26  time=0.17  loss=18268.58 active=171320 precision=0.681  recall=0.607  F1=0.627  Acc(item/seq)=0.958 0.722  feature_norm=19.79\n",
      "Iter 27  time=0.16  loss=17815.24 active=171320 precision=0.668  recall=0.611  F1=0.618  Acc(item/seq)=0.958 0.730  feature_norm=20.55\n",
      "Iter 28  time=0.16  loss=17284.00 active=171320 precision=0.682  recall=0.619  F1=0.626  Acc(item/seq)=0.959 0.738  feature_norm=22.00\n",
      "Iter 29  time=0.16  loss=16735.40 active=171320 precision=0.678  recall=0.630  F1=0.642  Acc(item/seq)=0.959 0.742  feature_norm=22.71\n",
      "Iter 30  time=0.16  loss=16329.82 active=171320 precision=0.701  recall=0.634  F1=0.658  Acc(item/seq)=0.961 0.745  feature_norm=23.25\n",
      "Iter 31  time=0.16  loss=15934.11 active=171320 precision=0.711  recall=0.634  F1=0.663  Acc(item/seq)=0.961 0.747  feature_norm=24.00\n",
      "Iter 32  time=0.17  loss=15793.83 active=171320 precision=0.733  recall=0.629  F1=0.665  Acc(item/seq)=0.961 0.740  feature_norm=25.18\n",
      "Iter 33  time=0.17  loss=15409.50 active=171320 precision=0.717  recall=0.639  F1=0.667  Acc(item/seq)=0.961 0.743  feature_norm=25.49\n",
      "Iter 34  time=0.17  loss=15173.77 active=171320 precision=0.707  recall=0.643  F1=0.664  Acc(item/seq)=0.962 0.749  feature_norm=25.46\n",
      "Iter 35  time=0.17  loss=14809.61 active=171320 precision=0.714  recall=0.661  F1=0.672  Acc(item/seq)=0.962 0.750  feature_norm=25.88\n",
      "Iter 36  time=0.17  loss=14298.46 active=171320 precision=0.712  recall=0.666  F1=0.680  Acc(item/seq)=0.963 0.753  feature_norm=27.18\n",
      "Iter 37  time=0.17  loss=14100.25 active=171320 precision=0.700  recall=0.643  F1=0.652  Acc(item/seq)=0.962 0.748  feature_norm=29.00\n",
      "Iter 38  time=0.17  loss=13835.29 active=171320 precision=0.704  recall=0.653  F1=0.667  Acc(item/seq)=0.963 0.752  feature_norm=29.18\n",
      "Iter 39  time=0.16  loss=13667.00 active=171320 precision=0.712  recall=0.650  F1=0.673  Acc(item/seq)=0.963 0.750  feature_norm=29.53\n",
      "Iter 40  time=0.17  loss=13405.07 active=171320 precision=0.738  recall=0.667  F1=0.696  Acc(item/seq)=0.964 0.752  feature_norm=30.41\n",
      "Iter 41  time=0.17  loss=13043.15 active=171320 precision=0.741  recall=0.674  F1=0.702  Acc(item/seq)=0.965 0.753  feature_norm=32.12\n",
      "Iter 42  time=0.31  loss=12878.32 active=171320 precision=0.745  recall=0.669  F1=0.700  Acc(item/seq)=0.965 0.752  feature_norm=33.10\n",
      "Iter 43  time=0.17  loss=12674.20 active=171320 precision=0.733  recall=0.666  F1=0.691  Acc(item/seq)=0.964 0.752  feature_norm=34.04\n",
      "Iter 44  time=0.16  loss=12481.12 active=171320 precision=0.730  recall=0.670  F1=0.689  Acc(item/seq)=0.964 0.755  feature_norm=34.73\n",
      "Iter 45  time=0.16  loss=12285.07 active=171320 precision=0.727  recall=0.669  F1=0.685  Acc(item/seq)=0.964 0.762  feature_norm=35.32\n",
      "Iter 46  time=0.17  loss=11996.62 active=171320 precision=0.727  recall=0.676  F1=0.690  Acc(item/seq)=0.965 0.765  feature_norm=36.59\n",
      "Iter 47  time=0.17  loss=11955.68 active=171320 precision=0.736  recall=0.677  F1=0.694  Acc(item/seq)=0.965 0.769  feature_norm=36.73\n",
      "Iter 48  time=0.16  loss=11670.63 active=171320 precision=0.726  recall=0.672  F1=0.689  Acc(item/seq)=0.965 0.762  feature_norm=37.13\n",
      "Iter 49  time=0.17  loss=11590.71 active=171320 precision=0.732  recall=0.676  F1=0.697  Acc(item/seq)=0.965 0.762  feature_norm=37.18\n",
      "Iter 50  time=0.17  loss=11508.82 active=171320 precision=0.736  recall=0.668  F1=0.696  Acc(item/seq)=0.965 0.758  feature_norm=37.21\n",
      "Iter 51  time=0.16  loss=11385.11 active=171320 precision=0.747  recall=0.679  F1=0.706  Acc(item/seq)=0.965 0.760  feature_norm=37.39\n",
      "Iter 52  time=0.17  loss=11221.01 active=171320 precision=0.757  recall=0.683  F1=0.714  Acc(item/seq)=0.966 0.766  feature_norm=37.95\n",
      "Iter 53  time=0.17  loss=11072.04 active=171320 precision=0.756  recall=0.691  F1=0.714  Acc(item/seq)=0.967 0.773  feature_norm=38.75\n",
      "Iter 54  time=0.17  loss=10976.00 active=171320 precision=0.753  recall=0.690  F1=0.712  Acc(item/seq)=0.967 0.776  feature_norm=39.29\n",
      "Iter 55  time=0.16  loss=10853.24 active=171320 precision=0.755  recall=0.692  F1=0.714  Acc(item/seq)=0.967 0.778  feature_norm=40.07\n",
      "Iter 56  time=0.17  loss=10762.11 active=171320 precision=0.745  recall=0.701  F1=0.719  Acc(item/seq)=0.968 0.781  feature_norm=41.34\n",
      "Iter 57  time=0.17  loss=10624.85 active=171320 precision=0.755  recall=0.696  F1=0.720  Acc(item/seq)=0.968 0.779  feature_norm=41.56\n",
      "Iter 58  time=0.16  loss=10453.10 active=171320 precision=0.765  recall=0.692  F1=0.720  Acc(item/seq)=0.968 0.782  feature_norm=42.18\n",
      "Iter 59  time=0.17  loss=10258.35 active=171320 precision=0.769  recall=0.689  F1=0.720  Acc(item/seq)=0.968 0.781  feature_norm=43.65\n",
      "Iter 60  time=0.31  loss=10183.71 active=171320 precision=0.779  recall=0.702  F1=0.730  Acc(item/seq)=0.968 0.782  feature_norm=44.49\n",
      "Iter 61  time=0.17  loss=10076.86 active=171320 precision=0.778  recall=0.704  F1=0.732  Acc(item/seq)=0.969 0.784  feature_norm=45.78\n",
      "Iter 62  time=0.16  loss=10015.00 active=171320 precision=0.782  recall=0.711  F1=0.739  Acc(item/seq)=0.969 0.783  feature_norm=46.24\n",
      "Iter 63  time=0.17  loss=9931.66  active=171320 precision=0.774  recall=0.711  F1=0.737  Acc(item/seq)=0.969 0.783  feature_norm=46.68\n",
      "Iter 64  time=0.17  loss=9803.68  active=171320 precision=0.767  recall=0.713  F1=0.736  Acc(item/seq)=0.969 0.784  feature_norm=47.45\n",
      "Iter 65  time=0.17  loss=9701.67  active=171320 precision=0.779  recall=0.705  F1=0.736  Acc(item/seq)=0.970 0.783  feature_norm=48.63\n",
      "Iter 66  time=0.17  loss=9639.15  active=171320 precision=0.779  recall=0.708  F1=0.737  Acc(item/seq)=0.969 0.784  feature_norm=48.33\n",
      "Iter 67  time=0.17  loss=9598.77  active=171320 precision=0.773  recall=0.703  F1=0.731  Acc(item/seq)=0.969 0.782  feature_norm=48.19\n",
      "Iter 68  time=0.16  loss=9540.64  active=171320 precision=0.774  recall=0.704  F1=0.732  Acc(item/seq)=0.969 0.784  feature_norm=48.31\n",
      "Iter 69  time=0.17  loss=9413.68  active=171320 precision=0.768  recall=0.701  F1=0.728  Acc(item/seq)=0.969 0.782  feature_norm=49.24\n",
      "Iter 70  time=0.17  loss=9380.77  active=171320 precision=0.761  recall=0.702  F1=0.726  Acc(item/seq)=0.969 0.788  feature_norm=49.99\n",
      "Iter 71  time=0.17  loss=9304.96  active=171320 precision=0.770  recall=0.698  F1=0.727  Acc(item/seq)=0.969 0.784  feature_norm=49.95\n",
      "Iter 72  time=0.17  loss=9281.71  active=171320 precision=0.768  recall=0.701  F1=0.729  Acc(item/seq)=0.969 0.783  feature_norm=50.13\n",
      "Iter 73  time=0.16  loss=9235.57  active=171320 precision=0.768  recall=0.690  F1=0.721  Acc(item/seq)=0.968 0.780  feature_norm=50.92\n",
      "Iter 74  time=0.17  loss=9174.10  active=171320 precision=0.772  recall=0.703  F1=0.731  Acc(item/seq)=0.969 0.782  feature_norm=51.34\n",
      "Iter 75  time=0.16  loss=9113.84  active=171320 precision=0.772  recall=0.704  F1=0.732  Acc(item/seq)=0.969 0.784  feature_norm=51.94\n",
      "Iter 76  time=0.16  loss=9070.10  active=171320 precision=0.773  recall=0.702  F1=0.730  Acc(item/seq)=0.969 0.788  feature_norm=52.42\n",
      "Iter 77  time=0.16  loss=9004.30  active=171320 precision=0.770  recall=0.702  F1=0.729  Acc(item/seq)=0.969 0.788  feature_norm=53.14\n",
      "Iter 78  time=0.17  loss=8984.87  active=171320 precision=0.778  recall=0.707  F1=0.735  Acc(item/seq)=0.969 0.789  feature_norm=54.97\n",
      "Iter 79  time=0.16  loss=8872.31  active=171320 precision=0.769  recall=0.710  F1=0.733  Acc(item/seq)=0.969 0.789  feature_norm=54.86\n",
      "Iter 80  time=0.17  loss=8835.66  active=171320 precision=0.767  recall=0.710  F1=0.732  Acc(item/seq)=0.969 0.787  feature_norm=54.89\n",
      "Iter 81  time=0.17  loss=8784.90  active=171320 precision=0.765  recall=0.702  F1=0.726  Acc(item/seq)=0.969 0.789  feature_norm=55.33\n",
      "Iter 82  time=0.17  loss=8767.61  active=171320 precision=0.775  recall=0.702  F1=0.733  Acc(item/seq)=0.969 0.787  feature_norm=56.06\n",
      "Iter 83  time=0.17  loss=8728.88  active=171320 precision=0.772  recall=0.703  F1=0.731  Acc(item/seq)=0.969 0.787  feature_norm=56.51\n",
      "Iter 84  time=0.17  loss=8712.06  active=171320 precision=0.781  recall=0.708  F1=0.738  Acc(item/seq)=0.970 0.791  feature_norm=56.58\n",
      "Iter 85  time=0.17  loss=8684.72  active=171320 precision=0.785  recall=0.703  F1=0.736  Acc(item/seq)=0.970 0.792  feature_norm=56.98\n",
      "Iter 86  time=0.17  loss=8648.08  active=171320 precision=0.784  recall=0.703  F1=0.734  Acc(item/seq)=0.969 0.790  feature_norm=57.61\n",
      "Iter 87  time=0.17  loss=8596.04  active=171320 precision=0.782  recall=0.711  F1=0.740  Acc(item/seq)=0.970 0.793  feature_norm=58.61\n",
      "Iter 88  time=0.31  loss=8578.00  active=171320 precision=0.780  recall=0.710  F1=0.738  Acc(item/seq)=0.970 0.792  feature_norm=59.13\n",
      "Iter 89  time=0.17  loss=8557.37  active=171320 precision=0.775  recall=0.710  F1=0.737  Acc(item/seq)=0.970 0.791  feature_norm=59.34\n",
      "Iter 90  time=0.17  loss=8534.07  active=171320 precision=0.772  recall=0.719  F1=0.741  Acc(item/seq)=0.970 0.793  feature_norm=59.73\n",
      "Iter 91  time=0.17  loss=8507.33  active=171320 precision=0.772  recall=0.716  F1=0.739  Acc(item/seq)=0.970 0.792  feature_norm=59.70\n",
      "Iter 92  time=0.17  loss=8483.16  active=171320 precision=0.772  recall=0.716  F1=0.739  Acc(item/seq)=0.970 0.792  feature_norm=59.70\n",
      "Iter 93  time=0.17  loss=8454.27  active=171320 precision=0.778  recall=0.715  F1=0.741  Acc(item/seq)=0.970 0.792  feature_norm=59.84\n",
      "Iter 94  time=0.17  loss=8424.34  active=171320 precision=0.780  recall=0.711  F1=0.739  Acc(item/seq)=0.970 0.791  feature_norm=59.85\n",
      "Iter 95  time=0.32  loss=8393.51  active=171320 precision=0.788  recall=0.713  F1=0.744  Acc(item/seq)=0.970 0.793  feature_norm=60.32\n",
      "Iter 96  time=0.17  loss=8353.12  active=171320 precision=0.789  recall=0.711  F1=0.743  Acc(item/seq)=0.970 0.792  feature_norm=60.27\n",
      "Iter 97  time=0.17  loss=8327.82  active=171320 precision=0.790  recall=0.718  F1=0.748  Acc(item/seq)=0.971 0.793  feature_norm=60.18\n",
      "Iter 98  time=0.17  loss=8312.62  active=171320 precision=0.792  recall=0.721  F1=0.751  Acc(item/seq)=0.971 0.792  feature_norm=60.07\n",
      "Iter 99  time=0.17  loss=8301.72  active=171320 precision=0.791  recall=0.724  F1=0.753  Acc(item/seq)=0.971 0.791  feature_norm=59.93\n",
      "Iter 100 time=0.17  loss=8286.46  active=171320 precision=0.787  recall=0.721  F1=0.748  Acc(item/seq)=0.970 0.791  feature_norm=59.82\n",
      "================================================\n",
      "Label      Precision    Recall     F1    Support\n",
      "-------  -----------  --------  -----  ---------\n",
      "B-LOC          0.814     0.806  0.810        479\n",
      "B-MISC         0.786     0.650  0.712        748\n",
      "B-ORG          0.837     0.599  0.698        686\n",
      "B-PER          0.753     0.825  0.787        703\n",
      "I-LOC          0.589     0.516  0.550         64\n",
      "I-MISC         0.617     0.516  0.562        215\n",
      "I-ORG          0.836     0.654  0.734        396\n",
      "I-PER          0.859     0.922  0.889        423\n",
      "O              0.988     0.998  0.993      33973\n",
      "------------------------------------------------\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 17.251\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 171320 (171320)\n",
      "Number of active attributes: 138015 (154350)\n",
      "Number of active labels: 9 (9)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.060\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, max_iterations=100, verbose=&#x27;true&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, max_iterations=100, verbose=&#x27;true&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CRF(algorithm='lbfgs', max_iterations=100, verbose='true')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "crf = crfsuite.CRF(\n",
    "    verbose='true',\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train, X_dev=X_dev, y_dev=y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an error, but this has to do with the sklearn version we use; to let the error disappear we should use sklearn < 24.0.\n",
    "\n",
    "Let's see whether we can write our model to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/peter/Documents/data/nlp/models/crf_model']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "OUTPUT_PATH = '/home/peter/Documents/data/nlp/models'\n",
    "OUTPUT_FILE = 'crf_model'\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "  os.mkdir(OUTPUT_PATH)\n",
    "\n",
    "joblib.dump(crf, os.path.join(OUTPUT_PATH, OUTPUT_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model saved, we now can evaluate the output of our CRF model. We will load our model from file and test it on the full test set.\n",
    "\n",
    "We will have a look at the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Dat is in Italië , Spanje of Engeland misschien geen probleem , maar volgens ' Der Kaiser ' in Duitsland wel .\n",
      "Predicted: O O O B-LOC O B-LOC O B-LOC O O O O O O O B-MISC I-MISC O O B-LOC O O\n",
      "Correct: O O O B-LOC O B-LOC O B-LOC O O O O O O O B-PER I-PER O O B-LOC O O\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "crf = joblib.load(os.path.join(OUTPUT_PATH, OUTPUT_FILE))\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "example_sent = test_sents[0]\n",
    "print(\"Sentence:\", ' '.join(sent2tokens(example_sent)))\n",
    "print(\"Predicted:\", ' '.join(crf.predict([sent2features(example_sent, word2cluster)])[0]))\n",
    "print(\"Correct:\", ' '.join(sent2labels(example_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to evalaute the whole test set. We print out a classification report for all labels except 'O'. They are the majority of labels anyway, so they will skew the results (they are most probably assigned correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.83      0.82      0.83       774\n",
      "       I-LOC       0.35      0.45      0.40        49\n",
      "      B-MISC       0.81      0.61      0.70      1187\n",
      "      I-MISC       0.54      0.41      0.46       410\n",
      "       B-ORG       0.79      0.70      0.74       882\n",
      "       I-ORG       0.75      0.61      0.67       551\n",
      "       B-PER       0.82      0.88      0.85      1098\n",
      "       I-PER       0.90      0.95      0.93       807\n",
      "\n",
      "   micro avg       0.80      0.74      0.77      5758\n",
      "   macro avg       0.72      0.68      0.70      5758\n",
      "weighted avg       0.80      0.74      0.76      5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "y_pred = crf.predict(X_test)\n",
    "sorted_labels = sorted(\n",
    "  labels,\n",
    "  key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "# The following code only runs with the updated metrics.py module in `sklearn_crfsuite` library.\n",
    "# Here: pip install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git\\#egg=sklearn_crfsuite\n",
    "print(metrics.flat_classification_report(y_test, y_pred, labels=sorted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have good scores, especially `B-LOC` and `B-PER` score very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use the `eli5` library to have a look at the most likely transitions the CRF model has identified. `Eli5` helps us to explain the predictions of our CRF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the optimal hyperparameters\n",
    "\n",
    "So far we've trained a model with the default parameters. It's unlikely that these will give us the best performance possible. Therefore we're going to search automatically for the best hyperparameter settings by iteratively training different models and evaluating them. Eventually we'll pick the best one.\n",
    "\n",
    "Here we'll focus on two parameters: c1 and c2. These are the parameters for L1 and L2 regularization, respectively. Regularization prevents overfitting on the training data by adding a penalty to the loss function. In L1 regularization, this penalty is the sum of the absolute values of the weights; in L2 regularization, it is the sum of the squared weights. L1 regularization performs a type of feature selection, as it assigns 0 weight to irrelevant features. L2 regularization, by contrast, makes the weight of irrelevant features small, but not necessarily zero. L1 regularization is often called the Lasso method, L2 is called the Ridge method, and the linear combination of both is called Elastic Net regularization.\n",
    "\n",
    "We define the parameter space for c1 and c2 and use the flat F1-score to compare the individual models. We'll rely on three-fold cross validation to score each of the 50 candidates. We use a randomized search, which means we're not going to try out all specified parameter settings, but instead, we'll let the process sample randomly from the distributions we've specified in the parameter space. It will do this 50 (n_iter) times. This process takes a while, but it's worth the wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                 all_possible_transitions=True,\n",
       "                                 keep_tempfiles=True, max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={&#x27;c1&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fccf4b8a070&gt;,\n",
       "                                        &#x27;c2&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcccc050640&gt;},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=[&#x27;B-ORG&#x27;, &#x27;B-MISC&#x27;, &#x27;B-PER&#x27;, &#x27;I-PER&#x27;, &#x27;B-LOC&#x27;, &#x27;I-MISC&#x27;, &#x27;I-ORG&#x27;, &#x27;I-LOC&#x27;]),\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm=&#x27;lbfgs&#x27;,\n",
       "                                 all_possible_transitions=True,\n",
       "                                 keep_tempfiles=True, max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={&#x27;c1&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fccf4b8a070&gt;,\n",
       "                                        &#x27;c2&#x27;: &lt;scipy.stats._distn_infrastructure.rv_frozen object at 0x7fcccc050640&gt;},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=[&#x27;B-ORG&#x27;, &#x27;B-MISC&#x27;, &#x27;B-PER&#x27;, &#x27;I-PER&#x27;, &#x27;B-LOC&#x27;, &#x27;I-MISC&#x27;, &#x27;I-ORG&#x27;, &#x27;I-LOC&#x27;]),\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, keep_tempfiles=True,\n",
       "    max_iterations=100)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CRF</label><div class=\"sk-toggleable__content\"><pre>CRF(algorithm=&#x27;lbfgs&#x27;, all_possible_transitions=True, keep_tempfiles=True,\n",
       "    max_iterations=100)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm='lbfgs',\n",
       "                                 all_possible_transitions=True,\n",
       "                                 keep_tempfiles=True, max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object>},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-MISC', 'I-ORG', 'I-LOC']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "import scipy\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "crf = crfsuite.CRF(\n",
    "  algorithm='lbfgs',\n",
    "  max_iterations=100,\n",
    "  all_possible_transitions=True,\n",
    "  keep_tempfiles=True\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runs now, since we installed a venv conda py38! But it uses scikit-learn 1.1.1 Duh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.011442612624257335, 'c2': 0.010103491512999968}\n",
      "best CV score: 0.7538097172556367\n",
      "model size: 2.13M\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.849     0.858     0.853       774\n",
      "       I-LOC      0.389     0.571     0.463        49\n",
      "      B-MISC      0.836     0.613     0.707      1187\n",
      "      I-MISC      0.658     0.417     0.510       410\n",
      "       B-ORG      0.827     0.724     0.772       882\n",
      "       I-ORG      0.798     0.652     0.717       551\n",
      "       B-PER      0.827     0.898     0.861      1098\n",
      "       I-PER      0.880     0.965     0.921       807\n",
      "\n",
      "   micro avg      0.824     0.756     0.789      5758\n",
      "   macro avg      0.758     0.712     0.726      5758\n",
      "weighted avg      0.821     0.756     0.781      5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "best_crf = rs.best_estimator_\n",
    "y_pred = best_crf.predict(X_test)\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
