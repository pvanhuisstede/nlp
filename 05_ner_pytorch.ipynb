{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "#|default_exp ner_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER with PyTorch\n",
    "(follows: )\n",
    "Use DL (neural network) for sequence labelling tasks: POS tagging or NER."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same, Dutch, datasets from NLTK we used in the previous notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('De', 'Art', 'O'),\n",
       "  ('tekst', 'N', 'O'),\n",
       "  ('van', 'Prep', 'O'),\n",
       "  ('het', 'Art', 'O'),\n",
       "  ('arrest', 'N', 'O'),\n",
       "  ('is', 'V', 'O'),\n",
       "  ('nog', 'Adv', 'O'),\n",
       "  ('niet', 'Adv', 'O'),\n",
       "  ('schriftelijk', 'Adj', 'O'),\n",
       "  ('beschikbaar', 'Adj', 'O'),\n",
       "  ('maar', 'Conj', 'O'),\n",
       "  ('het', 'Art', 'O'),\n",
       "  ('bericht', 'N', 'O'),\n",
       "  ('werd', 'V', 'O'),\n",
       "  ('alvast', 'Adv', 'O'),\n",
       "  ('bekendgemaakt', 'V', 'O'),\n",
       "  ('door', 'Prep', 'O'),\n",
       "  ('een', 'Art', 'O'),\n",
       "  ('communicatiebureau', 'N', 'O'),\n",
       "  ('dat', 'Conj', 'O'),\n",
       "  ('Floralux', 'N', 'B-ORG'),\n",
       "  ('inhuurde', 'V', 'O'),\n",
       "  ('.', 'Punc', 'O')],\n",
       " [('In', 'Prep', 'O'),\n",
       "  (\"'81\", 'Num', 'O'),\n",
       "  ('regulariseert', 'V', 'O'),\n",
       "  ('de', 'Art', 'O'),\n",
       "  ('toenmalige', 'Adj', 'O'),\n",
       "  ('Vlaamse', 'Adj', 'B-MISC'),\n",
       "  ('regering', 'N', 'O'),\n",
       "  ('de', 'Art', 'O'),\n",
       "  ('toestand', 'N', 'O'),\n",
       "  ('met', 'Prep', 'O'),\n",
       "  ('een', 'Art', 'O'),\n",
       "  ('BPA', 'N', 'B-MISC'),\n",
       "  ('dat', 'Pron', 'O'),\n",
       "  ('het', 'Art', 'O'),\n",
       "  ('bedrijf', 'N', 'O'),\n",
       "  ('op', 'Prep', 'O'),\n",
       "  ('eigen', 'Pron', 'O'),\n",
       "  ('kosten', 'N', 'O'),\n",
       "  ('heeft', 'V', 'O'),\n",
       "  ('laten', 'V', 'O'),\n",
       "  ('opstellen', 'V', 'O'),\n",
       "  ('.', 'Punc', 'O')],\n",
       " [('publicatie', 'N', 'O')]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "train_sents = list(nltk.corpus.conll2002.iob_sents('ned.train'))\n",
    "dev_sents = list(nltk.corpus.conll2002.iob_sents('ned.testa'))\n",
    "test_sents = list(nltk.corpus.conll2002.iob_sents('ned.testb'))\n",
    "\n",
    "train_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have to pre-process the data. We can use `torchtext` for this. It is a Python library for pre-processing of natural language. We want our data to be a dataset that consists of examples.\n",
    "\n",
    "Each example has two fields: A text filed and a label field. Both consist of sequential information: Tokens and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.8.1\n",
      "  Downloading torchtext-0.8.1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 22.5 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from torchtext==0.8.1) (4.64.0)\n",
      "Collecting torch==1.7.1\n",
      "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[2K     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 776.8/776.8 MB 6.8 MB/s eta 0:00:00\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from torchtext==0.8.1) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from torchtext==0.8.1) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from torch==1.7.1->torchtext==0.8.1) (4.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/peter/anaconda3/envs/py38/lib/python3.8/site-packages (from requests->torchtext==0.8.1) (2.0.4)\n",
      "Installing collected packages: torch, torchtext\n",
      "Successfully installed torch-1.7.1 torchtext-0.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U torchtext==0.8.1 # Also installed torch-1.7.1, which meant uninstalling torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': <torchtext.data.field.Field object>, 'text': <torchtext.data.field.Field object>}\n",
      "['De', 'tekst', 'van', 'het', 'arrest', 'is', 'nog', 'niet', 'schriftelijk', 'beschikbaar', 'maar', 'het', 'bericht', 'werd', 'alvast', 'bekendgemaakt', 'door', 'een', 'communicatiebureau', 'dat', 'Floralux', 'inhuurde', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O']\n",
      "Train: 15806\n",
      "Dev: 2895\n",
      "Test: 5195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/example.py:52: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Example\n",
    "from torchtext.data import Field, Dataset\n",
    "\n",
    "text_field = Field(sequential=True, tokenize=lambda x:x, include_lengths=True)\n",
    "label_field = Field(sequential=True, tokenize=lambda x:x, is_target=True)\n",
    "\n",
    "def read_data(sentences):\n",
    "  examples = []\n",
    "  fields = {'sentence_labels': ('labels', label_field),\n",
    "            'sentence_tokens': ('text', text_field)}\n",
    "\n",
    "  for sentence in sentences:\n",
    "    tokens = [t[0] for t in sentence]\n",
    "    labels = [t[2] for t in sentence]\n",
    "\n",
    "    e = Example.fromdict({\"sentence_labels\": labels, \"sentence_tokens\": tokens},\n",
    "                          fields=fields)\n",
    "    examples.append(e)\n",
    "  return Dataset(examples, fields=[('labels', label_field), ('text', text_field)])\n",
    "\n",
    "train_data = read_data(train_sents)\n",
    "dev_data = read_data(dev_sents)\n",
    "test_data = read_data(test_sents)\n",
    "\n",
    "# Get some grip on what is being done\n",
    "print(train_data.fields)\n",
    "print(train_data[0].text)\n",
    "print(train_data[0].labels)\n",
    "\n",
    "print(\"Train:\", len(train_data))\n",
    "print(\"Dev:\", len(dev_data))\n",
    "print(\"Test:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we build a vocabulary for both fields. The vocabulary will allow us to map every word and label to their index. One index is kept for unknown words, another for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20_000\n",
    "\n",
    "text_field.build_vocab(train_data, max_size=VOCAB_SIZE)\n",
    "label_field.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device='cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BucketIterator` takes care of a lot of details for our training runs:\n",
    "\n",
    "- creates batches of similar length examples in the data\n",
    "- maps the words and labels to the correct indices in their vocabularies\n",
    "- pads the sentences in order to have sentences of the same length (using minimal padding; see first bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_iter = BucketIterator(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "dev_iter = BucketIterator(dataset=dev_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "test_iter = BucketIterator(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained embeddings\n",
    "\n",
    "Pre-trained embeddings are useful to improve the performance of a model, especially if there is little training data. We use \"meaning\" from a larger dataset to sharpen our model: Better generalize between semantically related words.\n",
    "\n",
    "For this we use **FastText** embeddings:\n",
    "\n",
    "- download a `vec` file with the embeddings that we use to initialize our embedding matrix\n",
    "- we create a matrix filled with zeros of which the number of rows == the number of words in our vocabulary; the number of cols == the number of FasText vectors (300)\n",
    "- we must be sure that the FastText embedding for a particular word is in the correct row: The row whose index corresponds to the index of the word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained embeddings\n",
      "Initializing embedding matrix\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "EMBEDDING_PATH = os.path.join(os.path.expanduser('~'), \"Documents/data/nlp/cc.nl.300.vec\")\n",
    "\n",
    "def load_embeddings(path):\n",
    "  \"\"\"Load the FastText embeddings from the NL embeddings file.\"\"\"\n",
    "  print('Loading the pre-trained embeddings')\n",
    "\n",
    "  embeddings = {}\n",
    "  with open(path) as i:\n",
    "    for line in i:\n",
    "      if len(line) > 2:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = np.array(line[1:])\n",
    "        embeddings[word] = embedding\n",
    "  return embeddings\n",
    "\n",
    "def initialize_embeddings(embeddings, vocabulary):\n",
    "  \"\"\"Use the pre-trained embeddings to initialize an embedding matrix.\"\"\"\n",
    "  print('Initializing embedding matrix')\n",
    "  embedding_size = len(embeddings['.'])\n",
    "  embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n",
    "\n",
    "  for idx, word in enumerate(vocabulary.itos):\n",
    "    if word in embeddings:\n",
    "      embedding_matrix[idx,:] = embeddings[word]\n",
    "\n",
    "  return embedding_matrix\n",
    "\n",
    "embeddings = load_embeddings(EMBEDDING_PATH)\n",
    "embedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\n",
    "embedding_matrix = torch.from_numpy(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveat\n",
    "\n",
    "Using Python 3.9 with somewhat older PyTorch and torchtext libraries on GPU caused Jupyter Kernel crash due to incompatible GPU specs. Updated to newest PyTorch & torchtext for GPU specs did crash the Notebook environment.\n",
    "\n",
    "Then ran the code on Python 3.8. Explicitely setting torchtext to 0.81 and torch to 1.17 and running on the CPU. Seem stable.\n",
    "\n",
    "I do not think we will need to build our own CNN's. First of all, because some of the software we are working with already has this capacity build in (spaCy: Retraining NER entities), but, more importantly, we concentrate on **supervised ML** in well described (smaller) doamins instead of all out **deep learning**.\n",
    "\n",
    "If we were to dive in DL we must make sure that all these layers are stable over a longer period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the BiLSTM model\n",
    "\n",
    "The BiLSTM model consists of 4 layers:\n",
    "\n",
    "1. An embedding layer that maps one-hot vectors to dense word embeddings (pre-trained or trained from scratch);\n",
    "2. A bi-directional LSTM layer that reads text b2f and f2b. For each word 2 output vectors are produced `hidden_dim` that are concatenated in a vector `2*hidden_dim`;\n",
    "3. A dropout layer that helps to prevent overfitting by dropping a certain percentage of the items in the LSTM output;\n",
    "4. A dense layer that projects the LSTM output to an output vector with a dimensionality == number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n",
    "        super(BiLSTMTagger, self).__init__()\n",
    "        \n",
    "        # 1. Embedding Layer\n",
    "        if embeddings is None:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "        \n",
    "        # 2. LSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1)\n",
    "        \n",
    "        # 3. Optional dropout layer\n",
    "        self.dropout_layer = nn.Dropout(p=0.5)\n",
    "\n",
    "        # 4. Dense Layer\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, batch_text, batch_lengths):\n",
    "\n",
    "        embeddings = self.embeddings(batch_text)\n",
    "        \n",
    "        packed_seqs = pack_padded_sequence(embeddings, batch_lengths)\n",
    "        lstm_output, _ = self.lstm(packed_seqs)\n",
    "        lstm_output, _ = pad_packed_sequence(lstm_output)\n",
    "        lstm_output = self.dropout_layer(lstm_output)\n",
    "        \n",
    "        logits = self.hidden2tag(lstm_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our model, we need to train it. For that we need to make a couple of decisions:\n",
    "\n",
    "- we pick a loss function (aka `criterion`) to quantify how far the model predictions are from the correct output. In the context of NER this often is `CrossEntropyLoss`;\n",
    "- we need to choose an optimizer. In NLP: Adam optimizer (SGD variation).\n",
    "\n",
    "Then the actual training starts. This happens in several epochs. During each epoch, we show all of the training data to the network, in the batches produced by the BucketIterators we created above. Before we show the model a new batch, we set the gradients of the model to zero to avoid accumulating gradients across batches. Then we let the model make its predictions for the batch. We do this by taking the output, and finding out what label received the highest score, using the torch.max method. We then compute the loss with respect to the correct labels. loss.backward() then computes the gradients for all model parameters; optimizer.step() performs an optimization step.\n",
    "\n",
    "When we have shown all the training data in an epoch, we perform the precision, recall and F-score on the training data and development data. Note that we compute the loss for the development data, but we do not optimize the model with it. Whenever the F-score on the development data is better than before, we save the model. If the F-score is lower than the minimum F-score we've seen in the past few epochs (we call this number the patience), we stop training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "\n",
    "def remove_predictions_for_masked_items(predicted_labels, correct_labels): \n",
    "\n",
    "    predicted_labels_without_mask = []\n",
    "    correct_labels_without_mask = []\n",
    "        \n",
    "    for p, c in zip(predicted_labels, correct_labels):\n",
    "        if c > 1:\n",
    "            predicted_labels_without_mask.append(p)\n",
    "            correct_labels_without_mask.append(c)\n",
    "            \n",
    "    return predicted_labels_without_mask, correct_labels_without_mask\n",
    "\n",
    "\n",
    "def train(model, train_iter, dev_iter, batch_size, max_epochs, num_batches, patience, output_path):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=1)  # we mask the <pad> labels\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_f_score_history = []\n",
    "    dev_f_score_history = []\n",
    "    no_improvement = 0\n",
    "    for epoch in range(max_epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "            \n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size*text_length)\n",
    "            \n",
    "            loss = criterion(pred, gold)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            \n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        train_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "        train_f_score_history.append(train_scores[2])\n",
    "            \n",
    "        print(\"Total training loss:\", total_loss)\n",
    "        print(\"Training performance:\", train_scores)\n",
    "        \n",
    "        total_loss = 0\n",
    "        predictions, correct = [], []\n",
    "        for batch in dev_iter:\n",
    "\n",
    "            text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "            gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "            loss = criterion(pred, gold)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_indices = torch.max(pred, 1)\n",
    "            predicted_labels = list(pred_indices.cpu().numpy())\n",
    "            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "            \n",
    "            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                                   correct_labels)\n",
    "            \n",
    "            predictions += predicted_labels\n",
    "            correct += correct_labels\n",
    "\n",
    "        dev_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n",
    "            \n",
    "        print(\"Total development loss:\", total_loss)\n",
    "        print(\"Development performance:\", dev_scores)\n",
    "        \n",
    "        dev_f = dev_scores[2]\n",
    "        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n",
    "            no_improvement += 1\n",
    "\n",
    "        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n",
    "            print(\"Saving model.\")\n",
    "            torch.save(model, output_path)\n",
    "            no_improvement = 0\n",
    "            \n",
    "        if no_improvement > patience:\n",
    "            print(\"Development F-score does not improve anymore. Stop training.\")\n",
    "            dev_f_score_history.append(dev_f)\n",
    "            break\n",
    "            \n",
    "        dev_f_score_history.append(dev_f)\n",
    "        \n",
    "    return train_f_score_history, dev_f_score_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare to test the model, taking the same steps as above:\n",
    "\n",
    "- get the predictions;\n",
    "- remove the masked items;\n",
    "- print a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_iter, batch_size, labels, target_names): \n",
    "    \n",
    "    total_loss = 0\n",
    "    predictions, correct = [], []\n",
    "    for batch in test_iter:\n",
    "\n",
    "        text_length, cur_batch_size = batch.text[0].shape\n",
    "\n",
    "        pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n",
    "        gold = batch.labels.to(device).view(cur_batch_size * text_length)\n",
    "\n",
    "        _, pred_indices = torch.max(pred, 1)\n",
    "        predicted_labels = list(pred_indices.cpu().numpy())\n",
    "        correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n",
    "\n",
    "        predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n",
    "                                                                               correct_labels)\n",
    "\n",
    "        predictions += predicted_labels\n",
    "        correct += correct_labels\n",
    "    \n",
    "    print(classification_report(correct, predictions, labels=labels, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all these steps, we can start to actually train the model:\n",
    "\n",
    "- set embedding dimension to 300 (FastText dimensionality);\n",
    "- pick a hidden dimensionality for each component of the BiLSTM (outputs 512-dim vectors);\n",
    "- the number of classes (== length of the vocabulary of the label field) will become the dimesionality of the output layer;\n",
    "- compute the number of batches in an epoch in order to show a progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b5805041ff8448d9147bb5e7d973e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 226.21551628410816\n",
      "Training performance: (0.922321904423521, 0.922321904423521, 0.922321904423521, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 25.44461453706026\n",
      "Development performance: (0.9280919149839467, 0.9280919149839467, 0.9280919149839467, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caec748f4f249c99816ac50cc275f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 73.94669454917312\n",
      "Training performance: (0.9578127158958568, 0.9578127158958568, 0.9578127158958567, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 20.331317596137524\n",
      "Development performance: (0.9359195478546979, 0.9359195478546979, 0.9359195478546979, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34736b789151460ba920e42663a191a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 52.222703427542\n",
      "Training performance: (0.9672282426323997, 0.9672282426323997, 0.9672282426323997, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 19.240757752908394\n",
      "Development performance: (0.9393955475362857, 0.9393955475362857, 0.9393955475362857, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94ca34f776944ae8a5f7c51ebc6dd2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 43.15425827750005\n",
      "Training performance: (0.9718965279011469, 0.9718965279011469, 0.9718965279011469, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 17.042858469765633\n",
      "Development performance: (0.9435879746331626, 0.9435879746331626, 0.9435879746331626, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fa67cfa9de47f1a7a7498732fd226b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 36.53407029993832\n",
      "Training performance: (0.9752817749353546, 0.9752817749353546, 0.9752817749353546, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 16.781842106487602\n",
      "Development performance: (0.9441717303048797, 0.9441717303048797, 0.9441717303048797, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10c4c360f36d4ebf87362ac118259882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 31.784735803026706\n",
      "Training performance: (0.9778971990288388, 0.9778971990288388, 0.9778971990288388, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 15.139236290880945\n",
      "Development performance: (0.9482049513094701, 0.9482049513094701, 0.9482049513094701, None)\n",
      "Saving model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab523e9f0d941139b253a9dfb5f0eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 28.689999663620256\n",
      "Training performance: (0.9792394544126646, 0.9792394544126646, 0.9792394544126646, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 16.298973719996866\n",
      "Development performance: (0.9463740812481758, 0.9463740812481758, 0.9463740812481758, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c0973e9adf44e8b9cc18a0bfe63f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 25.47622849655454\n",
      "Training performance: (0.9813219241625708, 0.9813219241625708, 0.9813219241625708, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 16.30899855613825\n",
      "Development performance: (0.9471966460583225, 0.9471966460583225, 0.9471966460583225, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7566da2d04074ba8819fd476b09eb93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 22.8097926521732\n",
      "Training performance: (0.9825062671482995, 0.9825062671482995, 0.9825062671482995, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 15.224460061523132\n",
      "Development performance: (0.9477804017300395, 0.9477804017300395, 0.9477804017300395, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51cf5ceea26430a91fc0cf692ba4878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/494 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training loss: 20.798322341113817\n",
      "Training performance: (0.9842926511517736, 0.9842926511517736, 0.9842926511517736, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total development loss: 16.804522761842236\n",
      "Development performance: (0.9480988139146125, 0.9480988139146125, 0.9480988139146125, None)\n",
      "Development F-score does not improve anymore. Stop training.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = len(label_field.vocab)\n",
    "MAX_EPOCHS = 50\n",
    "PATIENCE = 3\n",
    "OUTPUT_PATH = \"/tmp/bilstmtagger\"\n",
    "num_batches = math.ceil(len(train_data) / BATCH_SIZE)\n",
    "\n",
    "tagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES, embeddings=embedding_matrix)  \n",
    "\n",
    "train_f, dev_f = train(tagger.to(device), train_iter, dev_iter, BATCH_SIZE, MAX_EPOCHS, \n",
    "                       num_batches, PATIENCE, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-score does not **improve** anymore. Let's plot the F-scores for our training and development sets. We aim for reaching an optimal F-score on the development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "df = pd.DataFrame({'epochs': range(0,len(train_f)), \n",
    "                  'train_f': train_f, \n",
    "                   'dev_f': dev_f})\n",
    " \n",
    "# multiple line plot\n",
    "plt.plot('epochs', 'train_f', data=df, color='blue', linewidth=2)\n",
    "plt.plot('epochs', 'dev_f', data=df, color='green', linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we test our model on the test data, we have to run its eval() method. This will put the model in eval mode, and deactivate dropout layers and other functionality that is only useful in training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMTagger(\n",
       "  (embeddings): Embedding(20002, 300)\n",
       "  (lstm): LSTM(300, 256, bidirectional=True)\n",
       "  (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  (hidden2tag): Linear(in_features=512, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = torch.load(OUTPUT_PATH)\n",
    "tagger.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to test the model. It's performance is lower than our CRF model (04_ner_crf.ipynb). It is not only the code that grew compared to the earlier CRF model, but also the extra work we still have to do:\n",
    "\n",
    "- make the architecture of the network more complex\n",
    "- optimize the hyperparameters\n",
    "- throw a lot more data at the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.89      0.63      0.73       774\n",
      "       I-LOC       0.53      0.39      0.45        49\n",
      "      B-MISC       0.90      0.44      0.59      1187\n",
      "      I-MISC       0.70      0.18      0.29       410\n",
      "       B-ORG       0.78      0.52      0.62       882\n",
      "       I-ORG       0.71      0.57      0.64       551\n",
      "       B-PER       0.85      0.69      0.76      1098\n",
      "       I-PER       0.95      0.68      0.79       807\n",
      "\n",
      "   micro avg       0.84      0.55      0.67      5758\n",
      "   macro avg       0.79      0.51      0.61      5758\n",
      "weighted avg       0.84      0.55      0.66      5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels = label_field.vocab.itos[3:]\n",
    "labels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\n",
    "label_idxs = [label_field.vocab.stoi[l] for l in labels]\n",
    "\n",
    "test(tagger, test_iter, BATCH_SIZE, labels = label_idxs, target_names = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we've trained a simple bidirectional LSTM for named entity recognition. Far from achieving state-of-the-art performance, our aim was to understand how neural networks can be implemented and trained in PyTorch. To improve our performance, one of the things that is typically done is to add an additional CRF layer to the neural network. This layer helps us optimize the complete label sequence, and not the labels individually. We leave that for future work."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
