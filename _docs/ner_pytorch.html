<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.38">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>nlp - NER with PyTorch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="nlp - NER with PyTorch">
<meta property="og:description" content="We will use the same, Dutch, datasets from NLTK we used in the previous notebook:">
<meta property="og:site-name" content="nlp">
<meta name="twitter:title" content="nlp - NER with PyTorch">
<meta name="twitter:description" content="We will use the same, Dutch, datasets from NLTK we used in the previous notebook:">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">nlp</span>
  </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pvanhuisstede/nlp/"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">NER with PyTorch</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">NLP Telematika tutorials</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word_embeddings.html" class="sidebar-item-text sidebar-link">An introduction to word embeddings</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pretrained_models.html" class="sidebar-item-text sidebar-link">NLP with pre-trained models: spaCy and Stanford NLP</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discovering_topics.html" class="sidebar-item-text sidebar-link">Discovering and Visualizing Topics in Texts</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./spacy_ner.html" class="sidebar-item-text sidebar-link">Updating spaCy’s NER system</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ner_crf.html" class="sidebar-item-text sidebar-link">NER with Conditional Random Fields (CRF)</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ner_pytorch.html" class="sidebar-item-text sidebar-link active">NER with PyTorch</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./text_classification_sklearn.html" class="sidebar-item-text sidebar-link">“Traditional” Text Classification with Scikit-learn</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#read_data" id="toc-read_data" class="nav-link active" data-scroll-target="#read_data">read_data</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#pre-trained-embeddings" id="toc-pre-trained-embeddings" class="nav-link" data-scroll-target="#pre-trained-embeddings">Pre-trained embeddings</a>
  <ul class="collapse">
  <li><a href="#initialize_embeddings" id="toc-initialize_embeddings" class="nav-link" data-scroll-target="#initialize_embeddings">initialize_embeddings</a></li>
  <li><a href="#load_embeddings" id="toc-load_embeddings" class="nav-link" data-scroll-target="#load_embeddings">load_embeddings</a></li>
  </ul></li>
  <li><a href="#caveat" id="toc-caveat" class="nav-link" data-scroll-target="#caveat">Caveat</a></li>
  <li><a href="#create-the-bilstm-model" id="toc-create-the-bilstm-model" class="nav-link" data-scroll-target="#create-the-bilstm-model">Create the BiLSTM model</a>
  <ul class="collapse">
  <li><a href="#bilstmtagger" id="toc-bilstmtagger" class="nav-link" data-scroll-target="#bilstmtagger">BiLSTMTagger</a></li>
  </ul></li>
  <li><a href="#training-1" id="toc-training-1" class="nav-link" data-scroll-target="#training-1">Training</a>
  <ul class="collapse">
  <li><a href="#train" id="toc-train" class="nav-link" data-scroll-target="#train">train</a></li>
  <li><a href="#remove_predictions_for_masked_items" id="toc-remove_predictions_for_masked_items" class="nav-link" data-scroll-target="#remove_predictions_for_masked_items">remove_predictions_for_masked_items</a></li>
  <li><a href="#test" id="toc-test" class="nav-link" data-scroll-target="#test">test</a></li>
  </ul></li>
  <li><a href="#testing-our-model" id="toc-testing-our-model" class="nav-link" data-scroll-target="#testing-our-model">Testing our model</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pvanhuisstede/nlp/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">NER with PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<p>We will use the same, Dutch, datasets from NLTK we used in the previous notebook:</p>
<p>Next, we have to pre-process the data. We can use <code>torchtext</code> for this. It is a Python library for pre-processing of natural language. We want our data to be a dataset that consists of examples.</p>
<p>Each example has two fields: A text filed and a label field. Both consist of sequential information: Tokens and labels.</p>
<hr>
<section id="read_data" class="level3">
<h3 class="anchored" data-anchor-id="read_data">read_data</h3>
<blockquote class="blockquote">
<pre><code> read_data (sentences)</code></pre>
</blockquote>
<p>Then, we build a vocabulary for both fields. The vocabulary will allow us to map every word and label to their index. One index is kept for unknown words, another for padding.</p>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p><code>BucketIterator</code> takes care of a lot of details for our training runs:</p>
<ul>
<li>creates batches of similar length examples in the data</li>
<li>maps the words and labels to the correct indices in their vocabularies</li>
<li>pads the sentences in order to have sentences of the same length (using minimal padding; see first bullet)</li>
</ul>
</section>
<section id="pre-trained-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="pre-trained-embeddings">Pre-trained embeddings</h2>
<p>Pre-trained embeddings are useful to improve the performance of a model, especially if there is little training data. We use “meaning” from a larger dataset to sharpen our model: Better generalize between semantically related words.</p>
<p>For this we use <strong>FastText</strong> embeddings:</p>
<ul>
<li>download a <code>vec</code> file with the embeddings that we use to initialize our embedding matrix</li>
<li>we create a matrix filled with zeros of which the number of rows == the number of words in our vocabulary; the number of cols == the number of FasText vectors (300)</li>
<li>we must be sure that the FastText embedding for a particular word is in the correct row: The row whose index corresponds to the index of the word in the vocabulary.</li>
</ul>
<hr>
<section id="initialize_embeddings" class="level3">
<h3 class="anchored" data-anchor-id="initialize_embeddings">initialize_embeddings</h3>
<blockquote class="blockquote">
<pre><code> initialize_embeddings (embeddings, vocabulary)</code></pre>
</blockquote>
<p>Use the pre-trained embeddings to initialize an embedding matrix.</p>
<hr>
</section>
<section id="load_embeddings" class="level3">
<h3 class="anchored" data-anchor-id="load_embeddings">load_embeddings</h3>
<blockquote class="blockquote">
<pre><code> load_embeddings (path)</code></pre>
</blockquote>
<p>Load the FastText embeddings from the NL embeddings file.</p>
</section>
</section>
<section id="caveat" class="level2">
<h2 class="anchored" data-anchor-id="caveat">Caveat</h2>
<p>Using Python 3.9 with somewhat older PyTorch and torchtext libraries on GPU caused Jupyter Kernel crash due to incompatible GPU specs. Updated to newest PyTorch &amp; torchtext for GPU specs did crash the Notebook environment.</p>
<p>Then ran the code on Python 3.8. Explicitely setting torchtext to 0.81 and torch to 1.17 and running on the CPU. Seem stable.</p>
<p>I do not think we will need to build our own CNN’s. First of all, because some of the software we are working with already has this capacity build in (spaCy: Retraining NER entities), but, more importantly, we concentrate on <strong>supervised ML</strong> in well described (smaller) doamins instead of all out <strong>deep learning</strong>.</p>
<p>If we were to dive in DL we must make sure that all these layers are stable over a longer period.</p>
</section>
<section id="create-the-bilstm-model" class="level2">
<h2 class="anchored" data-anchor-id="create-the-bilstm-model">Create the BiLSTM model</h2>
<p>The BiLSTM model consists of 4 layers:</p>
<ol type="1">
<li>An embedding layer that maps one-hot vectors to dense word embeddings (pre-trained or trained from scratch);</li>
<li>A bi-directional LSTM layer that reads text b2f and f2b. For each word 2 output vectors are produced <code>hidden_dim</code> that are concatenated in a vector <code>2*hidden_dim</code>;</li>
<li>A dropout layer that helps to prevent overfitting by dropping a certain percentage of the items in the LSTM output;</li>
<li>A dense layer that projects the LSTM output to an output vector with a dimensionality == number of labels.</li>
</ol>
<hr>
<section id="bilstmtagger" class="level3">
<h3 class="anchored" data-anchor-id="bilstmtagger">BiLSTMTagger</h3>
<blockquote class="blockquote">
<pre><code> BiLSTMTagger (embedding_dim, hidden_dim, vocab_size, output_size,
               embeddings=None)</code></pre>
</blockquote>
<p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))</code></pre>
<p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool</p>
</section>
</section>
<section id="training-1" class="level2">
<h2 class="anchored" data-anchor-id="training-1">Training</h2>
<p>Now that we have our model, we need to train it. For that we need to make a couple of decisions:</p>
<ul>
<li>we pick a loss function (aka <code>criterion</code>) to quantify how far the model predictions are from the correct output. In the context of NER this often is <code>CrossEntropyLoss</code>;</li>
<li>we need to choose an optimizer. In NLP: Adam optimizer (SGD variation).</li>
</ul>
<p>Then the actual training starts. This happens in several epochs. During each epoch, we show all of the training data to the network, in the batches produced by the BucketIterators we created above. Before we show the model a new batch, we set the gradients of the model to zero to avoid accumulating gradients across batches. Then we let the model make its predictions for the batch. We do this by taking the output, and finding out what label received the highest score, using the torch.max method. We then compute the loss with respect to the correct labels. loss.backward() then computes the gradients for all model parameters; optimizer.step() performs an optimization step.</p>
<p>When we have shown all the training data in an epoch, we perform the precision, recall and F-score on the training data and development data. Note that we compute the loss for the development data, but we do not optimize the model with it. Whenever the F-score on the development data is better than before, we save the model. If the F-score is lower than the minimum F-score we’ve seen in the past few epochs (we call this number the patience), we stop training.</p>
<hr>
<section id="train" class="level3">
<h3 class="anchored" data-anchor-id="train">train</h3>
<blockquote class="blockquote">
<pre><code> train (model, train_iter, dev_iter, batch_size, max_epochs, num_batches,
        patience, output_path)</code></pre>
</blockquote>
<hr>
</section>
<section id="remove_predictions_for_masked_items" class="level3">
<h3 class="anchored" data-anchor-id="remove_predictions_for_masked_items">remove_predictions_for_masked_items</h3>
<blockquote class="blockquote">
<pre><code> remove_predictions_for_masked_items (predicted_labels, correct_labels)</code></pre>
</blockquote>
<p>Let’s prepare to test the model, taking the same steps as above:</p>
<ul>
<li>get the predictions;</li>
<li>remove the masked items;</li>
<li>print a classification report.</li>
</ul>
<hr>
</section>
<section id="test" class="level3">
<h3 class="anchored" data-anchor-id="test">test</h3>
<blockquote class="blockquote">
<pre><code> test (model, test_iter, batch_size, labels, target_names)</code></pre>
</blockquote>
<p>After all these steps, we can start to actually train the model:</p>
<ul>
<li>set embedding dimension to 300 (FastText dimensionality);</li>
<li>pick a hidden dimensionality for each component of the BiLSTM (outputs 512-dim vectors);</li>
<li>the number of classes (== length of the vocabulary of the label field) will become the dimesionality of the output layer;</li>
<li>compute the number of batches in an epoch in order to show a progress bar.</li>
</ul>
<p>F-score does not <strong>improve</strong> anymore. Let’s plot the F-scores for our training and development sets. We aim for reaching an optimal F-score on the development data:</p>
<p>Before we test our model on the test data, we have to run its eval() method. This will put the model in eval mode, and deactivate dropout layers and other functionality that is only useful in training:</p>
<p>Finally we are ready to test the model. It’s performance is lower than our CRF model (04_ner_crf.ipynb). It is not only the code that grew compared to the earlier CRF model, but also the extra work we still have to do:</p>
<ul>
<li>make the architecture of the network more complex</li>
<li>optimize the hyperparameters</li>
<li>throw a lot more data at the model</li>
</ul>
</section>
</section>
<section id="testing-our-model" class="level2">
<h2 class="anchored" data-anchor-id="testing-our-model">Testing our model</h2>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>In this notebook we’ve trained a simple bidirectional LSTM for named entity recognition. Far from achieving state-of-the-art performance, our aim was to understand how neural networks can be implemented and trained in PyTorch. To improve our performance, one of the things that is typically done is to add an additional CRF layer to the neural network. This layer helps us optimize the complete label sequence, and not the labels individually. We leave that for future work.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>