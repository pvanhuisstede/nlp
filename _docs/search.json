[
  {
    "objectID": "word_embeddings.html",
    "href": "word_embeddings.html",
    "title": "An introduction to word embeddings",
    "section": "",
    "text": "Word embeddings are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding. One of the first algorithms that explored these word embeddings was word2vec. We will use this algorithm in this NB. Generating word embeddings we will step into so-called vector space.\nSuppose we have the following text: “Ronaldo, Messi, Dicaprio”.\nWe can use one-hot encoding to give each of the words of this text an unique position:\n\n\n\n–\nisRonaldo\nisMessi\nisDicaprio\n\n\n\n\nRonaldo\n1\n0\n0\n\n\nMessi\n0\n1\n0\n\n\nDicaprio\n0\n0\n1\n\n\n\nThe above encoding is not informative at all, there are no relationships between the words, every word is isolated.\nWe can do better if we use some world knowledge: Two of these persons are soccer players, the other is an actor. So we, manually, create features:\n\n\n\n–\nisSoccer\nisActor\n\n\n\n\nRonaldo\n1\n0\n\n\nMessi\n1\n0\n\n\nDicaprio\n0\n1\n\n\n\nIn our simple two-vector space, we now get:\n————-Messi, Ronaldo—–> isSoccer\n————-Dicaprio———–> isActor\nWe could add a lot more features: Age, gender, height, weight, etc. But that is impossible to do manually. So, can we do this: design features based on our world knoweledge of the relationships between words, with neural nets? Or, phrased differently, can we have neural nets comb through a large corpus of text and automatically generate word representations?\n\n\nFormulated in 2013 (Mikolov et al. https://arxiv.org/abs/1301.3781): Efficient method to learn vector representations of words from large amount of unstructured texts. Based on the idea of distributional semantics: “You shall know a word by the company it keeps” (J.R. Firth, 1957). Or, similar words appear in similar contexts.\nWord2vec representation learning:\n\nContinuous Bag of Words (CBOW): Given neighbouring words, predict the center word\nSkip-gram: Given the center word, predict the neigbouring words\n\nThere is room for improvement here:\n\nOut of vocabulary (OOV) words. w2v can’t handle words that are not in its vocabulary (seen during training).\nMorphology. Use internal structure of words (radicals, lemma’s: eat, eats, eaten, eater, eating) to get better vectors.\n\n\n\n\nBojanowski et al. (https://arxiv.org/abs/1607.04606) proposed a new embedding theory using the two improvement ideas: FastText.\n\nUse the internal structure of a word to improve vector representations obtained by the skip-gram methode:\n1.1 sub-word generation using n-grams via windowing:  ->  (for 3-grams); but because this explodes:\n1.2 hashing n-grams into buckets with an index:  -> 10\nSkip-gram with negative sampling\n\nFastText is really good on syntactic word analogy tasks in morphologically rich languages (German):\n\ncat -> cats\ndog -> ?\ngood -> better\nrough -> ?\n\nNot as good on semantic analogy tasks:\n\nman -> king\nwoman -> ?\n\nFastText is not as fast as word2vec.\nHere we use Gensim and word2vec, although FastText is also available in the Gensim library. We use the abstracts of all arXiv papers in the category cs.CL (CL: Computation and Language) published before mid-April 2021 (c. 25_000 documents). We tokenize the abstracts with spaCy. Note that the texts we work with share a context (Computation and Language)! Each row in the CSV file consists of two columns: 1. title and 2. abstract. We use the abstracts for the construction of our model.\n\n\n\n\n\n Corpus (filename)\n\nInitialize self. See help(type(self)) for accurate signature.\nUsing Gensim we can set a number of parameters for training:\n\nmin_count: the minimum frequency of words in our corpus\nwindow: number of words to the left and right to make up the context that word2vec will take into account\nvector_size: the dimensionality of the word vectors; usually between 100 and 1_000\nsg: One can choose fro 2 algorithms to train word2vec: Skip-gram (sg) tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. Default is sg=0, hence: default is CBOW."
  },
  {
    "objectID": "word_embeddings.html#using-word-embeddings",
    "href": "word_embeddings.html#using-word-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Using word embeddings",
    "text": "Using word embeddings\nWith the model trained, we can access the word embedding via the wv attribute on model using the token as a key. For example the embedding for “nlp” is:\nFind the similarity between two words. We use the cosine between two word embeddings, so we use a ranges between -1 and +1. The higher the cosine, the more similar two words are.\nFind words that are most similar to target words we line up words via the embeddings: semantically related, other types of pre-tained models, related general models, and generally related words:\nLook for words that are similar to something, but dissimilar to something else with this we can look for a kind of analogies:\nSo a related transformer to lstm is rnn, just like bert is a particular type of transformer; really powerful.\nWe can also zoom in on one of the meanings of ambiguous words. In NLP tree has a very specific meaning, is nearest neighbours being: constituency, parse, dependency, and syntax:\nIf we add syntax as a negative input to the query, we see that the ordinary meaning of tree kicks in: Now forest is one of the nearest neighbours.\nThrow a list of words at the model and filter out the odd one (here svm is the only non-neural model):"
  },
  {
    "objectID": "word_embeddings.html#plotting-embeddings",
    "href": "word_embeddings.html#plotting-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Plotting embeddings",
    "text": "Plotting embeddings\nAbout visualizing embeddings. We need to reduce our 100-dimensions space to 2-dimensions. We can use t-SNE method: map similar data to nearby points and dissimilar data to faraway points in low dimensional space.\nt-SNE is present in Scikit-learn. One has to specify two parameters: n_components (number of dimensions) and metric (similarity metric, here: cosine).\nIn order NOT to overcrowd the image we use a subset of embeddings of 200 most similar words based on a target word."
  },
  {
    "objectID": "word_embeddings.html#exploring-hyperparameters",
    "href": "word_embeddings.html#exploring-hyperparameters",
    "title": "An introduction to word embeddings",
    "section": "Exploring hyperparameters",
    "text": "Exploring hyperparameters\nWhat is the quality of the embeddings? Should embeddings capture syntax or semantical relations. Semantic similarity or topical relations?\nOne way of monitoring the quality is to check nearest neighbours: Are they two nouns, two verbs?\n\n\nevaluate\n\n evaluate (model, word2pos)\n\nNow we want to change some of the settings we used above:\n\nembedding size (dimensions of the trained embeddings): 100, 200, 300\ncontext window: 2, 5, 10\n\nWe will use a Pandas dataframe to keep track of the different scores (but this will take time: We train 9 models!!!):\nResults are close:\n\nSmaller contexts seem to yield better results. Which makes sense because we work with the syntax - nearer words often produce more information.\nHigher dimension word embeddings not always work better than lower dimension. Here we have a relatively small corpus, not enough data for such higher dimensions.\n\nLet’s visualize our findings:"
  },
  {
    "objectID": "word_embeddings.html#conclusions",
    "href": "word_embeddings.html#conclusions",
    "title": "An introduction to word embeddings",
    "section": "Conclusions",
    "text": "Conclusions\nWord embeddings allow us to model the usage and meaning of a word, and discover words that behave in a similar way.\nWe move from raw strings -> vector space: word embeddings which allows us to work with words that have a similar meaning and discover new patterns."
  },
  {
    "objectID": "word_embeddings.html#variables-in-this-notebook",
    "href": "word_embeddings.html#variables-in-this-notebook",
    "title": "An introduction to word embeddings",
    "section": "Variables in this Notebook",
    "text": "Variables in this Notebook\n\n\n\n\n\n\n\n\n\n\nName\nType\nCell #\nSize\nValue(s)\n\n\n\n\nacc\nfloat\n20\n–\n0.6350\n\n\ndf\nDataFrame\n20\n(4, 3)\n100 200 300 2 0.688609\n\n\ndocuments\nCorpus\n4\n–\n<__main__.corpus object at …>\n\n\nembeddings\nndarray\n15\n(201, 100)\n[[1.36567 -2.2555 …] […]]\n\n\nmapped_embeddings\nndarray\n15\n(201, 2)\n[[-0.3663 -1.3517] [8.5049 …]]\n\n\nmodel\nWord2Vec\n20\n–\nWord2Vec(vocab=3099, vector_size=300, alpha=0.025)\n\n\nnlp\nEnglish\n17\n–\nspacy.lang.en.English object at …\n\n\nselected_words\nlist\n15\n201\n[‘roberta’, ‘transformer’, ‘elmo’ …]\n\n\ntarget_word\nstr\n15\n4\n‘bert’\n\n\nword\nstr\n18\n7\n‘careful’\n\n\nword2pos\ndict\n17\n3099\n{‘’: ‘SPACE’, ‘the’: ‘PRON’, …}\n\n\nx\nndarray\n16\n(201,)\n[-0.3666572 8.504919 …]\n\n\ny\nndarray\n16\n(201,)\n[-1.3517823 1.9856246 …]"
  },
  {
    "objectID": "pretrained_models.html",
    "href": "pretrained_models.html",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "",
    "text": "By applying the spaCy model we assigned to the variable en. We can generate a processed document wit spaCy, doc_en that has sentences and tokens:\nspaCy also identifies a number of linguistic features for every token: lemma, pos_ (the universal POS tags), and tag_(contains the more finegrained, language-specific POS tags):\nspaCy also offers pre-trained models for NER (Named Entity Recognition). The results can be found on the ent_iob_ and ent_type_ attributes.\nThe ent_type_ attribute informs us about what type of entity the token refers to: ‘Donald Trump’ => person, ‘June 14, 1946’ => date, ‘45th’ => ordinal number, and ‘the United States’ => GPE (Geo Political Entity).\nThe ent_iob_ attribute gives, by way of the letters ‘I,O,B’ the position of the token in the entity, where O means that the token is outside of an entity, B the entity is at the beginning of a token, and I means it is inside a token. So basically the IOB scheme gives you information about begin and parts of entities (positional).\nWe can access the recognized entities directly when we use the ents attribute of the document directly:\nOn top of all this, the spaCy model also has a dependency parser on board that analyzes the grammatical realtions between the tokens:\nWe display the results, kept in the variable syntax, in the usual way:"
  },
  {
    "objectID": "pretrained_models.html#multilingual-nlp",
    "href": "pretrained_models.html#multilingual-nlp",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "Multilingual NLP",
    "text": "Multilingual NLP\nAs can be inferred from the spaCy model we called this model is based on and targeted at the English language.\nOne can use the spaCy website to select models to use for different usecases:\nhttps://spacy.io/usage/models\nBut models for other languages are also available. Let’s try one out on a Dutch text:\nBecause the Dutch model was trained in its particular way, there are differences with the English model.\nThe most important is that the Dutch models do not offer lemmatization, the lemma_ attribute returns the orth_ attribute.\nNB. whenever numbers turn up in the tables that are generated, they refer to the ID’s of tokens in vectorspace. This usually means that we specified the attribute of a token ent_iob without the ending underscore: ent_iob_.\nIf one is working with Dutch texts, then the Python library stanza is the one to use (in the Telematika notebook the stanfordnlp library is used, but this library is not recommended anymore.)\n\n# we ran 'stanza.download('nl') in the terminal\nnl_nlp = stanza.Pipeline('nl', use_gpu=True)\n\n\n\n\n2022-08-30 15:56:49 INFO: Loading these models for language: nl (Dutch):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | alpino  |\n| pos       | alpino  |\n| lemma     | alpino  |\n| depparse  | alpino  |\n| ner       | conll02 |\n=======================\n\n2022-08-30 15:56:49 INFO: Use device: cpu\n2022-08-30 15:56:49 INFO: Loading: tokenize\n2022-08-30 15:56:49 INFO: Loading: pos\n2022-08-30 15:56:50 INFO: Loading: lemma\n2022-08-30 15:56:50 INFO: Loading: depparse\n2022-08-30 15:56:50 INFO: Loading: ner\n2022-08-30 15:56:50 INFO: Done loading processors!\n\n\n\ndoc_nl_stanza = nl_nlp(text_nl)\n\nGot it working with the GPU. The thing is that the PyTorch stuff should be installed via Conda (not Pip) and that this is completely independent of the system76 CUDA stuff. The trick is to throw away all Pip installed stuff (torch, torchvision, torchaudio) and use the following conda command to install an cuda ensemble: ‘conda install pytorch torchvision torchaudio cudatoolkit=11.2 -c pytorch -c nvidia’.\nWe now have access, via the model, to text and lemma, but also to the attributes upos, xpos, govenor, and dependency_relation.\n\nstanza_info = []\nfor sentence in doc_nl_stanza.sentences:\n  for word in sentence.words:\n    stanza_info.append((len(stanza_info)+1, word.text, word.lemma, word.pos, word.upos, word.xpos, word.deprel))\n\n\ndisplay_html(tabulate.tabulate(stanza_info, tablefmt='html'))\n\n\n\n\n 1Mark              Mark              PROPNPROPNSPEC|deeleigen                   nsubj\n 2Rutte             Rutte             PROPNPROPNSPEC|deeleigen                   flat \n 3is                zijn              AUX  AUX  WW|pv|tgw|ev                     cop  \n 4minister-presidentminister_presidentNOUN NOUN N|soort|ev|basis|zijd|stan       root \n 5van               van               ADP  ADP  VZ|init                          case \n 6Nederland         Nederland         PROPNPROPNN|eigen|ev|basis|onz|stan        nmod \n 7.                 .                 PUNCTPUNCTLET                              punct\n 8Hij               hij               PRON PRON VNW|pers|pron|nomin|vol|3|ev|mascnsubj\n 9is                zijn              AUX  AUX  WW|pv|tgw|ev                     root \n10van               van               ADP  ADP  VZ|init                          case \n11de                de                DET  DET  LID|bep|stan|rest                det  \n12VVD               VVD               PROPNPROPNN|eigen|ev|basis|zijd|stan       obl  \n13en                en                CCONJCCONJVG|neven                         cc   \n14heeft             hebben            VERB VERB WW|pv|tgw|met-t                  conj \n15een               een               DET  DET  LID|onbep|stan|agr               det  \n16slecht            slecht            ADJ  ADJ  ADJ|prenom|basis|zonder          amod \n17geheugen          geheug            NOUN NOUN N|soort|ev|basis|onz|stan        obj  \n18.                 .                 PUNCTPUNCTLET                              punct\n\n\n\n\n\nCombining spaCy and Stanza\nThanks to the spacy-stanza wrapper we can combine the 2 libraries in pipelines. First we install spacy_stanza with Pip.\n\nnlp_spacy_stanza = spacy_stanza.load_pipeline('nl', use_gpu=True)\n\n\n\n\n2022-08-30 16:02:21 INFO: Loading these models for language: nl (Dutch):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | alpino  |\n| pos       | alpino  |\n| lemma     | alpino  |\n| depparse  | alpino  |\n| ner       | conll02 |\n=======================\n\n2022-08-30 16:02:21 INFO: Use device: cpu\n2022-08-30 16:02:21 INFO: Loading: tokenize\n2022-08-30 16:02:21 INFO: Loading: pos\n2022-08-30 16:02:21 INFO: Loading: lemma\n2022-08-30 16:02:21 INFO: Loading: depparse\n2022-08-30 16:02:21 INFO: Loading: ner\n2022-08-30 16:02:22 INFO: Done loading processors!\n\n\n\ndoc_nlp_spacy_stanza = nlp_spacy_stanza(\"Mark Rutte is minister-president van Nederland.\" \"Hij is van de VVD en heeft een slecht actief geheugen.\")\nfor token in doc_nlp_spacy_stanza:\n  print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\nprint(doc_nlp_spacy_stanza.ents)\n\nMark Mark PROPN nsubj PER\nRutte Rutte PROPN flat PER\nis zijn AUX cop \nminister-president minister_president NOUN root \nvan van ADP case \nNederland Nederland PROPN nmod LOC\n. . PUNCT punct \nHij hij PRON nsubj \nis zijn AUX root \nvan van ADP case \nde de DET det \nVVD VVD PROPN obl ORG\nen en CCONJ cc \nheeft hebben VERB conj \neen een DET det \nslecht slecht ADJ advmod \nactief actief ADJ amod \ngeheugen geheug NOUN obj \n. . PUNCT punct \n(Mark Rutte, Nederland, VVD)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Telematika tutorials",
    "section": "",
    "text": "All notebooks run under Python 3.9.12.\n\nNLP 101:\n1.1 00_word_embeddings.ipynb (An introduction to word embeddings)\n1.2 01_pretrained_models.ipynb (NLP with pre-trained models from spaCy and Stanza)\n1.3 02_discovering_topics.ipynb (Discovering and Visualizing Topics in Texts with LDA)\nNamed Entity Recognition (NER):\n2.1 03_spacy_ner.ipynb (Using spaCy’s Named Entity Recognition System)\n2.2 04_ner_crf.ipynb (Named Entity Recognition (NER) with Conditional Random Fields (CRF))"
  },
  {
    "objectID": "pretrained_models.html#variables-in-this-notebook",
    "href": "pretrained_models.html#variables-in-this-notebook",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "Variables in this Notebook",
    "text": "Variables in this Notebook\n\n\n\n\n\n\n\n\n\n\nName\nType\nCell #\nSize\nValue(s)\n\n\n\n\ndoc_en\nDoc\n\n34\nDonald John Trump …\n\n\ndoc_nl\nDoc\n\n18\nMark Rutte is …\n\n\ndoc_nl_stanza\nDocument\n\n\n[[ {“id”: 1, “text”: “Mark”, }]]\n\n\ndoc_nlp_spacy_stanza\nDoc\n\n19\nMark Rutte is …\n\n\nen\nEnglish\n\n\n<spacy.lang.en.English …>\n\n\nentities\nlist\n\n34\n[(‘Donald’, ‘B’, ‘PERSON’), (‘John’, ‘I’, ‘PERSON’), ]\n\n\nfeatures\nlist\n\n34\n[[‘Donald’, ‘Donald’, ‘PROPN’, ‘NNP’], ]\n\n\ninfo\nlist\n\n18\n[(‘Mark’, ‘PROPN’, ’SPEC\n\n\nnl\nDutch\n\n\n<spacy.lang.nl.Dutch object at >\n\n\nnl_nlp\nPipeline\n\n\n<stanza.pipeline.core >\n\n\nnlp_spacy_stanza\nDutch\n\n\n<spacy.lang.nl.Dutch object at >\n\n\nsentence\nSentence\n\n\n[{“id”: 1, “text”: “Hij”, “lemma”: “hij”, “upos”: …}]\n\n\nstanza_info\nlist\n\n18\n[(1, ‘Mark’, ‘Mark’, ‘PROPN’, …), ]\n\n\nsyntax\nlist\n\n34\n[[‘Donald’, ‘compound’, ‘Trump’], …]\n\n\ntext\nstr\n\n169\n’Donald John Trump …\n\n\ntext_nl\nstr\n\n94\n‘Mark Rutte is de …’\n\n\ntoken\nToken\n\n1\n\n\n\ntokens\nlist\n\n34\n[[Donald], [John], …]\n\n\nword\nWord\n\n\n{‘id’: 11, ‘text’: “.”, “lemma”: …, }"
  },
  {
    "objectID": "discovering_topics.html",
    "href": "discovering_topics.html",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "",
    "text": "Often texts are just that: texts without metadata and labels that tell us what the texts are about. We can use unsupervised ML, topic models, in such cases to find out about the topics discussed in the texts.\nTopics: Groups of related words that often occur together in texts. Topic models can find clusters of related words. The humans interpret these clusters and assign them labels. So, a “natural workflow” seems to present itself: Contextualize: Group publications according to a context (person, affiliation, research topic), then use “unsupervised ML to get at topics. After that humans can interpret these topics and assign them labels (thesaurus-like broader terms). With these assigned labels (supervised ML) we can optimize training sets.\nPopular topic model: Latent Dirichlet Allocation (LDA). It uses a prior distribution topics in a text will have (Dirichlet probability distribution). LDA is often used to model open-ended survey questions.\nHere we will use the data from the Grand Debat Nationale in France.\nThe structure of the CSV file is as follows:\nWe will focus on the contents of the last, open question of the questionnaire:"
  },
  {
    "objectID": "discovering_topics.html#preprocessing",
    "href": "discovering_topics.html#preprocessing",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Preprocessing",
    "text": "Preprocessing\nBefore we can train a model, we need to tokenize the texts. For this we use the spaCy NLP library. The author uses a blank model (does not work anymore).\nThe are 4 NaN’s in the first 10 answers, so we throw these out and keep all the texts in the target column.\nNext we use spaCy to perform the first pre-processing pass:\nNow we have a list of spaCy documents that we need to transform into a list of tokens. We will work with lemmatized tokens in order to be able to work with the lemmas. So, these are the following pre-processing steps:\n\nremove all words < 3 characters (interesting for sentiment analysis, but no so much for topic analysis)\ndrop all stopwords\ntake the lemmas of all remaining words and lowercase them\n\ndocs is a list of lists. The lists contain the lemmas of the answers of the survey participants.\nBut we want to take frequent bigrams into account when topic modelling. In tge French language they often carry important meaning (“poids lourds” = “trucks”).\nFor this we use the Python Gensim library:\n\nidentify frequent bigrams in the corpus\nappend these to the list of tokens for the documents in which they appear\n\nLets have a look at the fifth document:\nPerfect, we have found two frequently used (over the corpus) in this particular document of the corpus.\nNext, the final Gensim-specific pre-processing steps:\n\ncreate a dictionary representation of the documents; the dictionary will map each word to an unique ID so that we can make BoW representations of each document. The dictionary will contain ids of words in documents and their frequency;\nwe can remove the least and most frequent words from the vocabulary (faster, better quality). We express the min freq as an absolute number, the max freq is the proportion of documents a word is allowed to occur in:\n\nNext, we create bag-of-word (BoW) representations for each of our documents in the corpus:"
  },
  {
    "objectID": "discovering_topics.html#training",
    "href": "discovering_topics.html#training",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Training",
    "text": "Training"
  },
  {
    "objectID": "discovering_topics.html#results",
    "href": "discovering_topics.html#results",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Results",
    "text": "Results\nWhat did the model learn? We start by printing out the 10 words that were most characteristic for each of the topics. Some of the topics are general, but others more precise:\nSome interesting topics:\n\nagriculture (topic 1)\nvehicles (topic 2)\nenergy (topic 4)\nwaste and recycling (topic 8)\ntax incentives (topic 9)\n\nLet’s check the topics the model assigns to some individual documents. LDA assigns a high probability to a low number of topics for each document:"
  },
  {
    "objectID": "discovering_topics.html#variables-in-this-notebook",
    "href": "discovering_topics.html#variables-in-this-notebook",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Variables in this Notebook",
    "text": "Variables in this Notebook\n\n\n\n\n\n\n\n\n\n\nName\nType\nCell #\nSize\nValue(s)\n\n\n\n\ndf\nDataFrame\n\n(153809, 27)\nid reference  0 UHJ\n\n\ndictionary\nDictionary\n\n32718\n[‘centrale’, ‘geothermique’, …]\n\n\nidx\nint\n\n\n110562\n\n\nquestion\nstr\n\n120\n’QUCX … Y a-t-il d’autres …\n\n\nspacy_docs\nlist\n\n110563\n[Multiplier les centrales geothermiques, Les …]\n\n\ntext\nstr\n\n448\n’Il faut utiliser TOUS les …]\n\n\ntexts\nSeries\n\n(110563,)\n0 Multiplier les centrales …\n\n\ntoken\nstr\n\n7\n‘periode’\n\n\ntopic\nint\n\n\n9\n\n\nwords\nstr\n\n175\n‘0.024“ville” + 0.020”zone” + …’"
  },
  {
    "objectID": "spacy_ner.html",
    "href": "spacy_ner.html",
    "title": "Updating spaCy’s NER system",
    "section": "",
    "text": "Although pre-trained models are simple to use, we just have to plug them in, results will be disappointing when the data we work with differs, even slightly, from the data the model was trained on.\nSo, we want to be able to train our own model. SpaCy has us covered:\nThis second option aligns a bit more with our views: Context is king. We start with a contextualized dataset (persons, affiliations, topics together with the content of publications), start with unsupervised ML. Then use the output TOGETHER with the context of the questions we want to answer to use supervised ML (data) to make the models better.\nLet’s look at a toy example:\nAlthough the spaCy NER is actually quite good, we want to train the model some more with extra training data. So that a word like “Brexit” for example is properly recognized (Brexit is now labelled as PERSON). For this we do not use the actual sentence itself, too easy. But we will use similar sentences. Here we will use just a couple of sentences from Wikipedia.\nBelow are the 18 NER labels that spaCy uses:\nSo, we have this existing pre-trained spaCy model that we want to update with some new examples (ideally these should be around 200-300 examples).\nThese examples should be presented to spaCy as a list of tuples, that contain the text, and a dictionary of tuples, named entities that contains: the start and end indices of the named entity in the text, and the label of that named entity:\nBefore we set up the NER pipeline with the content of our training data, we make sure that we got the indices right:\nTo set thing up, let’s check if we have a NER in our pipeline:\nThat looks OK, now we assign the NER to a variable:\nNext step is to add these labels to the NER:\nNow we can start training, but only for the NER component of the pipeline, hence the following code snippet:\nIn order to properly train the NER model, we need to:\nLet’s check how our updated NER model now performs, using our earlier sentence:\nBetter, but we trained a little bit on the subject which is precisely why we, the RePubXL team, propose supervised ML on smaller contextual datasets. The power of these NER updates is that, based on the examples, the model can still generalize due to the word-embeddings vectorspace.\nNow, we want to keep our updated model for future use:\nIn the cells above we started with a pre-trained model. One can also choose to start with an empty model, using spacy.blank(), passing in the “en” argument for the English language. Because it is an empty model, we have to add this ner to the pipeline using add_pipe(). We do not have to disable other pipelines, as we are just adding a new one, not changing an existing one, and just that one and not the other parts of the pipeline.\nOne does have to use a large(r) number of training cases.\nJust a small example below:\nWe can use our new model to get more info and train it:"
  },
  {
    "objectID": "spacy_ner.html#training-a-completely-new-entity-type-in-spacy",
    "href": "spacy_ner.html#training-a-completely-new-entity-type-in-spacy",
    "title": "Updating spaCy’s NER system",
    "section": "Training a completely new entity type in spaCy",
    "text": "Training a completely new entity type in spaCy\nAll code above was directed at training the ner to categorize correctly, either adjusting a pre-trained model or starting from a new blank model and adjusting that as one goes.\nBut what to do if you want to work with a category that is NOT defined?\nWe have to train the model:\n\nfirst add the new label with ner.add_label()\nResume training\nSelect the pipes to be trained\nSingle out the pipes NOT to be trained\n\nWith the training complete, let’s test our ner:"
  },
  {
    "objectID": "ner_crf.html",
    "href": "ner_crf.html",
    "title": "NER with Conditional Random Fields (CRF)",
    "section": "",
    "text": "This notebook is about sequence labelling. CRF is a powerful technique dating from before Deep Learning became popular. The goal is to label each word in a text with a word class. POS tagging takes care of parts of speech (POS) tagging or labelling. NER relates words to more generic named entities: Names, places, organizations, etc. But, depending on the context of the research, there might be more specialized entities (i.e. diseases, symptomes in health, etc.) in a text that need labelling in order to get the most important information out for analytics, search or matching applications.\nFor this we use sklearn-crfsuite for Conditional Random Fields (CRF): Sequence labelling. sklearn-crfsuite is a wrapper around python-crfsuite, which is a Python binding of CRFSuite. The reason to use sklearn-crfsuite are its handy utility functions for evaluating the output of the model.\nI guess, due to the upsurge of Deep Learning the sklearn-crfsuite is not updated anymore. So instead of installing the library with ‘pip install sklearn-crfsuite’, we should use a patched version of the library in order to generate the evaluation reports.\nAs an alternative we could use a variant library: spaCy-crfsuite.\nData comes from NLTK. It is the CoNLL-2002 data. Spanish and Dutch texts labellled with 4 types of entities: locations (LOC), persons (PER), organizations (ORG), and miscellaneous entities (MISC). The data is split up in three chunks: train_sents and two little test chunks: dev_sents and test_sents.\nLet’s have a look at the data. They are a list of tokenized sentences: the string, POS tag and it’s entity tag. Nowadays the POS tag is not used in deep learning, but with CRF it provides useful information: Nouns are more common denoting entities than verbs, so the POS tags carry useful information."
  },
  {
    "objectID": "ner_crf.html#finding-the-optimal-hyperparameters",
    "href": "ner_crf.html#finding-the-optimal-hyperparameters",
    "title": "NER with Conditional Random Fields (CRF)",
    "section": "Finding the optimal hyperparameters",
    "text": "Finding the optimal hyperparameters\nSo far we’ve trained a model with the default parameters. It’s unlikely that these will give us the best performance possible. Therefore we’re going to search automatically for the best hyperparameter settings by iteratively training different models and evaluating them. Eventually we’ll pick the best one.\nHere we’ll focus on two parameters: c1 and c2. These are the parameters for L1 and L2 regularization, respectively. Regularization prevents overfitting on the training data by adding a penalty to the loss function. In L1 regularization, this penalty is the sum of the absolute values of the weights; in L2 regularization, it is the sum of the squared weights. L1 regularization performs a type of feature selection, as it assigns 0 weight to irrelevant features. L2 regularization, by contrast, makes the weight of irrelevant features small, but not necessarily zero. L1 regularization is often called the Lasso method, L2 is called the Ridge method, and the linear combination of both is called Elastic Net regularization.\nWe define the parameter space for c1 and c2 and use the flat F1-score to compare the individual models. We’ll rely on three-fold cross validation to score each of the 50 candidates. We use a randomized search, which means we’re not going to try out all specified parameter settings, but instead, we’ll let the process sample randomly from the distributions we’ve specified in the parameter space. It will do this 50 (n_iter) times. This process takes a while, but it’s worth the wait."
  }
]