[
  {
    "objectID": "word_embeddings.html",
    "href": "word_embeddings.html",
    "title": "An introduction to word embeddings",
    "section": "",
    "text": "Word embeddings are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding. One of the first algorithms that explored these word embeddings was word2vec. We will use this algorithm in this NB. Generating word embeddings we will step into so-called vector space.\nSuppose we have the following text: “Ronaldo, Messi, Dicaprio”.\nWe can use one-hot encoding to give each of the words of this text an unique position:\n\n\n\n–\nisRonaldo\nisMessi\nisDicaprio\n\n\n\n\nRonaldo\n1\n0\n0\n\n\nMessi\n0\n1\n0\n\n\nDicaprio\n0\n0\n1\n\n\n\nThe above encoding is not informative at all, there are no relationships between the words, every word is isolated.\nWe can do better if we use some world knowledge: Two of these persons are soccer players, the other is an actor. So we, manually, create features:\n\n\n\n–\nisSoccer\nisActor\n\n\n\n\nRonaldo\n1\n0\n\n\nMessi\n1\n0\n\n\nDicaprio\n0\n1\n\n\n\nIn our simple two-vector space, we now get:\n————-Messi, Ronaldo—–> isSoccer\n————-Dicaprio———–> isActor\nWe could add a lot more features: Age, gender, height, weight, etc. But that is impossible to do manually. So, can we do this: design features based on our world knoweledge of the relationships between words, with neural nets? Or, phrased differently, can we have neural nets comb through a large corpus of text and automatically generate word representations?\n\n\nFormulated in 2013 (Mikolov et al. https://arxiv.org/abs/1301.3781): Efficient method to learn vector representations of words from large amount of unstructured texts. Based on the idea of distributional semantics: “You shall know a word by the company it keeps” (J.R. Firth, 1957). Or, similar words appear in similar contexts.\nWord2vec representation learning:\n\nContinuous Bag of Words (CBOW): Given neighbouring words, predict the center word\nSkip-gram: Given the center word, predict the neigbouring words\n\nThere is room for improvement here:\n\nOut of vocabulary (OOV) words. w2v can’t handle words that are not in its vocabulary (seen during training).\nMorphology. Use internal structure of words (radicals, lemma’s: eat, eats, eaten, eater, eating) to get better vectors.\n\n\n\n\nBojanowski et al. (https://arxiv.org/abs/1607.04606) proposed a new embedding theory using the two improvement ideas: FastText.\n\nUse the internal structure of a word to improve vector representations obtained by the skip-gram methode:\n1.1 sub-word generation using n-grams via windowing:  ->  (for 3-grams); but because this explodes:\n1.2 hashing n-grams into buckets with an index:  -> 10\nSkip-gram with negative sampling\n\nFastText is really good on syntactic word analogy tasks in morphologically rich languages (German):\n\ncat -> cats\ndog -> ?\ngood -> better\nrough -> ?\n\nNot as good on semantic analogy tasks:\n\nman -> king\nwoman -> ?\n\nFastText is not as fast as word2vec.\nHere we use Gensim and word2vec, although FastText is also available in the Gensim library. We use the abstracts of all arXiv papers in the category cs.CL (CL: Computation and Language) published before mid-April 2021 (c. 25_000 documents). We tokenize the abstracts with spaCy. Note that the texts we work with share a context (Computation and Language)! Each row in the CSV file consists of two columns: 1. title and 2. abstract. We use the abstracts for the construction of our model.\n\n\n\n\n\n Corpus (filename)\n\nInitialize self. See help(type(self)) for accurate signature.\nUsing Gensim we can set a number of parameters for training:\n\nmin_count: the minimum frequency of words in our corpus\nwindow: number of words to the left and right to make up the context that word2vec will take into account\nvector_size: the dimensionality of the word vectors; usually between 100 and 1_000\nsg: One can choose fro 2 algorithms to train word2vec: Skip-gram (sg) tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. Default is sg=0, hence: default is CBOW."
  },
  {
    "objectID": "word_embeddings.html#using-word-embeddings",
    "href": "word_embeddings.html#using-word-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Using word embeddings",
    "text": "Using word embeddings\nWith the model trained, we can access the word embedding via the wv attribute on model using the token as a key. For example the embedding for “nlp” is:\nFind the similarity between two words. We use the cosine between two word embeddings, so we use a ranges between -1 and +1. The higher the cosine, the more similar two words are.\nFind words that are most similar to target words we line up words via the embeddings: semantically related, other types of pre-tained models, related general models, and generally related words:\nLook for words that are similar to something, but dissimilar to something else with this we can look for a kind of analogies:\nSo a related transformer to lstm is rnn, just like bert is a particular type of transformer; really powerful.\nWe can also zoom in on one of the meanings of ambiguous words. In NLP tree has a very specific meaning, is nearest neighbours being: constituency, parse, dependency, and syntax:\nIf we add syntax as a negative input to the query, we see that the ordinary meaning of tree kicks in: Now forest is one of the nearest neighbours.\nThrow a list of words at the model and filter out the odd one (here svm is the only non-neural model):"
  },
  {
    "objectID": "word_embeddings.html#plotting-embeddings",
    "href": "word_embeddings.html#plotting-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Plotting embeddings",
    "text": "Plotting embeddings\nAbout visualizing embeddings. We need to reduce our 100-dimensions space to 2-dimensions. We can use t-SNE method: map similar data to nearby points and dissimilar data to faraway points in low dimensional space.\nt-SNE is present in Scikit-learn. One has to specify two parameters: n_components (number of dimensions) and metric (similarity metric, here: cosine).\nIn order NOT to overcrowd the image we use a subset of embeddings of 200 most similar words based on a target word."
  },
  {
    "objectID": "word_embeddings.html#exploring-hyperparameters",
    "href": "word_embeddings.html#exploring-hyperparameters",
    "title": "An introduction to word embeddings",
    "section": "Exploring hyperparameters",
    "text": "Exploring hyperparameters\nWhat is the quality of the embeddings? Should embeddings capture syntax or semantical relations. Semantic similarity or topical relations?\nOne way of monitoring the quality is to check nearest neighbours: Are they two nouns, two verbs?\n\n\nevaluate\n\n evaluate (model, word2pos)\n\nNow we want to change some of the settings we used above:\n\nembedding size (dimensions of the trained embeddings): 100, 200, 300\ncontext window: 2, 5, 10\n\nWe will use a Pandas dataframe to keep track of the different scores (but this will take time: We train 9 models!!!):\nResults are close:\n\nSmaller contexts seem to yield better results. Which makes sense because we work with the syntax - nearer words often produce more information.\nHigher dimension word embeddings not always work better than lower dimension. Here we have a relatively small corpus, not enough data for such higher dimensions.\n\nLet’s visualize our findings:"
  },
  {
    "objectID": "word_embeddings.html#conclusions",
    "href": "word_embeddings.html#conclusions",
    "title": "An introduction to word embeddings",
    "section": "Conclusions",
    "text": "Conclusions\nWord embeddings allow us to model the usage and meaning of a word, and discover words that behave in a similar way.\nWe move from raw strings -> vector space: word embeddings which allows us to work with words that have a similar meaning and discover new patterns."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Telematika tutorials",
    "section": "",
    "text": "NLP 101:\n\n00_word_embeddings.ipynb (An introduction to word embeddings)\n01_pretrained_models.ipynb (NLP with pre-trained models from spaCy and StanfordNLP)\n02_discovering_topics.ipynb (Discovering and visualizing topics in texts with LDA)\n\nNamed Entity Recognition (NER)\n\n03_spacy_ner.ipynb (Updating spaCy’s NER system)"
  },
  {
    "objectID": "word_embeddings.html#variables-in-this-notebook",
    "href": "word_embeddings.html#variables-in-this-notebook",
    "title": "An introduction to word embeddings",
    "section": "Variables in this Notebook",
    "text": "Variables in this Notebook\n\n\n\n\n\n\n\n\n\n\nName\nType\nCell #\nSize\nValue(s)\n\n\n\n\nacc\nfloat\n20\n–\n0.6350\n\n\ndf\nDataFrame\n20\n(4, 3)\n100 200 300 2 0.688609\n\n\ndocuments\nCorpus\n4\n–\n<__main__.corpus object at …>\n\n\nembeddings\nndarray\n15\n(201, 100)\n[[1.36567 -2.2555 …] […]]\n\n\nmapped_embeddings\nndarray\n15\n(201, 2)\n[[-0.3663 -1.3517] [8.5049 …]]\n\n\nmodel\nWord2Vec\n20\n–\nWord2Vec(vocab=3099, vector_size=300, alpha=0.025)\n\n\nnlp\nEnglish\n17\n–\nspacy.lang.en.English object at …\n\n\nselected_words\nlist\n15\n201\n[‘roberta’, ‘transformer’, ‘elmo’ …]\n\n\ntarget_word\nstr\n15\n4\n‘bert’\n\n\nword\nstr\n18\n7\n‘careful’\n\n\nword2pos\ndict\n17\n3099\n{‘’: ‘SPACE’, ‘the’: ‘PRON’, …}\n\n\nx\nndarray\n16\n(201,)\n[-0.3666572 8.504919 …]\n\n\ny\nndarray\n16\n(201,)\n[-1.3517823 1.9856246 …]"
  }
]