[
  {
    "objectID": "00_word_embeddings.html",
    "href": "00_word_embeddings.html",
    "title": "An introduction to word embeddings",
    "section": "",
    "text": "We use Gensim. We use the abstracts of all arXiv papers in the category cs.CL (CL: Computation and Language) published before mid-April 2021 (c. 25_000 documents). We tokenize the abstracts with spaCy.\n\n\n\n\n Corpus (filename)\n\nInitialize self. See help(type(self)) for accurate signature.\nUsing Gensim we can set a number of parameters for training:\n\nmin_count: the minimum frequency of words in our corpus\nwindow: number of words to the left and right to make up the context that word2vec will take into account\nvector_size: the dimensionality of the word vectors; usually between 100 and 1_000\nsg: One can choose fro 2 algorithms to train word2vec: Skip-gram (sg) tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. Default is sg=0, hence: default is CBOW."
  },
  {
    "objectID": "00_word_embeddings.html#using-word-embeddings",
    "href": "00_word_embeddings.html#using-word-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Using word embeddings",
    "text": "Using word embeddings\nWith the model trained, we can access the word embedding via the wv attribute on model using the token as a key. For example the embedding for “nlp” is:\nFind the similarity between two words. We use the cosine between two word embeddings, so we use a ranges between -1 and +1. The higher the cosine, the more similar two words are.\nFind words that are most similar to target words we line up words via the embeddings: semantically related, other types of pre-tained models, related general models, and generally related words:\nLook for words that are similar to something, but dissimilar to something else with this we can look for a kind of analogies:\nSo a related transformer to lstm is rnn, just like bert is a particular type of transformer; really powerful.\nWe can also zoom in on one of the meanings of ambiguous words. In NLP tree has a very specific meaning, is nearest neighbours being: constituency, parse, dependency, and syntax:\nIf we add syntax as a negative input to the query, we see that the ordinary meaning of tree kicks in: Now forest is one of the nearest neighbours.\n\n#! export\nmodel.wv.most_similar(positive=[\"tree\"], negative=[\"syntax\"], topn=10)\n\n[('forest', 0.542504072189331),\n ('crf', 0.47240906953811646),\n ('random', 0.45181718468666077),\n ('greedy', 0.4484288692474365),\n ('feed', 0.44518253207206726),\n ('bayes', 0.4260954260826111),\n ('logistic', 0.4220973253250122),\n ('binary', 0.4216032922267914),\n ('top', 0.4214417338371277),\n ('modified', 0.40552183985710144)]\n\n\nThrow a list of words at the model and filter out the odd one (here svm is the only non-neural model):"
  },
  {
    "objectID": "00_word_embeddings.html#plotting-embeddings",
    "href": "00_word_embeddings.html#plotting-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Plotting embeddings",
    "text": "Plotting embeddings\nAbout visualizing embeddings. We need to reduce our 100-dimensions space to 2-dimensions. We can use t-SNE method: map similar data to nearby points and dissimilar data to faraway points in low dimensional space.\nt-SNE is present in Scikit-learn. One has to specify two parameters: n_components (number of dimensions) and metric (similarity metric, here: cosine).\nIn order NOT to overcrowd the image we use a subset of embeddings of 200 most similar words based on a target word."
  },
  {
    "objectID": "00_word_embeddings.html#exploring-hyperparameters",
    "href": "00_word_embeddings.html#exploring-hyperparameters",
    "title": "An introduction to word embeddings",
    "section": "Exploring hyperparameters",
    "text": "Exploring hyperparameters\nWhat is the quality of the embeddings? Should embeddings capture syntax or semantical relations. Semantic similarity or topical relations?\nOne way of monitoring the quality is to check nearest neighbours: Are they two nouns, two verbs?\n\n\nevaluate\n\n evaluate (model, word2pos)\n\nNow we want to change some of the settings we used above:\n\nembedding size (dimensions of the trained embeddings): 100, 200, 300\ncontext window: 2, 5, 10\n\nWe will use a Pandas dataframe to keep track of the different scores (but this will take time: We train 9 models!!!):\nResults are close:\n\nSmaller contexts seem to yield better results. Which makes sense because we work with the syntax - nearer words often produce more information.\nHigher dimension word embeddings not always work better than lower dimension. Here we have a relatively small corpus, not enough data for such higher dimensions.\n\nLet’s visualize our findings:\n\ndf.plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "00_word_embeddings.html#conclusions",
    "href": "00_word_embeddings.html#conclusions",
    "title": "An introduction to word embeddings",
    "section": "Conclusions",
    "text": "Conclusions\nWord embeddings allow us to model the usage and meaning of a word, and discover words that behave in a similar way.\nWe move from raw strings -> vector space: word embeddings which allows us to work with words that have a similar meaning and discover new patterns."
  },
  {
    "objectID": "01_pretrained_models.html",
    "href": "01_pretrained_models.html",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "",
    "text": "import spacy\nen = spacy.load('en_core_web_sm')\nBy applying the spaCy model we assigned to the variable en. We can generate a processed document wit spaCy, doc_en that has sentences and tokens:\nspaCy also identifies a number of linguistic features for every token: lemma, pos_ (the universal POS tags), and tag_(contains the more finegrained, language-specific POS tags):\nspaCy also offers pre-trained models for NER (Named Entity Recognition). The results can be found on the ent_iob_ and ent_type_ attributes.\nThe ent_type_ attribute informs us about what type of entity the token refers to: ‘Donald Trump’ => person, ‘June 14, 1946’ => date, ‘45th’ => ordinal number, and ‘the United States’ => GPE (Geo Political Entity).\nThe ent_iob_ attribute gives, by way of the letters ‘I,O,B’ the position of the token in the entity, where O means that the token is outside of an entity, B the entity is at the beginning of a token, and I means it is inside a token. So basically the IOB scheme gives you information about begin and parts of entities (positional).\nWe can access the recognized entities directly when we use the ents attribute of the document directly:\nOn top of all this, the spaCy model also has a dependency parser on board that analyzes the grammatical realtions between the tokens:\nWe display the results, kept in the variable syntax, in the usual way:"
  },
  {
    "objectID": "01_pretrained_models.html#multilingual-nlp",
    "href": "01_pretrained_models.html#multilingual-nlp",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "Multilingual NLP",
    "text": "Multilingual NLP\nAs can be inferred from the spaCy model we called this model is based on and targeted at the English language.\nOne can use the spaCy website to select models to use for different usecases:\nhttps://spacy.io/usage/models\nBut models for other languages are also available. Let’s try one out on a Dutch text:\n\nnl = spacy.load('nl_core_news_sm')\ntext_nl = (\"Mark Rutte is minister-president van Nederland.\" \"Hij is van de VVD en heeft een slecht geheugen.\")\n\n\ndoc_nl = nl(text_nl)\n\nBecause the Dutch model was trained in its particular way, there are differences with the English model.\nThe most important is that the Dutch models do not offer lemmatization, the lemma_ attribute returns the orth_ attribute.\nNB. whenever numbers turn up in the tables that are generated, they refer to the ID’s of tokens in vectorspace. This usually means that we specified the attribute of a token ent_iob without the ending underscore: ent_iob_.\n\ninfo = [(t.lemma_, t.pos_, t.tag_, t.ent_iob_, t.ent_type_) for t in doc_nl]\ndisplay(display_html(tabulate.tabulate(info, tablefmt='html')))\n\n\n\n\nmark              PROPNSPEC|deeleigen                   BPERSON\nrutte             PROPNSPEC|deeleigen                   IPERSON\nis                AUX  WW|pv|tgw|ev                     O      \nminister-presidentNOUN N|soort|ev|basis|zijd|stan       O      \nvan               ADP  VZ|init                          O      \nnederland         PROPNN|eigen|ev|basis|onz|stan        BGPE   \n.                 PUNCTLET                              O      \nhij               PRON VNW|pers|pron|nomin|vol|3|ev|mascO      \nis                AUX  WW|pv|tgw|ev                     O      \nvan               ADP  VZ|init                          O      \nde                DET  LID|bep|stan|rest                O      \nvvd               PROPNN|eigen|ev|basis|zijd|stan       BORG   \nen                CCONJVG|neven                         O      \nhebben            VERB WW|pv|tgw|met-t                  O      \neen               DET  LID|onbep|stan|agr               O      \nslecht            ADJ  ADJ|prenom|basis|zonder          O      \ngeheugen          NOUN N|soort|mv|basis                 O      \n.                 PUNCTLET                              O      \n\n\n\n\nNone\n\n\nIf one is working with Dutch texts, then the Python library StanfordNLP that is build on top of PyTorchprovides a fully neural pipeline with lemmatization.\n\nimport stanfordnlp\n\n#stanfordnlp.download('nl') # just run this once.\nnl_stanford = stanfordnlp.Pipeline(lang='nl')\n\nUse device: gpu\n---\nLoading: tokenize\nWith settings: \n{'model_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino_tokenizer.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}\n---\nLoading: pos\nWith settings: \n{'model_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino_tagger.pt', 'pretrain_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino.pretrain.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}\n---\nLoading: lemma\nWith settings: \n{'model_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino_lemmatizer.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}\nBuilding an attentional Seq2Seq model...\nUsing a Bi-LSTM encoder\nUsing soft attention for LSTM.\nFinetune all embeddings.\n[Running seq2seq lemmatizer with edit classifier]\n---\nLoading: depparse\nWith settings: \n{'model_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino_parser.pt', 'pretrain_path': '/home/peter/stanfordnlp_resources/nl_alpino_models/nl_alpino.pretrain.pt', 'lang': 'nl', 'shorthand': 'nl_alpino', 'mode': 'predict'}\nDone loading processors!\n---\n\n\n\n# The following line throws an error\n# TODO: debug\n# doc_nl_stanford = nl_stanford(text_nl)\n\"\"\"\nRuntimeError: \"index_select_out_cuda_impl\" not implemented for 'Float'\n\"\"\"\n\n'\\nRuntimeError: \"index_select_out_cuda_impl\" not implemented for \\'Float\\'\\n'\n\n\nThe other thing we tried, in the cell below, is to run the Stanford library using a spaCy pipeline. But this also throws an error: A missing vocab attribute in the pipeline.\nTODO: Solve this first: stanford nlp called as a spaCy library makes sense.\n\n\"\"\"\n# Install this library with pip\nfrom spacy_stanfordnlp import StanfordNLPLanguage\n\nsnlp = stanfordnlp.Pipeline(lang=\"nl\")\nnlp = StanfordNLPLanguage(snlp)\n\"\"\"\n\n'\\n# Install this library with pip\\nfrom spacy_stanfordnlp import StanfordNLPLanguage\\n\\nsnlp = stanfordnlp.Pipeline(lang=\"nl\")\\nnlp = StanfordNLPLanguage(snlp)\\n'\n\n\nLet’s try to use a lemmatizer as such.\n\ndef lemmatizer(texts):\n  docs = nl.pipe(texts)\n  texts = [text.replace(\"\\n\", \"\").strip() for text in texts]\n  cleaned_lemmas = [[t.lemma_ for t in doc] for doc in docs]\n  return cleaned_lemmas\n\n\ncleaned = lemmatizer(text_nl)\nprint(cleaned)\n\n[['m'], ['a'], ['r'], ['k'], [' '], ['r'], ['u'], ['t'], ['t'], ['e'], [' '], ['i'], ['s'], [' '], ['m'], ['i'], ['n'], ['i'], ['s'], ['t'], ['e'], ['r'], ['-'], ['p'], ['r'], ['e'], ['s'], ['i'], ['d'], ['e'], ['n'], ['t'], [' '], ['v'], ['a'], ['n'], [' '], ['n'], ['e'], ['d'], ['e'], ['r'], ['l'], ['a'], ['n'], ['d'], ['.'], ['h'], ['i'], ['j'], [' '], ['i'], ['s'], [' '], ['v'], ['a'], ['n'], [' '], ['d'], ['e'], [' '], ['v'], ['v'], ['d'], [' '], ['e'], ['n'], [' '], ['h'], ['e'], ['e'], ['f'], ['t'], [' '], ['e'], ['e'], ['n'], [' '], ['s'], ['l'], ['e'], ['c'], ['h'], ['t'], [' '], ['g'], ['e'], ['h'], ['e'], ['u'], ['g'], ['e'], ['n'], ['.']]\n\n\nAdd a lookup based lemmatizer for Dutch:\nhttps://github.com/explosion/spaCy/blob/master/spacy/lang/de/tokenizer_exceptions.py"
  },
  {
    "objectID": "03_spacy_ner.html",
    "href": "03_spacy_ner.html",
    "title": "Updating spaCy’s NER system",
    "section": "",
    "text": "Although pre-trained models are simple to use, we just have to plug them in, results will be disappointing when the data we work with differs, even slightly, from the data the model was trained on.\nSo, we want to be able to train our own model. SpaCy has us covered:\nLet’s look at a toy example:\nAlthough the spaCy NER is actually quite good, we want to train the model some more with extra training data. So that a word like “Brexit” for example is properly recognized. For this we do not use the actual sentence itself, too easy. But we will use similar sentences. Here we will use just a couple of sentences from Wikipedia.\nBelow are the 18 NER labels that spaCy uses:\nSo, we have this existing pre-trained spaCy model that we want to update with some new examples (ideally these should be around 200-300 examples).\nThese examples should be presented to spaCy as a list of tuples, that contain the text, and a dictionary of tuples, named entities that contains: the start and end indices of the named entity in the text, and the label of that named entity:\nBefore we set up the NER pipeline with the content of our training data, we make sure that we got the indices right:\nTo set thing up, let’s check if we have a NER in our pipeline:\nThat looks OK, now we assign the NER to a variable:\nNext step is to add these labels to the NER:\nNow we can start training, but only for the NER component of the pipeline, hence the following code snippet:\nIn order to properly train the NER model, we need to:\nLet’s check how our updated NER model now performs, using our earlier sentence:\nBetter, but we trained a little bit on the subject which is precisely why we, the RePubXL team, propose supervised ML on smaller contextual datasets. The power of these NER updates is that, based on the examples, the model can generalize due to the word-embeddings vectorspace.\nNow, we want to keep our updated model for future use:\nIn the cells above we started with a pre-trained model. One can also choose to start with an empty model, using spacy.blank(), passing in the “en” argument for the English language. Because it is an empty model, we have to add this ner to the pipeline using add_pipe(). We do not have to disable other pipelines, as we are just adding a new one, not changing an existing one.\nOne does have to use a large(r) number of training cases.\nJust a small example below:\nWe can use our new model to get more info and train it:"
  },
  {
    "objectID": "03_spacy_ner.html#training-a-completely-new-entity-type-in-spacy",
    "href": "03_spacy_ner.html#training-a-completely-new-entity-type-in-spacy",
    "title": "Updating spaCy’s NER system",
    "section": "Training a completely new entity type in spaCy",
    "text": "Training a completely new entity type in spaCy\nAll code above was directed at training the ner to categorize correctly, either adjusting a pre-trained model or starting from a new blank model and adjusting that as one goes.\nBut what to do if you want to work with a category that is NOT defined?\n\n# Get the `ner` component of the pipeline\nnlp = spacy.load('en_core_web_sm')\nner = nlp.get_pipe('ner')\n\n\n# Add the new label\nLABEL = \"FOOD\"\n\n# Training examples in the required format\nTRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n           ]\n\nWe have to train the model:\n\nfirst add the new label with ner.add_label()\nResume training\nSelect the pipes to be trained\nSingle out the pipes NOT to be trained\n\n\n# Add the new label to ner\nner.add_label(LABEL)\n\n# Resume training\noptimizer = nlp.resume_training()\nmove_names = list(ner.move_names)\n\n# List of pipes you want to train\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n\n# List of pipes which should remain unaffected in training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n\n\n# Importing requirements\nfrom spacy.util import minibatch, compounding\nimport random\n\n# Begin training by disabling other pipeline components\n\nwith nlp.disable_pipes(*other_pipes):\n  sizes = compounding(1.0, 4.0, 1.001)\n  for iteration in range(30):\n    random.shuffle(TRAIN_DATA)\n    losses = {}\n    batches = minibatch(TRAIN_DATA, size=sizes)\n    for batch in batches:\n      texts, annotations = zip(*batch)\n      # new way of updating nlp NOT using nlp.update() anymore\n      example = []\n      # update the model with iterating each text\n      for i in range(len(texts)):\n        doc = nlp.make_doc(texts[i])\n        example.append(Example.from_dict(doc, annotations[i]))\n\n      nlp.update(example, drop=0.5, losses=losses)\n\nprint(losses)\n\n{'ner': 1.698520319920043}\n\n\nWith the training complete, let’s test our ner:\n\ntest_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\ndoc = nlp(test_text)\nprint(\"Entities in '%s'\" % test_text)\nfor ent in doc.ents:\n  print(ent)\n\nEntities in 'I ate Sushi yesterday. Maggi is a common fast food '\nSushi\nMaggi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Telematika tutorials",
    "section": "",
    "text": "NLP 101:\n\n00_word_embeddings.ipynb (An introduction to word embeddings)\n01_pretrained_models.ipynb (NLP with pre-trained models from spaCy and StanfordNLP)\n02_discovering_topics.ipynb (Discovering and visualizing topics in texts with LDA)\n\nNamed Entity Recognition (NER)\n\n03_spacy_ner.ipynb (Updating spaCy’s NER system)"
  },
  {
    "objectID": "04_ner_crf.html",
    "href": "04_ner_crf.html",
    "title": "NER with Conditional Random Fields (CRF)",
    "section": "",
    "text": "CRF is a powerful technique before Deep Learning became popular. POS tagging of sequences in order to label the most important information related to the problem at hand: generic named entities (locations, people, and organizations) or more specialized entities such as disease or symptomes.\nFor this we use sklearn-crfsuite.\nI guess, due to the upsurge of Deep Learning the sklearn-crfsuite is not updated anymore. So instead of the snippet below, we have to install an updated version of the library to be able to produce reports:\nData comes from NLTK:\nLet’s have a look at the data. They are a list of tokenized sentences: the string, POS tag and it’s entity tag. Nowadays the POS tag is not used in deep learning, but with CRF it provides useful information: Nouns are more common denoting entities than verbs, so the POS tags carry useful information.\nHow does CRF work? Deep learning neural nets just learn their relevant features from the input texts themselves. CRFs learn the realtionship between the features we give them and the label of a token in a given context. They do not learn these features themselves, the quality of the model highly depends on the features we present to them.\nWe therefore:\nLet’s try the sent2features function out using the first word from the first training_sent:\nNow we assign our training and test sets the appropriate labels:\nNext we create a CRF model and start the training using the standard lbfgs algorithm for parameter estimation and run it for 100 iterations.\nWhen done, we save the model using joblib.\nThere is an error, but this has to do with the sklearn version we use; to let the error disappear we should use sklearn < 24.0.\nLet’s see whether we can write our model to file:\nWith our model saved, we now can evaluate the output of our CRF model. We will load our model from file and test it on the full test set.\nWe will have a look at the first sentence.\nWe are now ready to evalaute the whole test set. We print out a classification report for all labels except ‘O’. They are the majority of labels anyway, so they will skew the results (they are most probably assigned correctly).\nWe have good scores, especially B-LOC and B-PER score very good.\nNext we will use the eli5 library to have a look at the most likely transitions the CRF model has identified. Eli5 helps us to explain the predictions of our CRF model"
  },
  {
    "objectID": "04_ner_crf.html#finding-the-optimal-hyperparameters",
    "href": "04_ner_crf.html#finding-the-optimal-hyperparameters",
    "title": "NER with Conditional Random Fields (CRF)",
    "section": "Finding the optimal hyperparameters",
    "text": "Finding the optimal hyperparameters\nSo far we’ve trained a model with the default parameters. It’s unlikely that these will give us the best performance possible. Therefore we’re going to search automatically for the best hyperparameter settings by iteratively training different models and evaluating them. Eventually we’ll pick the best one.\nHere we’ll focus on two parameters: c1 and c2. These are the parameters for L1 and L2 regularization, respectively. Regularization prevents overfitting on the training data by adding a penalty to the loss function. In L1 regularization, this penalty is the sum of the absolute values of the weights; in L2 regularization, it is the sum of the squared weights. L1 regularization performs a type of feature selection, as it assigns 0 weight to irrelevant features. L2 regularization, by contrast, makes the weight of irrelevant features small, but not necessarily zero. L1 regularization is often called the Lasso method, L2 is called the Ridge method, and the linear combination of both is called Elastic Net regularization.\nWe define the parameter space for c1 and c2 and use the flat F1-score to compare the individual models. We’ll rely on three-fold cross validation to score each of the 50 candidates. We use a randomized search, which means we’re not going to try out all specified parameter settings, but instead, we’ll let the process sample randomly from the distributions we’ve specified in the parameter space. It will do this 50 (n_iter) times. This process takes a while, but it’s worth the wait.\n\nimport scipy\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import RandomizedSearchCV\n\ncrf = crfsuite.CRF(\n  algorithm='lbfgs',\n  max_iterations=100,\n  all_possible_transitions=True,\n  keep_tempfiles=True\n)\n\nparams_space = {\n    'c1': scipy.stats.expon(scale=0.5),\n    'c2': scipy.stats.expon(scale=0.05),\n}\n\nf1_scorer = make_scorer(metrics.flat_f1_score,\n                        average='weighted', labels=labels)\n\nrs = RandomizedSearchCV(crf, params_space,\n                        cv=3,\n                        verbose=1,\n                        n_jobs=-1,\n                        n_iter=50,\n                        scoring=f1_scorer)\nrs.fit(X_train, y_train)\n\nFitting 3 folds for each of 50 candidates, totalling 150 fits\n\n\nRandomizedSearchCV(cv=3,\n                   estimator=CRF(algorithm='lbfgs',\n                                 all_possible_transitions=True,\n                                 keep_tempfiles=True, max_iterations=100),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe8e46521c0>,\n                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe8e4652700>},\n                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-MISC', 'I-ORG', 'I-LOC']),\n                   verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=3,\n                   estimator=CRF(algorithm='lbfgs',\n                                 all_possible_transitions=True,\n                                 keep_tempfiles=True, max_iterations=100),\n                   n_iter=50, n_jobs=-1,\n                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe8e46521c0>,\n                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fe8e4652700>},\n                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-MISC', 'I-ORG', 'I-LOC']),\n                   verbose=1)estimator: CRFCRF(algorithm='lbfgs', all_possible_transitions=True, keep_tempfiles=True,\n    max_iterations=100)CRFCRF(algorithm='lbfgs', all_possible_transitions=True, keep_tempfiles=True,\n    max_iterations=100)\n\n\nRuns now, since we installed a venv conda py38! But it uses scikit-learn 1.1.1 Duh?\n\nprint('best params:', rs.best_params_)\nprint('best CV score:', rs.best_score_)\nprint('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))\n\nbest params: {'c1': 0.011676207714138713, 'c2': 0.006516931147184303}\nbest CV score: 0.7541792978982037\nmodel size: 1.93M\n\n\n\nbest_crf = rs.best_estimator_\ny_pred = best_crf.predict(X_test)\nprint(metrics.flat_classification_report(\n    y_test, y_pred, labels=sorted_labels, digits=3\n))\n\n              precision    recall  f1-score   support\n\n       B-LOC      0.846     0.860     0.853       774\n       I-LOC      0.403     0.592     0.479        49\n      B-MISC      0.838     0.617     0.710      1187\n      I-MISC      0.669     0.434     0.527       410\n       B-ORG      0.812     0.719     0.762       882\n       I-ORG      0.786     0.639     0.705       551\n       B-PER      0.834     0.899     0.865      1098\n       I-PER      0.890     0.970     0.928       807\n\n   micro avg      0.824     0.757     0.789      5758\n   macro avg      0.760     0.716     0.729      5758\nweighted avg      0.821     0.757     0.782      5758"
  },
  {
    "objectID": "05_ner_pytorch.html",
    "href": "05_ner_pytorch.html",
    "title": "NER with PyTorch",
    "section": "",
    "text": "We will use the same, Dutch, datasets from NLTK we used in the previous notebook:\nNext, we have to pre-process the data. We can use torchtext for this. It is a Python library for pre-processing of natural language. We want our data to be a dataset that consists of examples.\nEach example has two fields: A text filed and a label field. Both consist of sequential information: Tokens and labels.\nThen, we build a vocabulary for both fields. The vocabulary will allow us to map every word and label to their index. One index is kept for unknown words, another for padding."
  },
  {
    "objectID": "05_ner_pytorch.html#training",
    "href": "05_ner_pytorch.html#training",
    "title": "NER with PyTorch",
    "section": "Training",
    "text": "Training\n\nimport torch\n\ndevice='cpu'\nprint(device)\n\ncpu\n\n\nBucketIterator takes care of a lot of details for our training runs:\n\ncreates batches of similar length examples in the data\nmaps the words and labels to the correct indices in their vocabularies\npads the sentences in order to have sentences of the same length (using minimal padding; see first bullet)\n\n\nfrom torchtext.data import BucketIterator\n\nBATCH_SIZE = 32\ntrain_iter = BucketIterator(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\ndev_iter = BucketIterator(dataset=dev_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\ntest_iter = BucketIterator(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True, sort_key=lambda x: len(x.text), sort_within_batch=True)\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)"
  },
  {
    "objectID": "05_ner_pytorch.html#pre-trained-embeddings",
    "href": "05_ner_pytorch.html#pre-trained-embeddings",
    "title": "NER with PyTorch",
    "section": "Pre-trained embeddings",
    "text": "Pre-trained embeddings\nPre-trained embeddings are useful to improve the performance of a model, especially if there is little training data. We use “meaning” from a larger dataset to sharpen our model: Better generalize between semantically related words.\nFor this we use FastText embeddings:\n\ndownload a vec file with the embeddings that we use to initialize our embedding matrix\nwe create a matrix filled with zeros of which the number of rows == the number of words in our vocabulary; the number of cols == the number of FasText vectors (300)\nwe must be sure that the FastText embedding for a particular word is in the correct row: The row whose index corresponds to the index of the word in the vocabulary.\n\n\nimport random\nimport os\nimport numpy as np\n\nEMBEDDING_PATH = os.path.join(os.path.expanduser('~'), \"Documents/data/nlp/cc.nl.300.vec\")\n\ndef load_embeddings(path):\n  \"\"\"Load the FastText embeddings from the NL embeddings file.\"\"\"\n  print('Loading the pre-trained embeddings')\n\n  embeddings = {}\n  with open(path) as i:\n    for line in i:\n      if len(line) > 2:\n        line = line.strip().split()\n        word = line[0]\n        embedding = np.array(line[1:])\n        embeddings[word] = embedding\n  return embeddings\n\ndef initialize_embeddings(embeddings, vocabulary):\n  \"\"\"Use the pre-trained embeddings to initialize an embedding matrix.\"\"\"\n  print('Initializing embedding matrix')\n  embedding_size = len(embeddings['.'])\n  embedding_matrix = np.zeros((len(vocabulary), embedding_size), dtype=np.float32)\n\n  for idx, word in enumerate(vocabulary.itos):\n    if word in embeddings:\n      embedding_matrix[idx,:] = embeddings[word]\n\n  return embedding_matrix\n\nembeddings = load_embeddings(EMBEDDING_PATH)\nembedding_matrix = initialize_embeddings(embeddings, text_field.vocab)\nembedding_matrix = torch.from_numpy(embedding_matrix).to(device)\n\nLoading the pre-trained embeddings\nInitializing embedding matrix\n\n\n\nembedding_matrix[0]\n\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
  },
  {
    "objectID": "05_ner_pytorch.html#caveat",
    "href": "05_ner_pytorch.html#caveat",
    "title": "NER with PyTorch",
    "section": "Caveat",
    "text": "Caveat\nUsing Python 3.9 with somewhat older PyTorch and torchtext libraries on GPU caused Jupyter Kernel crash due to incompatible GPU specs. Updated to newest PyTorch & torchtext for GPU specs did crash the Notebook environment.\nThen ran the code on Python 3.8. Explicitely setting torchtext to 0.81 and torch to 1.17 and running on the CPU. Seem stable.\nI do not think we will need to build our own CNN’s. First of all, because some of the software we are working with already has this capacity build in (spaCy: Retraining NER entities), but, more importantly, we concentrate on supervised ML in well described (smaller) doamins instead of all out deep learning.\nIf we were to dive in DL we must make sure that all these layers are stable over a longer period."
  },
  {
    "objectID": "05_ner_pytorch.html#create-the-bilstm-model",
    "href": "05_ner_pytorch.html#create-the-bilstm-model",
    "title": "NER with PyTorch",
    "section": "Create the BiLSTM model",
    "text": "Create the BiLSTM model\nThe BiLSTM model consists of 4 layers:\n\nAn embedding layer that maps one-hot vectors to dense word embeddings (pre-trained or trained from scratch);\nA bi-directional LSTM layer that reads text b2f and f2b. For each word 2 output vectors are produced hidden_dim that are concatenated in a vector 2*hidden_dim;\nA dropout layer that helps to prevent overfitting by dropping a certain percentage of the items in the LSTM output;\nA dense layer that projects the LSTM output to an output vector with a dimensionality == number of labels.\n\n\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass BiLSTMTagger(nn.Module):\n\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, output_size, embeddings=None):\n        super(BiLSTMTagger, self).__init__()\n        \n        # 1. Embedding Layer\n        if embeddings is None:\n            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        else:\n            self.embeddings = nn.Embedding.from_pretrained(embeddings)\n        \n        # 2. LSTM Layer\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, num_layers=1)\n        \n        # 3. Optional dropout layer\n        self.dropout_layer = nn.Dropout(p=0.5)\n\n        # 4. Dense Layer\n        self.hidden2tag = nn.Linear(2*hidden_dim, output_size)\n        \n    def forward(self, batch_text, batch_lengths):\n\n        embeddings = self.embeddings(batch_text)\n        \n        packed_seqs = pack_padded_sequence(embeddings, batch_lengths)\n        lstm_output, _ = self.lstm(packed_seqs)\n        lstm_output, _ = pad_packed_sequence(lstm_output)\n        lstm_output = self.dropout_layer(lstm_output)\n        \n        logits = self.hidden2tag(lstm_output)\n        return logits"
  },
  {
    "objectID": "05_ner_pytorch.html#training-1",
    "href": "05_ner_pytorch.html#training-1",
    "title": "NER with PyTorch",
    "section": "Training",
    "text": "Training\nNow that we have our model, we need to train it. For that we need to make a couple of decisions:\n\nwe pick a loss function (aka criterion) to quantify how far the model predictions are from the correct output. In the context of NER this often is CrossEntropyLoss;\nwe need to choose an optimizer. In NLP: Adam optimizer (SGD variation).\n\nThen the actual training starts. This happens in several epochs. During each epoch, we show all of the training data to the network, in the batches produced by the BucketIterators we created above. Before we show the model a new batch, we set the gradients of the model to zero to avoid accumulating gradients across batches. Then we let the model make its predictions for the batch. We do this by taking the output, and finding out what label received the highest score, using the torch.max method. We then compute the loss with respect to the correct labels. loss.backward() then computes the gradients for all model parameters; optimizer.step() performs an optimization step.\nWhen we have shown all the training data in an epoch, we perform the precision, recall and F-score on the training data and development data. Note that we compute the loss for the development data, but we do not optimize the model with it. Whenever the F-score on the development data is better than before, we save the model. If the F-score is lower than the minimum F-score we’ve seen in the past few epochs (we call this number the patience), we stop training.\n\nimport torch.optim as optim\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report\n\n\ndef remove_predictions_for_masked_items(predicted_labels, correct_labels): \n\n    predicted_labels_without_mask = []\n    correct_labels_without_mask = []\n        \n    for p, c in zip(predicted_labels, correct_labels):\n        if c > 1:\n            predicted_labels_without_mask.append(p)\n            correct_labels_without_mask.append(c)\n            \n    return predicted_labels_without_mask, correct_labels_without_mask\n\n\ndef train(model, train_iter, dev_iter, batch_size, max_epochs, num_batches, patience, output_path):\n    criterion = nn.CrossEntropyLoss(ignore_index=1)  # we mask the <pad> labels\n    optimizer = optim.Adam(model.parameters())\n\n    train_f_score_history = []\n    dev_f_score_history = []\n    no_improvement = 0\n    for epoch in range(max_epochs):\n\n        total_loss = 0\n        predictions, correct = [], []\n        for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n            optimizer.zero_grad()\n            \n            text_length, cur_batch_size = batch.text[0].shape\n            \n            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size*text_length, NUM_CLASSES)\n            gold = batch.labels.to(device).view(cur_batch_size*text_length)\n            \n            loss = criterion(pred, gold)\n            \n            total_loss += loss.item()\n\n            loss.backward()\n            optimizer.step()\n\n            _, pred_indices = torch.max(pred, 1)\n            \n            predicted_labels = list(pred_indices.cpu().numpy())\n            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n            \n            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                                   correct_labels)\n            \n            predictions += predicted_labels\n            correct += correct_labels\n\n        train_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n        train_f_score_history.append(train_scores[2])\n            \n        print(\"Total training loss:\", total_loss)\n        print(\"Training performance:\", train_scores)\n        \n        total_loss = 0\n        predictions, correct = [], []\n        for batch in dev_iter:\n\n            text_length, cur_batch_size = batch.text[0].shape\n\n            pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n            gold = batch.labels.to(device).view(cur_batch_size * text_length)\n            loss = criterion(pred, gold)\n            total_loss += loss.item()\n\n            _, pred_indices = torch.max(pred, 1)\n            predicted_labels = list(pred_indices.cpu().numpy())\n            correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n            \n            predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                                   correct_labels)\n            \n            predictions += predicted_labels\n            correct += correct_labels\n\n        dev_scores = precision_recall_fscore_support(correct, predictions, average=\"micro\")\n            \n        print(\"Total development loss:\", total_loss)\n        print(\"Development performance:\", dev_scores)\n        \n        dev_f = dev_scores[2]\n        if len(dev_f_score_history) > patience and dev_f < max(dev_f_score_history):\n            no_improvement += 1\n\n        elif len(dev_f_score_history) == 0 or dev_f > max(dev_f_score_history):\n            print(\"Saving model.\")\n            torch.save(model, output_path)\n            no_improvement = 0\n            \n        if no_improvement > patience:\n            print(\"Development F-score does not improve anymore. Stop training.\")\n            dev_f_score_history.append(dev_f)\n            break\n            \n        dev_f_score_history.append(dev_f)\n        \n    return train_f_score_history, dev_f_score_history\n\nLet’s prepare to test the model, taking the same steps as above:\n\nget the predictions;\nremove the masked items;\nprint a classification report.\n\n\ndef test(model, test_iter, batch_size, labels, target_names): \n    \n    total_loss = 0\n    predictions, correct = [], []\n    for batch in test_iter:\n\n        text_length, cur_batch_size = batch.text[0].shape\n\n        pred = model(batch.text[0].to(device), batch.text[1].to(device)).view(cur_batch_size * text_length, NUM_CLASSES)\n        gold = batch.labels.to(device).view(cur_batch_size * text_length)\n\n        _, pred_indices = torch.max(pred, 1)\n        predicted_labels = list(pred_indices.cpu().numpy())\n        correct_labels = list(batch.labels.view(cur_batch_size*text_length).numpy())\n\n        predicted_labels, correct_labels = remove_predictions_for_masked_items(predicted_labels, \n                                                                               correct_labels)\n\n        predictions += predicted_labels\n        correct += correct_labels\n    \n    print(classification_report(correct, predictions, labels=labels, target_names=target_names))\n\nAfter all these steps, we can start to actually train the model:\n\nset embedding dimension to 300 (FastText dimensionality);\npick a hidden dimensionality for each component of the BiLSTM (outputs 512-dim vectors);\nthe number of classes (== length of the vocabulary of the label field) will become the dimesionality of the output layer;\ncompute the number of batches in an epoch in order to show a progress bar.\n\n\nimport math\n\nEMBEDDING_DIM = 300\nHIDDEN_DIM = 256\nNUM_CLASSES = len(label_field.vocab)\nMAX_EPOCHS = 50\nPATIENCE = 3\nOUTPUT_PATH = \"/tmp/bilstmtagger\"\nnum_batches = math.ceil(len(train_data) / BATCH_SIZE)\n\ntagger = BiLSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, VOCAB_SIZE+2, NUM_CLASSES, embeddings=embedding_matrix)  \n\ntrain_f, dev_f = train(tagger.to(device), train_iter, dev_iter, BATCH_SIZE, MAX_EPOCHS, \n                       num_batches, PATIENCE, OUTPUT_PATH)\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 226.21551628410816\nTraining performance: (0.922321904423521, 0.922321904423521, 0.922321904423521, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 25.44461453706026\nDevelopment performance: (0.9280919149839467, 0.9280919149839467, 0.9280919149839467, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 73.94669454917312\nTraining performance: (0.9578127158958568, 0.9578127158958568, 0.9578127158958567, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 20.331317596137524\nDevelopment performance: (0.9359195478546979, 0.9359195478546979, 0.9359195478546979, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 52.222703427542\nTraining performance: (0.9672282426323997, 0.9672282426323997, 0.9672282426323997, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 19.240757752908394\nDevelopment performance: (0.9393955475362857, 0.9393955475362857, 0.9393955475362857, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 43.15425827750005\nTraining performance: (0.9718965279011469, 0.9718965279011469, 0.9718965279011469, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 17.042858469765633\nDevelopment performance: (0.9435879746331626, 0.9435879746331626, 0.9435879746331626, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 36.53407029993832\nTraining performance: (0.9752817749353546, 0.9752817749353546, 0.9752817749353546, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 16.781842106487602\nDevelopment performance: (0.9441717303048797, 0.9441717303048797, 0.9441717303048797, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 31.784735803026706\nTraining performance: (0.9778971990288388, 0.9778971990288388, 0.9778971990288388, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 15.139236290880945\nDevelopment performance: (0.9482049513094701, 0.9482049513094701, 0.9482049513094701, None)\nSaving model.\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 28.689999663620256\nTraining performance: (0.9792394544126646, 0.9792394544126646, 0.9792394544126646, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 16.298973719996866\nDevelopment performance: (0.9463740812481758, 0.9463740812481758, 0.9463740812481758, None)\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 25.47622849655454\nTraining performance: (0.9813219241625708, 0.9813219241625708, 0.9813219241625708, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 16.30899855613825\nDevelopment performance: (0.9471966460583225, 0.9471966460583225, 0.9471966460583225, None)\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 22.8097926521732\nTraining performance: (0.9825062671482995, 0.9825062671482995, 0.9825062671482995, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 15.224460061523132\nDevelopment performance: (0.9477804017300395, 0.9477804017300395, 0.9477804017300395, None)\n\n\n/tmp/ipykernel_13013/4049597506.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for batch in tqdm(train_iter, total=num_batches, desc=f\"Epoch {epoch}\"):\n\n\n\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal training loss: 20.798322341113817\nTraining performance: (0.9842926511517736, 0.9842926511517736, 0.9842926511517736, None)\n\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\nTotal development loss: 16.804522761842236\nDevelopment performance: (0.9480988139146125, 0.9480988139146125, 0.9480988139146125, None)\nDevelopment F-score does not improve anymore. Stop training.\n\n\nF-score does not improve anymore. Let’s plot the F-scores for our training and development sets. We aim for reaching an optimal F-score on the development data:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Data\ndf = pd.DataFrame({'epochs': range(0,len(train_f)), \n                  'train_f': train_f, \n                   'dev_f': dev_f})\n \n# multiple line plot\nplt.plot('epochs', 'train_f', data=df, color='blue', linewidth=2)\nplt.plot('epochs', 'dev_f', data=df, color='green', linewidth=2)\nplt.legend()\nplt.show()\n\n\n\n\nBefore we test our model on the test data, we have to run its eval() method. This will put the model in eval mode, and deactivate dropout layers and other functionality that is only useful in training:\n\ntagger = torch.load(OUTPUT_PATH)\ntagger.eval()\n\nBiLSTMTagger(\n  (embeddings): Embedding(20002, 300)\n  (lstm): LSTM(300, 256, bidirectional=True)\n  (dropout_layer): Dropout(p=0.5, inplace=False)\n  (hidden2tag): Linear(in_features=512, out_features=11, bias=True)\n)\n\n\nFinally we are ready to test the model. It’s performance is lower than our CRF model (04_ner_crf.ipynb). It is not only the code that grew compared to the earlier CRF model, but also the extra work we still have to do:\n\nmake the architecture of the network more complex\noptimize the hyperparameters\nthrow a lot more data at the model"
  },
  {
    "objectID": "05_ner_pytorch.html#testing-our-model",
    "href": "05_ner_pytorch.html#testing-our-model",
    "title": "NER with PyTorch",
    "section": "Testing our model",
    "text": "Testing our model\n\nlabels = label_field.vocab.itos[3:]\nlabels = sorted(labels, key=lambda x: x.split(\"-\")[-1])\nlabel_idxs = [label_field.vocab.stoi[l] for l in labels]\n\ntest(tagger, test_iter, BATCH_SIZE, labels = label_idxs, target_names = labels)\n\n/home/peter/anaconda3/envs/py38/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n\n\n              precision    recall  f1-score   support\n\n       B-LOC       0.89      0.63      0.73       774\n       I-LOC       0.53      0.39      0.45        49\n      B-MISC       0.90      0.44      0.59      1187\n      I-MISC       0.70      0.18      0.29       410\n       B-ORG       0.78      0.52      0.62       882\n       I-ORG       0.71      0.57      0.64       551\n       B-PER       0.85      0.69      0.76      1098\n       I-PER       0.95      0.68      0.79       807\n\n   micro avg       0.84      0.55      0.67      5758\n   macro avg       0.79      0.51      0.61      5758\nweighted avg       0.84      0.55      0.66      5758"
  },
  {
    "objectID": "05_ner_pytorch.html#conclusion",
    "href": "05_ner_pytorch.html#conclusion",
    "title": "NER with PyTorch",
    "section": "Conclusion",
    "text": "Conclusion\nIn this notebook we’ve trained a simple bidirectional LSTM for named entity recognition. Far from achieving state-of-the-art performance, our aim was to understand how neural networks can be implemented and trained in PyTorch. To improve our performance, one of the things that is typically done is to add an additional CRF layer to the neural network. This layer helps us optimize the complete label sequence, and not the labels individually. We leave that for future work."
  },
  {
    "objectID": "02_discovering_topics.html",
    "href": "02_discovering_topics.html",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "",
    "text": "Often texts are just that: texts without metadata and labels that tell us what the texts are about. We can use unsupervised ML, topic models, in such cases to find out about the topics discussed in the texts.\nTopics: Groups of related words that often occur together in texts. Topic models can find clusters of related words. The humans interpret these clusters and assign them labels.\nPopular topic model: Latent Dirichlet Allocation (LDA). It uses a prior distribution topics in a text will have (Dirichlet probability distribution). LDA is often used to model open-ended survey questions.\nHere we will use the data from the Grand Debat Nationale in France.\nWe will focus on the contents of the last, open question of the questionnaire:"
  },
  {
    "objectID": "02_discovering_topics.html#preprocessing",
    "href": "02_discovering_topics.html#preprocessing",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Preprocessing",
    "text": "Preprocessing\nBefore we can train a model, we need to tokenize the texts. For this we use the spaCy NLP library. The author uses a blank model (does not work anymore).\n\nimport spacy\nnlp = spacy.load('fr_core_news_sm')\n\nThe are 4 NaN’s in the first 10 answers, so we throw these out and keep all the texts in the target column.\n\ntexts = df[df[question].notnull()][question]\n\nNext we use spaCy to perform the first pre-processing pass:\n\n\n\nCPU times: user 8min 2s, sys: 3.2 s, total: 8min 6s\nWall time: 8min 6s\n\n\nNow we have a list of spaCy documents that we need to transform into a list of tokens. We will work with lemmatized tokens in order to be able to work with the lemmas. So, these are the following pre-processing steps:\n\nremove all words < 3 characters (interesting for sentiment analysis, but no so much for topic analysis)\ndrop all stopwords\ntake the lemmas of all remaining words and lowercase them\n\n\ndocs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 3 and not t.is_stop] for doc in spacy_docs]\n\ndocs is a list of lists. The lists contain the lemmas of the answers of the survey participants.\nBut we want to take frequent bigrams into account when topic modelling. In tge French language they often carry important meaning (“poids lourds” = “trucks”).\nFor this we use the Python Gensim library:\n\nidentify frequent bigrams in the corpus\nappend these to the list of tokens for the documents in which they appear\n\n\nimport re\nfrom gensim.models import Phrases\n\nbigram = Phrases(docs, min_count=10)\n\nfor idx in range(len(docs)):\n  for token in bigram[docs[idx]]:\n    if '_' in token: # bigrams can be picked out by using the '_' that joins the individual words\n      docs[idx].append(token) # appended to the end, but topic modelling is BoW, so order is not important!\n\nLets have a look at the fifth document:\n\ndocs[4]\n\n['pédagogie',\n 'sens',\n 'petit',\n 'école',\n 'sensibilisation',\n 'parc',\n 'naturel',\n 'enfant',\n 'devenir',\n 'prescripteur',\n 'génération',\n 'futur',\n 'urgence',\n 'parc_naturel',\n 'génération_futur']\n\n\nPerfect, we have found two frequently used (over the corpus) in this particular document of the corpus.\nNext, the final Gensim-specific pre-processing steps:\n\ncreate a dictionary representation of the documents; the dictionary will map each word to an unique ID so that we can make BoW representations of each document. The dictionary will contain ids of words in documents and their frequency;\nwe can remove the least and most frequent words from the vocabulary (faster, better quality). We express the min freq as an absolute number, the max freq is the proportion of documents a word is allowed to occur in:\n\n\nfrom gensim.corpora import Dictionary\n\ndictionary = Dictionary(docs)\nprint(f\"Number of unique words in original documents: {len(dictionary)}\")\n\ndictionary.filter_extremes(no_below=3, no_above=0.25)\nprint(f\"Number of unique words after removing rare and common words: {len(dictionary)}\")\n\n# Let's look at an example document:\nprint(f\"Example representation of document 5: {dictionary.doc2bow(docs[5])}\")\n\nNumber of unique words in original documents: 80955\nNumber of unique words after removing rare and common words: 32718\nExample representation of document 5: [(191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1)]\n\n\nNext, we create bag-of-word (BoW) representations for each of our documents in the corpus:\n\ncorpus = [dictionary.doc2bow(doc) for doc in docs]\ncorpus[5]\n\n[(191, 1),\n (192, 1),\n (193, 1),\n (194, 1),\n (195, 1),\n (196, 1),\n (197, 1),\n (198, 1),\n (199, 1),\n (200, 1)]"
  },
  {
    "objectID": "02_discovering_topics.html#training",
    "href": "02_discovering_topics.html#training",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Training",
    "text": "Training\n\nfrom gensim.models import LdaModel\n\nCPU times: user 2min 55s, sys: 843 ms, total: 2min 55s\nWall time: 1min 28s"
  },
  {
    "objectID": "02_discovering_topics.html#results",
    "href": "02_discovering_topics.html#results",
    "title": "Discovering and Visualizing Topics in Texts",
    "section": "Results",
    "text": "Results\nWhat did the model learn? We start by printing out the 10 words that were most characteristic for each of the topics. Some of the topics are general, but others more precise:\n\nfor (topic, words) in model.print_topics():\n  print(topic + 1, \":\", words)\n\n1 : 0.034*\"agriculture\" + 0.024*\"animal\" + 0.020*\"pesticide\" + 0.020*\"agriculteur\" + 0.018*\"produit\" + 0.014*\"santé\" + 0.012*\"environnement\" + 0.011*\"agricole\" + 0.011*\"interdire\" + 0.011*\"production\"\n2 : 0.048*\"voiture\" + 0.038*\"véhicule\" + 0.037*\"électrique\" + 0.017*\"solution\" + 0.017*\"batterie\" + 0.016*\"diesel\" + 0.014*\"falloir\" + 0.014*\"voiture_électrique\" + 0.014*\"moteur\" + 0.013*\"pollution\"\n3 : 0.012*\"france\" + 0.011*\"bien\" + 0.008*\"année\" + 0.007*\"grand\" + 0.007*\"pays\" + 0.006*\"pouvoir\" + 0.006*\"français\" + 0.006*\"voir\" + 0.006*\"temps\" + 0.005*\"aucun\"\n4 : 0.061*\"énergie\" + 0.032*\"nucléaire\" + 0.018*\"production\" + 0.017*\"renouvelable\" + 0.016*\"éolien\" + 0.015*\"solaire\" + 0.013*\"énergie_renouvelable\" + 0.012*\"électricité\" + 0.012*\"centrale\" + 0.012*\"développer\"\n5 : 0.074*\"transport\" + 0.017*\"taxer\" + 0.015*\"avion\" + 0.014*\"train\" + 0.014*\"camion\" + 0.014*\"route\" + 0.013*\"commun\" + 0.012*\"transport_commun\" + 0.011*\"ligne\" + 0.011*\"routier\"\n6 : 0.030*\"écologique\" + 0.026*\"transition\" + 0.017*\"transition_écologique\" + 0.014*\"falloir\" + 0.014*\"politique\" + 0.013*\"faire\" + 0.012*\"citoyen\" + 0.010*\"environnement\" + 0.008*\"action\" + 0.008*\"prendre\"\n7 : 0.025*\"falloir\" + 0.017*\"faire\" + 0.014*\"planète\" + 0.011*\"climatique\" + 0.010*\"humain\" + 0.010*\"terre\" + 0.010*\"monde\" + 0.010*\"enfant\" + 0.010*\"arrêter\" + 0.009*\"contre\"\n8 : 0.040*\"produit\" + 0.020*\"déchet\" + 0.019*\"plastique\" + 0.013*\"emballage\" + 0.012*\"falloir\" + 0.011*\"consommation\" + 0.011*\"entreprise\" + 0.011*\"faire\" + 0.010*\"taxer\" + 0.009*\"recyclage\"\n9 : 0.017*\"aide\" + 0.016*\"taxe\" + 0.016*\"entreprise\" + 0.014*\"public\" + 0.013*\"travail\" + 0.011*\"logement\" + 0.010*\"écologique\" + 0.009*\"financer\" + 0.009*\"énergétique\" + 0.008*\"impôt\"\n10 : 0.024*\"ville\" + 0.020*\"zone\" + 0.012*\"espace\" + 0.011*\"centre\" + 0.010*\"territoire\" + 0.009*\"grand\" + 0.009*\"public\" + 0.008*\"urbain\" + 0.008*\"agricole\" + 0.008*\"construction\"\n\n\nSome interesting topics:\n\nagriculture (topic 1)\nvehicles (topic 2)\nenergy (topic 4)\nwaste and recycling (topic 8)\ntax incentives (topic 9)\n\n\nimport pyLDAvis.gensim_models\nimport warnings\n\npyLDAvis.enable_notebook()\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\npyLDAvis.gensim_models.prepare(model, corpus, dictionary, sort_topics=False)\n\n/home/peter/anaconda3/lib/python3.9/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n  default_term_info = default_term_info.sort_values(\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n  from imp import reload\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n/home/peter/anaconda3/lib/python3.9/site-packages/joblib/backports.py:36: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(np.__version__) < '1.13':\n/home/peter/anaconda3/lib/python3.9/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  other = LooseVersion(other)\n\n\n\n\n\n\n\n\n\n\n\nLet’s check the topics the model assigns to some individual documents. LDA assigns a high probability to a low number of topics for each document:\n\nfor (text, doc) in zip(texts[:10], docs[:10]):\n    print(text)\n    print([(topic+1, prob) for (topic, prob) in model[dictionary.doc2bow(doc)] if prob > 0.1])\n\nMultiplier les centrales géothermiques\n[(4, 0.77493656)]\nLes problèmes auxquels se trouve confronté l’ensemble de la planète et que dénoncent, dans le plus parfait désordre, les gilets jaunes de France ne sont-ils pas dus, avant tout, à la surpopulation mondiale ? Cette population est passée d’1,5 milliards d’habitants en 1900 à 7 milliards en 2020 et montera bientôt à 10 milliards vers 2040.  Avec les progrès de la communication dans ce village mondial qu'est notre planète, chaque individu, du fin fond de l’Asie au fin fond de l’Afrique, en passant par les « quartiers » et les « campagnes » de notre pays, aspire à vivre – et on ne peu l’en blâmer – comme les moins mal lotis de nos concitoyens (logement, nourriture, biens de consommation, déplacement, etc.).  Voilà la mère de tous les problèmes. Si tel est bien le cas, la solution à tous les problèmes (stabilisation de la croissance démographique, partage des richesses, partage des terres, partage de l’eau, protection de la biodiversité, règlement des conflits, lutte contre la déforestation, lutte contre dérèglement climatique, règlement des conflits, stabilisation des migrations, concurrence commerciale mondiale, etc.) ne sera ni française, ni européenne, mais mondiale. La France se doit d’y jouer un rôle moteur.  Le reste, autour duquel se déroulera « le Grand débat », paraît assez anecdotique.\n[(3, 0.3713212), (7, 0.412301)]\nUne vrai politique écologique et non économique\n[(6, 0.8199872)]\nLes bonnes idées ne grandissent que par le partage.  En ces jours pénibles où s'affrontent le peuple et ceux entre lesquels ils ont remis leur sort. (tous concernés puis qu’élus).  Une idée qui flotte en ce moment sur la mobilité propre dans les zones non ou mal desservies et qui est encore expérimentale dans quelques communes.  L'avenir  est entre vos mains et les nôtres. Pourquoi ne pas planifier l'installation de flotte de véhicules électriques partagés en location, dans ces zones isolées.  En construisant un partenariat avec : état, région, département, commune ou communauté, professionnel de la location, entreprises locales, constructeurs de véhicules, les bailleurs sociaux, pour la mise en place. Avec l'apport financier de la TICPE qui baissera au fil de la décroissance de la consommation, il faudra penser à la déconnecter du budget de fonctionnement de l'état !  Avec un forfait de location pour les utilisateurs, adapté à leur ressources et peut être tout simplement basé sur leurs dépenses de mobilité actuelles. Ainsi pas d'investissements inaccessibles pour eux.  Tout le monde y gagnerait, cela  pourrait prendre l'image du relais de poste comme par le passé et réglerait le problème d'autonomie actuel et engendrerait quelques emplois de service, nouveaux.\n[(2, 0.19174817), (3, 0.280193), (9, 0.30848867), (10, 0.12146572)]\nPédagogie dans ce sens là dés la petite école pour sensibilisation via les parcs naturels . Les enfants doivent devenir des prescripteurs  pour les générations futures . Il y a urgence\n[(6, 0.11756099), (7, 0.531163), (8, 0.1052282), (10, 0.13208893)]\nfaire de l'écologie incitative et non punitive on n'est pas des gosses à qui on distribue des bonnes ou mauvaises notes\n[(1, 0.54360867), (6, 0.38292602)]\nDévelopper le ferroutage pour les poids lourds, Instaurer une vignette pour les poids lourds étrangers qui traversent la France\n[(5, 0.8875869)]\n- Favoriser le tri des déchets en mettant en place une redevance incitatrice et non punitive, - Arrêt d'une écologie punitive à base de taxes qui est néfaste à la compréhension d'une écologie intelligente, adaptée et durable. - Revoir la politique de pose des panneaux solaire en Ferme qui en fait gâche des terrains et sert plus à récolter des taxes pour les caisses des collectivités, - Revoir la politique de mise en place des éoliennes qui ne sont pas adaptées, peu rentable (< 30%) et qui produisent des effets néfastes par du bruit et des infra-ondes, - Arrêt du rachat, à des prix exorbitants, de l'énergie produite par des panneaux solaire, par des éoliennes ou par tout autre moyen dit propre (Bio-masse, hydraulique), - Favoriser le développement de la bio-masse, - Développer le stockage intelligent de l'énergie, - Réaliser une grande opération pour assurer l'isolation des bâtiments énergivores, - Éteindre les éclairages la nuit, - Développer les transports dit propres à base de méthane, d'hydrogène d'électricité. - Équiper tous les toits des grandes surfaces des bâtiments industriels avec des panneaux solaires (non chinois).\n[(1, 0.11670059), (4, 0.43600103), (6, 0.14467861)]\naider les petits commerces ou agriculteurs  et supprimer le glysophate\n[(1, 0.31115368), (9, 0.19260257), (10, 0.40820125)]\nIl faut utiliser TOUS les logements vacants au lieu d'aller en construire de nouveaux sans arrêt, qui bousillent la terre et détruisent l'habitat des rares animaux restants encore (6è extinction de masse).  Taxer de façon extrêmement sévère les piscines, l'eau au-delà du minimum nécessaire par personne et par jour, les gros SUV inutiles, les voyages en avion qui détruisent la planète et ne servent qu'à satisfaire l'occidental riche capitaliste.\n[(3, 0.25267956), (7, 0.21683298), (8, 0.10709418), (10, 0.19728792)]"
  }
]