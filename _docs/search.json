[
  {
    "objectID": "word_embeddings.html",
    "href": "word_embeddings.html",
    "title": "An introduction to word embeddings",
    "section": "",
    "text": "Word embeddings are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding. One of the first algorithms that explored these word embeddings was word2vec. We will use this algorithm in this NB. Generating word embeddings we will step into so-called vector space.\nSuppose we have the following text: “Ronaldo, Messi, Dicaprio”.\nWe can use one-hot encoding to give each of the words of this text an unique position:\n\n\n\n–\nisRonaldo\nisMessi\nisDicaprio\n\n\n\n\nRonaldo\n1\n0\n0\n\n\nMessi\n0\n1\n0\n\n\nDicaprio\n0\n0\n1\n\n\n\nThe above encoding is not informative at all, there are no relationships between the words, every word is isolated.\nWe can do better if we use some world knowledge: Two of these persons are soccer players, the other is an actor. So we, manually, create features:\n\n\n\n–\nisSoccer\nisActor\n\n\n\n\nRonaldo\n1\n0\n\n\nMessi\n1\n0\n\n\nDicaprio\n0\n1\n\n\n\nIn our simple two-vector space, we now get:\n————-Messi, Ronaldo—–> isSoccer\n————-Dicaprio———–> isActor\nWe could add a lot more features: Age, gender, height, weight, etc. But that is impossible to do manually. So, can we do this: design features based on our world knoweledge of the relationships between words, with neural nets? Or, phrased differently, can we have neural nets comb through a large corpus of text and automatically generate word representations?\n\n\nFormulated in 2013 (Mikolov et al. https://arxiv.org/abs/1301.3781): Efficient method to learn vector representations of words from large amount of unstructured texts. Based on the idea of distributional semantics: “You shall know a word by the company it keeps” (J.R. Firth, 1957). Or, similar words appear in similar contexts.\nWord2vec representation learning:\n\nContinuous Bag of Words (CBOW): Given neighbouring words, predict the center word\nSkip-gram: Given the center word, predict the neigbouring words\n\nThere is room for improvement here:\n\nOut of vocabulary (OOV) words. w2v can’t handle words that are not in its vocabulary (seen during training).\nMorphology. Use internal structure of words (radicals, lemma’s: eat, eats, eaten, eater, eating) to get better vectors.\n\n\n\n\nBojanowski et al. (https://arxiv.org/abs/1607.04606) proposed a new embedding theory using the two improvement ideas: FastText.\n\nUse the internal structure of a word to improve vector representations obtained by the skip-gram methode:\n1.1 sub-word generation using n-grams via windowing:  ->  (for 3-grams); but because this explodes:\n1.2 hashing n-grams into buckets with an index:  -> 10\nSkip-gram with negative sampling\n\nFastText is really good on syntactic word analogy tasks in morphologically rich languages (German):\n\ncat -> cats\ndog -> ?\ngood -> better\nrough -> ?\n\nNot as good on semantic analogy tasks:\n\nman -> king\nwoman -> ?\n\nFastText is not as fast as word2vec.\nHere we use Gensim and word2vec, although FastText is also available in the Gensim library. We use the abstracts of all arXiv papers in the category cs.CL (CL: Computation and Language) published before mid-April 2021 (c. 25_000 documents). We tokenize the abstracts with spaCy. Note that the texts we work with share a context (Computation and Language)! Each row in the CSV file consists of two columns: 1. title and 2. abstract. We use the abstracts for the construction of our model.\n\n\n\n\n\n Corpus (filename)\n\nInitialize self. See help(type(self)) for accurate signature.\nUsing Gensim we can set a number of parameters for training:\n\nmin_count: the minimum frequency of words in our corpus\nwindow: number of words to the left and right to make up the context that word2vec will take into account\nvector_size: the dimensionality of the word vectors; usually between 100 and 1_000\nsg: One can choose fro 2 algorithms to train word2vec: Skip-gram (sg) tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. Default is sg=0, hence: default is CBOW."
  },
  {
    "objectID": "word_embeddings.html#using-word-embeddings",
    "href": "word_embeddings.html#using-word-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Using word embeddings",
    "text": "Using word embeddings\nWith the model trained, we can access the word embedding via the wv attribute on model using the token as a key. For example the embedding for “nlp” is:\nFind the similarity between two words. We use the cosine between two word embeddings, so we use a ranges between -1 and +1. The higher the cosine, the more similar two words are.\nFind words that are most similar to target words we line up words via the embeddings: semantically related, other types of pre-tained models, related general models, and generally related words:\nLook for words that are similar to something, but dissimilar to something else with this we can look for a kind of analogies:\nSo a related transformer to lstm is rnn, just like bert is a particular type of transformer; really powerful.\nWe can also zoom in on one of the meanings of ambiguous words. In NLP tree has a very specific meaning, is nearest neighbours being: constituency, parse, dependency, and syntax:\nIf we add syntax as a negative input to the query, we see that the ordinary meaning of tree kicks in: Now forest is one of the nearest neighbours.\nThrow a list of words at the model and filter out the odd one (here svm is the only non-neural model):"
  },
  {
    "objectID": "word_embeddings.html#plotting-embeddings",
    "href": "word_embeddings.html#plotting-embeddings",
    "title": "An introduction to word embeddings",
    "section": "Plotting embeddings",
    "text": "Plotting embeddings\nAbout visualizing embeddings. We need to reduce our 100-dimensions space to 2-dimensions. We can use t-SNE method: map similar data to nearby points and dissimilar data to faraway points in low dimensional space.\nt-SNE is present in Scikit-learn. One has to specify two parameters: n_components (number of dimensions) and metric (similarity metric, here: cosine).\nIn order NOT to overcrowd the image we use a subset of embeddings of 200 most similar words based on a target word."
  },
  {
    "objectID": "word_embeddings.html#exploring-hyperparameters",
    "href": "word_embeddings.html#exploring-hyperparameters",
    "title": "An introduction to word embeddings",
    "section": "Exploring hyperparameters",
    "text": "Exploring hyperparameters\nWhat is the quality of the embeddings? Should embeddings capture syntax or semantical relations. Semantic similarity or topical relations?\nOne way of monitoring the quality is to check nearest neighbours: Are they two nouns, two verbs?\n\n\nevaluate\n\n evaluate (model, word2pos)\n\nNow we want to change some of the settings we used above:\n\nembedding size (dimensions of the trained embeddings): 100, 200, 300\ncontext window: 2, 5, 10\n\nWe will use a Pandas dataframe to keep track of the different scores (but this will take time: We train 9 models!!!):\nResults are close:\n\nSmaller contexts seem to yield better results. Which makes sense because we work with the syntax - nearer words often produce more information.\nHigher dimension word embeddings not always work better than lower dimension. Here we have a relatively small corpus, not enough data for such higher dimensions.\n\nLet’s visualize our findings:"
  },
  {
    "objectID": "word_embeddings.html#conclusions",
    "href": "word_embeddings.html#conclusions",
    "title": "An introduction to word embeddings",
    "section": "Conclusions",
    "text": "Conclusions\nWord embeddings allow us to model the usage and meaning of a word, and discover words that behave in a similar way.\nWe move from raw strings -> vector space: word embeddings which allows us to work with words that have a similar meaning and discover new patterns."
  },
  {
    "objectID": "word_embeddings.html#variables-in-this-notebook",
    "href": "word_embeddings.html#variables-in-this-notebook",
    "title": "An introduction to word embeddings",
    "section": "Variables in this Notebook",
    "text": "Variables in this Notebook\n\n\n\n\n\n\n\n\n\n\nName\nType\nCell #\nSize\nValue(s)\n\n\n\n\nacc\nfloat\n20\n–\n0.6350\n\n\ndf\nDataFrame\n20\n(4, 3)\n100 200 300 2 0.688609\n\n\ndocuments\nCorpus\n4\n–\n<__main__.corpus object at …>\n\n\nembeddings\nndarray\n15\n(201, 100)\n[[1.36567 -2.2555 …] […]]\n\n\nmapped_embeddings\nndarray\n15\n(201, 2)\n[[-0.3663 -1.3517] [8.5049 …]]\n\n\nmodel\nWord2Vec\n20\n–\nWord2Vec(vocab=3099, vector_size=300, alpha=0.025)\n\n\nnlp\nEnglish\n17\n–\nspacy.lang.en.English object at …\n\n\nselected_words\nlist\n15\n201\n[‘roberta’, ‘transformer’, ‘elmo’ …]\n\n\ntarget_word\nstr\n15\n4\n‘bert’\n\n\nword\nstr\n18\n7\n‘careful’\n\n\nword2pos\ndict\n17\n3099\n{‘’: ‘SPACE’, ‘the’: ‘PRON’, …}\n\n\nx\nndarray\n16\n(201,)\n[-0.3666572 8.504919 …]\n\n\ny\nndarray\n16\n(201,)\n[-1.3517823 1.9856246 …]"
  },
  {
    "objectID": "pretrained_models.html",
    "href": "pretrained_models.html",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "",
    "text": "By applying the spaCy model we assigned to the variable en. We can generate a processed document wit spaCy, doc_en that has sentences and tokens:\nspaCy also identifies a number of linguistic features for every token: lemma, pos_ (the universal POS tags), and tag_(contains the more finegrained, language-specific POS tags):\nspaCy also offers pre-trained models for NER (Named Entity Recognition). The results can be found on the ent_iob_ and ent_type_ attributes.\nThe ent_type_ attribute informs us about what type of entity the token refers to: ‘Donald Trump’ => person, ‘June 14, 1946’ => date, ‘45th’ => ordinal number, and ‘the United States’ => GPE (Geo Political Entity).\nThe ent_iob_ attribute gives, by way of the letters ‘I,O,B’ the position of the token in the entity, where O means that the token is outside of an entity, B the entity is at the beginning of a token, and I means it is inside a token. So basically the IOB scheme gives you information about begin and parts of entities (positional).\nWe can access the recognized entities directly when we use the ents attribute of the document directly:\nOn top of all this, the spaCy model also has a dependency parser on board that analyzes the grammatical realtions between the tokens:\nWe display the results, kept in the variable syntax, in the usual way:"
  },
  {
    "objectID": "pretrained_models.html#multilingual-nlp",
    "href": "pretrained_models.html#multilingual-nlp",
    "title": "NLP with pre-trained models: spaCy and Stanford NLP",
    "section": "Multilingual NLP",
    "text": "Multilingual NLP\nAs can be inferred from the spaCy model we called this model is based on and targeted at the English language.\nOne can use the spaCy website to select models to use for different usecases:\nhttps://spacy.io/usage/models\nBut models for other languages are also available. Let’s try one out on a Dutch text:\nBecause the Dutch model was trained in its particular way, there are differences with the English model.\nThe most important is that the Dutch models do not offer lemmatization, the lemma_ attribute returns the orth_ attribute.\nNB. whenever numbers turn up in the tables that are generated, they refer to the ID’s of tokens in vectorspace. This usually means that we specified the attribute of a token ent_iob without the ending underscore: ent_iob_.\nIf one is working with Dutch texts, then the Python library stanza is the one to use (in the Telematika notebook the stanfordnlp library is used, but this library is not recommended anymore.)\n\n# we ran 'stanza.download('nl') in the terminal\nnl_nlp = stanza.Pipeline('nl', use_gpu=False)\n\n\n\n\n2022-08-29 16:39:16 INFO: Loading these models for language: nl (Dutch):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | alpino  |\n| pos       | alpino  |\n| lemma     | alpino  |\n| depparse  | alpino  |\n| ner       | conll02 |\n=======================\n\n2022-08-29 16:39:16 INFO: Use device: cpu\n2022-08-29 16:39:16 INFO: Loading: tokenize\n2022-08-29 16:39:16 INFO: Loading: pos\n2022-08-29 16:39:16 INFO: Loading: lemma\n2022-08-29 16:39:16 INFO: Loading: depparse\n2022-08-29 16:39:17 INFO: Loading: ner\n2022-08-29 16:39:17 INFO: Done loading processors!\n\n\n\ndoc_nl_stanza = nl_nlp(text_nl)\n\nStill get this to work with GPU, but so far, so good. We now have access, via the model, to text and lemma, but also to the attributes upos, xpos, govenor, and dependency_relation.\n\nstanza_info = []\nfor sentence in doc_nl_stanza.sentences:\n  for word in sentence.words:\n    stanza_info.append((len(stanza_info)+1, word.text, word.lemma, word.pos, word.upos, word.xpos, word.deprel))\n\n\ndisplay_html(tabulate.tabulate(stanza_info, tablefmt='html'))\n\n\n\n\n 1Mark              Mark              PROPNPROPNSPEC|deeleigen                   nsubj\n 2Rutte             Rutte             PROPNPROPNSPEC|deeleigen                   flat \n 3is                zijn              AUX  AUX  WW|pv|tgw|ev                     cop  \n 4minister-presidentminister_presidentNOUN NOUN N|soort|ev|basis|zijd|stan       root \n 5van               van               ADP  ADP  VZ|init                          case \n 6Nederland         Nederland         PROPNPROPNN|eigen|ev|basis|onz|stan        nmod \n 7.                 .                 PUNCTPUNCTLET                              punct\n 8Hij               hij               PRON PRON VNW|pers|pron|nomin|vol|3|ev|mascnsubj\n 9is                zijn              AUX  AUX  WW|pv|tgw|ev                     root \n10van               van               ADP  ADP  VZ|init                          case \n11de                de                DET  DET  LID|bep|stan|rest                det  \n12VVD               VVD               PROPNPROPNN|eigen|ev|basis|zijd|stan       obl  \n13en                en                CCONJCCONJVG|neven                         cc   \n14heeft             hebben            VERB VERB WW|pv|tgw|met-t                  conj \n15een               een               DET  DET  LID|onbep|stan|agr               det  \n16slecht            slecht            ADJ  ADJ  ADJ|prenom|basis|zonder          amod \n17geheugen          geheug            NOUN NOUN N|soort|ev|basis|onz|stan        obj  \n18.                 .                 PUNCTPUNCTLET                              punct\n\n\n\n\n\nCombining spaCy and Stanza\nThanks to the spacy-stanza wrapper we can combine the 2 libraries in pipelines. First we install spacy_stanza with Pip.\n\nnlp_spacy_stanza = spacy_stanza.load_pipeline('nl', use_gpu=False)\n\n\n\n\n2022-08-29 21:00:33 INFO: Loading these models for language: nl (Dutch):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | alpino  |\n| pos       | alpino  |\n| lemma     | alpino  |\n| depparse  | alpino  |\n| ner       | conll02 |\n=======================\n\n2022-08-29 21:00:33 INFO: Use device: cpu\n2022-08-29 21:00:33 INFO: Loading: tokenize\n2022-08-29 21:00:33 INFO: Loading: pos\n2022-08-29 21:00:33 INFO: Loading: lemma\n2022-08-29 21:00:33 INFO: Loading: depparse\n2022-08-29 21:00:33 INFO: Loading: ner\n2022-08-29 21:00:34 INFO: Done loading processors!\n\n\n\ndoc_nlp_spacy_stanza = nlp_spacy_stanza(\"Mark Rutte is minister-president van Nederland.\" \"Hij is van de VVD en heeft een slecht actief geheugen.\")\nfor token in doc_nlp_spacy_stanza:\n  print(token.text, token.lemma_, token.pos_, token.dep_, token.ent_type_)\nprint(doc_nlp_spacy_stanza.ents)\n\nMark Mark PROPN nsubj PER\nRutte Rutte PROPN flat PER\nis zijn AUX cop \nminister-president minister_president NOUN root \nvan van ADP case \nNederland Nederland PROPN nmod LOC\n. . PUNCT punct \nHij hij PRON nsubj \nis zijn AUX root \nvan van ADP case \nde de DET det \nVVD VVD PROPN obl ORG\nen en CCONJ cc \nheeft hebben VERB conj \neen een DET det \nslecht slecht ADJ advmod \nactief actief ADJ amod \ngeheugen geheug NOUN obj \n. . PUNCT punct \n(Mark Rutte, Nederland, VVD)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Telematika tutorials",
    "section": "",
    "text": "NLP 101:\n\n00_word_embeddings.ipynb (An introduction to word embeddings)\n01_pretrained_models.ipynb (NLP with pre-trained models from spaCy and Stanza)\n\nNamed Entity Recognition (NER)"
  }
]