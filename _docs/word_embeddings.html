<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.147">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>nlp - An introduction to word embeddings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="nlp - An introduction to word embeddings">
<meta property="og:description" content="Word embeddings are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding.">
<meta property="og:site-name" content="nlp">
<meta name="twitter:title" content="nlp - An introduction to word embeddings">
<meta name="twitter:description" content="Word embeddings are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding.">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">nlp</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pvanhuisstede/nlp"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div class="quarto-toggle-container">
                  <a href="" class="quarto-reader-toggle nav-link" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
              </div>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">An introduction to word embeddings</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">NLP Telematika tutorials</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./word_embeddings.html" class="sidebar-item-text sidebar-link active">An introduction to word embeddings</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pretrained_models.html" class="sidebar-item-text sidebar-link">NLP with pre-trained models: spaCy and Stanford NLP</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#training-word-embeddings" id="toc-training-word-embeddings" class="nav-link active" data-scroll-target="#training-word-embeddings">Training word embeddings</a>
  <ul>
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link" data-scroll-target="#word2vec">Word2vec</a></li>
  <li><a href="#fasttext" id="toc-fasttext" class="nav-link" data-scroll-target="#fasttext">FastText</a></li>
  <li><a href="#corpus" id="toc-corpus" class="nav-link" data-scroll-target="#corpus">Corpus</a></li>
  </ul></li>
  <li><a href="#using-word-embeddings" id="toc-using-word-embeddings" class="nav-link" data-scroll-target="#using-word-embeddings">Using word embeddings</a></li>
  <li><a href="#plotting-embeddings" id="toc-plotting-embeddings" class="nav-link" data-scroll-target="#plotting-embeddings">Plotting embeddings</a></li>
  <li><a href="#exploring-hyperparameters" id="toc-exploring-hyperparameters" class="nav-link" data-scroll-target="#exploring-hyperparameters">Exploring hyperparameters</a>
  <ul>
  <li><a href="#evaluate" id="toc-evaluate" class="nav-link" data-scroll-target="#evaluate">evaluate</a></li>
  </ul></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  <li><a href="#variables-in-this-notebook" id="toc-variables-in-this-notebook" class="nav-link" data-scroll-target="#variables-in-this-notebook">Variables in this Notebook</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pvanhuisstede/nlp/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block">An introduction to word embeddings</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->
<section id="training-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="training-word-embeddings">Training word embeddings</h2>
<p><strong>Word embeddings</strong> are a form of unsupervised ML that as representations of a bunch of texts (input) show syntactic and semantic “understanding. One of the first algorithms that explored these word embeddings was <code>word2vec</code>. We will use this algorithm in this NB. Generating word embeddings we will step into so-called <strong>vector space</strong>.</p>
<p>Suppose we have the following text: “Ronaldo, Messi, Dicaprio”.</p>
<p>We can use <strong>one-hot encoding</strong> to give each of the words of this text an unique position:</p>
<table class="table">
<thead>
<tr class="header">
<th>–</th>
<th>isRonaldo</th>
<th>isMessi</th>
<th>isDicaprio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ronaldo</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>Messi</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Dicaprio</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>The above encoding is not informative at all, there are no relationships between the words, every word is isolated.</p>
<p>We can do better if we use some <strong>world knowledge</strong>: Two of these persons are soccer players, the other is an actor. So we, manually, create <strong>features</strong>:</p>
<table class="table">
<thead>
<tr class="header">
<th>–</th>
<th>isSoccer</th>
<th>isActor</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ronaldo</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>Messi</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="odd">
<td>Dicaprio</td>
<td>0</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>In our simple <strong>two-vector</strong> space, we now get:</p>
<p>————-Messi, Ronaldo—–&gt; isSoccer</p>
<p>————-Dicaprio———–&gt; isActor</p>
<p>We could add a lot more features: Age, gender, height, weight, etc. But that is impossible to do <strong>manually</strong>. So, can we do this: design features based on our world knoweledge of the relationships between words, with <strong>neural nets</strong>? Or, phrased differently, can we have neural nets comb through a large corpus of text and <strong>automatically</strong> generate word representations?</p>
<section id="word2vec" class="level3">
<h3 class="anchored" data-anchor-id="word2vec">Word2vec</h3>
<p>Formulated in 2013 (Mikolov et al.&nbsp;https://arxiv.org/abs/1301.3781): Efficient method to learn vector representations of words from large amount of unstructured texts. Based on the idea of <strong>distributional semantics</strong>: “You shall know a word by the company it keeps” (J.R. Firth, 1957). Or, similar words appear in similar contexts.</p>
<p>Word2vec representation learning:</p>
<ol type="1">
<li>Continuous Bag of Words (CBOW): Given neighbouring words, predict the center word</li>
<li>Skip-gram: Given the center word, predict the neigbouring words</li>
</ol>
<p>There is room for improvement here:</p>
<ul>
<li>Out of vocabulary (OOV) words. w2v can’t handle words that are not in its vocabulary (seen during training).</li>
<li>Morphology. Use internal structure of words (radicals, lemma’s: eat, eats, eaten, eater, eating) to get better vectors.</li>
</ul>
</section>
<section id="fasttext" class="level3">
<h3 class="anchored" data-anchor-id="fasttext">FastText</h3>
<p>Bojanowski et al.&nbsp;(https://arxiv.org/abs/1607.04606) proposed a new embedding theory using the two improvement ideas: <strong>FastText</strong>.</p>
<ol type="1">
<li><p>Use the internal structure of a word to improve vector representations obtained by the skip-gram methode:</p>
<p>1.1 sub-word generation using n-grams via windowing: <eating> -&gt; <ea eat="" ati="" tin="" ing="" ng=""> (for 3-grams); but because this explodes:</ea></eating></p>
<p>1.2 hashing n-grams into buckets with an index: <ing> -&gt; 10</ing></p></li>
<li><p>Skip-gram with negative sampling</p></li>
</ol>
<p>FastText is really good on syntactic word analogy tasks in morphologically rich languages (German):</p>
<ul>
<li>cat -&gt; cats</li>
<li>dog -&gt; ?</li>
<li>good -&gt; better</li>
<li>rough -&gt; ?</li>
</ul>
<p>Not as good on semantic analogy tasks:</p>
<ul>
<li>man -&gt; king</li>
<li>woman -&gt; ?</li>
</ul>
<p>FastText is not as fast as word2vec.</p>
<p>Here we use Gensim and word2vec, although FastText is also available in the Gensim library. We use the abstracts of all arXiv papers in the category cs.CL (CL: Computation and Language) published before mid-April 2021 (c.&nbsp;25_000 documents). We tokenize the abstracts with spaCy. Note that the texts we work with share a context (Computation and Language)! Each row in the CSV file consists of two columns: 1. title and 2. abstract. We use the abstracts for the construction of our model.</p>
<hr>
</section>
<section id="corpus" class="level3">
<h3 class="anchored" data-anchor-id="corpus">Corpus</h3>
<blockquote class="blockquote">
<pre><code> Corpus (filename)</code></pre>
</blockquote>
<p>Initialize self. See help(type(self)) for accurate signature.</p>
<p>Using Gensim we can set a number of parameters for training:</p>
<ul>
<li>min_count: the minimum frequency of words in our corpus</li>
<li>window: number of words to the left and right to make up the context that word2vec will take into account</li>
<li>vector_size: the dimensionality of the word vectors; usually between 100 and 1_000</li>
<li>sg: One can choose fro 2 algorithms to train word2vec: Skip-gram (sg) tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. Default is sg=0, hence: default is CBOW.</li>
</ul>
</section>
</section>
<section id="using-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="using-word-embeddings">Using word embeddings</h2>
<p>With the model trained, we can access the word embedding via the <strong>wv</strong> attribute on model using the token as a key. For example the embedding for “nlp” is:</p>
<p><strong>Find the similarity between two words.</strong> We use the cosine between two word embeddings, so we use a ranges between -1 and +1. The higher the cosine, the more similar two words are.</p>
<p><strong>Find words that are most similar to target words</strong> we line up words via the embeddings: semantically related, other types of pre-tained models, related general models, and generally related words:</p>
<p><strong>Look for words that are similar to something, but dissimilar to something else</strong> with this we can look for a kind of <strong>analogies</strong>:</p>
<p>So a related transformer to lstm is rnn, just like bert is a particular type of transformer; really powerful.</p>
<p>We can also zoom in on <strong>one of the meanings of ambiguous words</strong>. In NLP <strong>tree</strong> has a very specific meaning, is nearest neighbours being: constituency, parse, dependency, and syntax:</p>
<p>If we add <strong>syntax</strong> as a negative input to the query, we see that the ordinary meaning of tree kicks in: Now forest is one of the nearest neighbours.</p>
<p><strong>Throw a list of words at the model</strong> and filter out the odd one (here svm is the only non-neural model):</p>
</section>
<section id="plotting-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="plotting-embeddings">Plotting embeddings</h2>
<p>About visualizing embeddings. We need to reduce our 100-dimensions space to 2-dimensions. We can use t-SNE method: map similar data to nearby points and dissimilar data to faraway points in low dimensional space.</p>
<p>t-SNE is present in Scikit-learn. One has to specify two parameters: <strong>n_components</strong> (number of dimensions) and <strong>metric</strong> (similarity metric, here: cosine).</p>
<p>In order NOT to overcrowd the image we use a subset of embeddings of 200 most similar words based on a <strong>target word</strong>.</p>
</section>
<section id="exploring-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="exploring-hyperparameters">Exploring hyperparameters</h2>
<p>What is the quality of the embeddings? Should embeddings capture syntax or semantical relations. Semantic similarity or topical relations?</p>
<p>One way of monitoring the quality is to check nearest neighbours: Are they two nouns, two verbs?</p>
<hr>
<section id="evaluate" class="level3">
<h3 class="anchored" data-anchor-id="evaluate">evaluate</h3>
<blockquote class="blockquote">
<pre><code> evaluate (model, word2pos)</code></pre>
</blockquote>
<p>Now we want to change some of the settings we used above:</p>
<ul>
<li>embedding size (dimensions of the trained embeddings): 100, 200, 300</li>
<li>context window: 2, 5, 10</li>
</ul>
<p>We will use a Pandas dataframe to keep track of the different scores (but this will take time: We train 9 models!!!):</p>
<p>Results are close:</p>
<ol type="1">
<li>Smaller contexts seem to yield better results. Which makes sense because we work with the syntax - nearer words often produce more information.</li>
<li>Higher dimension word embeddings not always work better than lower dimension. Here we have a relatively small corpus, not enough data for such higher dimensions.</li>
</ol>
<p>Let’s visualize our findings:</p>
</section>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Word embeddings allow us to model the usage and meaning of a word, and discover words that behave in a similar way.</p>
<p>We move from raw strings -&gt; vector space: word embeddings which allows us to work with words that have a similar meaning and discover new patterns.</p>
</section>
<section id="variables-in-this-notebook" class="level2">
<h2 class="anchored" data-anchor-id="variables-in-this-notebook">Variables in this Notebook</h2>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 22%">
<col style="width: 16%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th>Name</th>
<th>Type</th>
<th>Cell #</th>
<th>Size</th>
<th>Value(s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>acc</td>
<td>float</td>
<td>20</td>
<td>–</td>
<td>0.6350</td>
</tr>
<tr class="even">
<td>df</td>
<td>DataFrame</td>
<td>20</td>
<td>(4, 3)</td>
<td>100 200 300 2 0.688609</td>
</tr>
<tr class="odd">
<td>documents</td>
<td>Corpus</td>
<td>4</td>
<td>–</td>
<td>&lt;__main__.corpus object at …&gt;</td>
</tr>
<tr class="even">
<td>embeddings</td>
<td>ndarray</td>
<td>15</td>
<td>(201, 100)</td>
<td>[[1.36567 -2.2555 …] […]]</td>
</tr>
<tr class="odd">
<td>mapped_embeddings</td>
<td>ndarray</td>
<td>15</td>
<td>(201, 2)</td>
<td>[[-0.3663 -1.3517] [8.5049 …]]</td>
</tr>
<tr class="even">
<td>model</td>
<td>Word2Vec</td>
<td>20</td>
<td>–</td>
<td>Word2Vec(vocab=3099, vector_size=300, alpha=0.025)</td>
</tr>
<tr class="odd">
<td>nlp</td>
<td>English</td>
<td>17</td>
<td>–</td>
<td>spacy.lang.en.English object at …</td>
</tr>
<tr class="even">
<td>selected_words</td>
<td>list</td>
<td>15</td>
<td>201</td>
<td>[‘roberta’, ‘transformer’, ‘elmo’ …]</td>
</tr>
<tr class="odd">
<td>target_word</td>
<td>str</td>
<td>15</td>
<td>4</td>
<td>‘bert’</td>
</tr>
<tr class="even">
<td>word</td>
<td>str</td>
<td>18</td>
<td>7</td>
<td>‘careful’</td>
</tr>
<tr class="odd">
<td>word2pos</td>
<td>dict</td>
<td>17</td>
<td>3099</td>
<td>{‘’: ‘SPACE’, ‘the’: ‘PRON’, …}</td>
</tr>
<tr class="even">
<td>x</td>
<td>ndarray</td>
<td>16</td>
<td>(201,)</td>
<td>[-0.3666572 8.504919 …]</td>
</tr>
<tr class="odd">
<td>y</td>
<td>ndarray</td>
<td>16</td>
<td>(201,)</td>
<td>[-1.3517823 1.9856246 …]</td>
</tr>
</tbody>
</table>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>