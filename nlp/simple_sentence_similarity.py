# AUTOGENERATED! DO NOT EDIT! File to edit: ../07_simple_sentence_similarity.ipynb.

# %% auto 0
__all__ = ['sts_dev', 'sts_test', 'sick_train', 'sick_dev', 'sick_test', 'sick_all', 'STOP', 'PATH_TO_WORD2VEC', 'PATH_TO_GLOVE',
           'word2vec', 'PATH_TO_FREQUENCIES_FILE', 'PATH_TO_DOC_FREQUENCIES_FILE', 'frequencies', 'doc_frequencies',
           'load_sts_dataset', 'download_sick', 'Sentence', 'read_tsv', 'run_avg_benchmark']

# %% ../07_simple_sentence_similarity.ipynb 4
import pandas as pd
import numpy as np
import scipy
import math
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns

# %% ../07_simple_sentence_similarity.ipynb 5
def load_sts_dataset(filename):
  """ Loads a subset of the STS dataset into a DataFrame. In particular both sentences and their human rated similarity score."""
  sent_pairs = []
  with open(filename, "r") as f:
    for line in f:
      ts = line.strip().split('\t')
      sent_pairs.append((ts[5], ts[6], float(ts[4])))
  return pd.DataFrame(sent_pairs, columns=["sent_1", "sent_2", "sim"])

"""
# commented out: We will use the local downloaded files
def download_and_load_sts_data():
  # We will grab the STS datasets from their website.
  sts_dataset = tf.keras.utils.get_file(
    fname="Stsbenchmark.tar.gz",
    origin="http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz",
    extract=True
  )
"""

sts_dev = load_sts_dataset('/home/peter/Documents/data/nlp/stsbenchmark/sts-dev.csv')
sts_test = load_sts_dataset('/home/peter/Documents/data/nlp/stsbenchmark/sts-test.csv')

# %% ../07_simple_sentence_similarity.ipynb 9
import requests

def download_sick(f): 

    response = requests.get(f).text

    lines = response.split("\n")[1:]
    lines = [l.split("\t") for l in lines if len(l) > 0]
    lines = [l for l in lines if len(l) == 5]

    df = pd.DataFrame(lines, columns=["idx", "sent_1", "sent_2", "sim", "label"])
    df['sim'] = pd.to_numeric(df['sim'])
    return df
    
sick_train = download_sick("https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_train.txt")
sick_dev = download_sick("https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_trial.txt")
sick_test = download_sick("https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt")
sick_all = pd.concat([sick_train, sick_test, sick_dev])

# %% ../07_simple_sentence_similarity.ipynb 12
import nltk

STOP = set(nltk.corpus.stopwords.words('english'))

class Sentence:
  def __init__(self, sentence):
    self.raw = sentence
    normalized_sentence = sentence.replace("‘", "'").replace("’", "'")
    self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]
    self.tokens_without_stop = [t for t in self.tokens if t not in STOP]

# %% ../07_simple_sentence_similarity.ipynb 14
import gensim
from gensim.models import Word2Vec
from gensim.scripts.glove2word2vec import glove2word2vec

PATH_TO_WORD2VEC = os.path.expanduser("~/Documents/data/nlp/GoogleNews-vectors-negative300.bin")
PATH_TO_GLOVE = os.path.expanduser("~/Documents/data/nlp/glove.840B.300d.txt")

word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)

# %% ../07_simple_sentence_similarity.ipynb 18
import csv

PATH_TO_FREQUENCIES_FILE = os.path.expanduser('~/Documents/data/nlp/frequencies.tsv')
PATH_TO_DOC_FREQUENCIES_FILE = os.path.expanduser('~/Documents/data/nlp/doc_frequencies.tsv')

def read_tsv(f):
  frequencies = {}
  with open(f) as tsv:
    tsv_reader = csv.reader(tsv, delimiter='\t')
    for row in tsv_reader:
      frequencies[row[0]] = int(row[1])
  return frequencies

frequencies = read_tsv(PATH_TO_FREQUENCIES_FILE)
doc_frequencies = read_tsv(PATH_TO_DOC_FREQUENCIES_FILE)
doc_frequencies["NUM_DOCS"] = 1_288_431

# %% ../07_simple_sentence_similarity.ipynb 21
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
import math

def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplists=False, doc_freqs=None):
  if doc_freqs is not None:
    N = doc_freqs['NUM_DOCS']
